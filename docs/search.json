[
  {
    "objectID": "Tools2.html#data-wrangling-with-tidyverse.",
    "href": "Tools2.html#data-wrangling-with-tidyverse.",
    "title": "4  Time Series Tools",
    "section": "4.2 Data Wrangling With tidyverse.",
    "text": "4.2 Data Wrangling With tidyverse.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive to the user. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado<-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function imports the data as a tibble (a data structure similar to a data frame). There are three variables that are classified as character, while the rest are double. At this point you can preview the data with either the spec() or glimpse() commands.\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\"\n\n\nWhen using dplyr it’s always helpful to use piping. Generally speaking, piping allows us to chain functions. Piping (%>%) passes the object on the left of the pipe as the first argument to the right of the pipe. We can illustrate this by using the select() and arrange() functions.\n\navocado %>% select(c(average_price,geography)) %>%\n  arrange(desc(average_price)) \n\n# A tibble: 33,045 × 2\n   average_price geography           \n           <dbl> <chr>               \n 1          3.25 San Francisco       \n 2          3.17 Tampa               \n 3          3.12 San Francisco       \n 4          3.05 Miami/Ft. Lauderdale\n 5          3.04 Raleigh/Greensboro  \n 6          3.03 Las Vegas           \n 7          3    San Francisco       \n 8          3    Raleigh/Greensboro  \n 9          2.99 San Francisco       \n10          2.99 Jacksonville        \n# … with 33,035 more rows\n\n\n\navocado %>% filter(date!=ymd(\"2018-01-01\")) -> avocado\n\nThere is a lot to unpack in this line of code. Let’s start with the functions used. Both the select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data. By default it will sort in ascending order, hence we have used the desc() function to use descending order.\nNow, let’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%>%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. That data frame is sorted in descending order so that the highest average avocado price is displayed first.\nThis example highlights the use of dplyr functions to transform your data. There are plenty of other functions you can use, but learning these are outside the scope of this book. To find out more we recommend reading Wickham (2017) chapter 4. For now we will use one more data transformation technique to retrieve average price of organic avocados for California.\n\navocado %>% \n  filter(geography==\"California\", type==\"organic\",\n         year<=2018) %>%\n  select(date, average_price, geography) -> cali\n\nWhereas the select() function chooses particular variables, the filter() function chooses rows of the tibble that meet the conditions listed."
  },
  {
    "objectID": "Tools2.html#the-avocado-data-set",
    "href": "Tools2.html#the-avocado-data-set",
    "title": "4  Time Series Tools",
    "section": "4.1 The Avocado Data Set",
    "text": "4.1 The Avocado Data Set\nTo demonstrate these tools, we will be using the avocado data set. This data is weekly retail scan data for U.S retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLUs) in the data are only for Hass avocados. Other avocados (e.g. greenskins) are not included in this data.\nNote: When inspecting the data you will notice that each entry is recorded weekly. However, there is an entry for 01/01/2018, that is right after 12/31/2017. This is a single observation that is not weekly. You will also note that there are missing dates from 12/02/2018-12/31/2018."
  },
  {
    "objectID": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "href": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "title": "4  Time Series Tools",
    "section": "4.1 Chipotle Wants You to Forecast Avocado Prices",
    "text": "4.1 Chipotle Wants You to Forecast Avocado Prices\nChipotle is an American chain specializing in tacos and burritos that are made to order in front of the customer. Guacamole is the perfect pairing to their delicious food and one of Chipotle’s best sellers. Their guac uses just six ingredients: avocados, lime juice, cilantro, red onion, jalapeño, and kosher salt. Because of its popularity, each restaurant goes through approximately five cases of avocados a day, amounting to more than 44,000 pounds of avocados annually. Chipotle wants you to develop a model to forecast the price of avocados. This model will allow the company to understand the cost of one of it’s most essential product."
  },
  {
    "objectID": "Tools2.html#lessons-learned-in-this-chapter",
    "href": "Tools2.html#lessons-learned-in-this-chapter",
    "title": "4  Time Series Tools",
    "section": "4.9 Lessons Learned in This Chapter",
    "text": "4.9 Lessons Learned in This Chapter\nIn this module you have been introduced to data wrangling, plotting, tsibbles and time series decomposition. You have learned how to:\n\nManipulate dates with lubridate.\nSelect and filter variables using dplyr.\nPlot time series using ggplot.\nApply tsibbles in time series analysis.\nDecompose a series using the model() and STL() functions in fable.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz."
  },
  {
    "objectID": "Tools2.html#readings",
    "href": "Tools2.html#readings",
    "title": "4  Time Series Tools",
    "section": "4.8 Readings",
    "text": "4.8 Readings\nHyndman (2021) Chapter 1, and Chapter 2.\nWickham (2017) Chapter 2, Chapter 4, and Chapter 19.\ntsibble: https://tsibble.tidyverts.org\nfable: https://fable.tidyverts.org"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Modeling",
    "section": "",
    "text": "Preface\nMaking informed decisions is crucial in a world of uncertainty and ever-changing dynamics. This book is a guide that explores decision analysis, simulation, and time series techniques using R. It is recommended for anyone seeking to learn data-driven approaches to predict outcomes, simulate scenarios, and make informed choices.\nIn the book’s first part, we delve into decision analysis. Here, we explore various frameworks, such as decision trees and risk analysis, enabling us to assess our choices’ potential outcomes and consequences. We uncover strategies to quantify uncertainties, analyze trade-offs, and optimize decision paths. Real-world examples illustrate how these techniques are applied in business scenarios.\nThe book’s second part deals with simulation. We create dynamic, virtual environments to mimic complex systems in business and evaluate multiple scenarios. Through Monte Carlo simulation, we unlock the power to predict outcomes and quantify the impacts of our decisions. Practical exercises and step-by-step guidance equip readers with the skills to build and analyze simulations, enabling them to gain deeper insights into the consequences of their choices.\nIn the final part of this book, we introduce time series analysis. As time is fundamental in many domains, we explore techniques to extract meaningful patterns and forecast future values. The classical methods of ARIMA and ETS are introduced by illustrating their real-world applications in business. Readers will learn to navigate the intricacies of forecasting and leverage time series insights to enhance decision-making.\nComments are welcomed at jagelves@wm.edu"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gelves, J. Alejandro. 2022. “Business Statistics.” https://jagelves.github.io/BusinessStats/.\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/#license.\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.”\nhttps://otexts.com/fpp3/.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. “Business\nStatistics.”\n\n\nKwak, Young, and Lisa Ingall. 2007. “Exploring Monte Carlo\nSimulation Applications for Project Management.”\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical\nManagement Science.”"
  },
  {
    "objectID": "Benchmarks.html#benchmarks",
    "href": "Benchmarks.html#benchmarks",
    "title": "5  Model Benchmarks",
    "section": "5.1 Benchmarks",
    "text": "5.1 Benchmarks\nOne of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A naive prediction sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T\\)\n\nwhere \\(\\hat y_{T+h}\\) is the predicted value for \\(h\\) periods ahead, and \\(y_T\\) is the value observed at the current time period \\(T\\). We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would “drift” the naive prediction upward. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T+h(\\frac{y_t-y_1}{T-1})\\)\n\nwhere \\(h(\\frac{y_t-y_1}{T-1})\\) can be thought as the average increase of \\(y\\) from period \\(1\\) to the current period \\(T\\). One could also predict weight by observing weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:\n\n\\(\\hat y_{T+h}=\\frac{(y_1+y_2+...+y_T)}{T}\\)\n\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:\n\n\\(\\hat y_{T+h}=b_0+b_1(T+h)\\)"
  },
  {
    "objectID": "Benchmarks.html#accuracy",
    "href": "Benchmarks.html#accuracy",
    "title": "5  Model Benchmarks",
    "section": "5.3 Accuracy",
    "text": "5.3 Accuracy\nWe will assess the fit of the benchmarks by comparing the fitted values against actual values. Generally, a good fit is determined by how far the fitted values are from the actual ones. If we square all of the distances (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE). Formally:\nHow we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE’s but then find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since the measures are the smallest. We have highlighted these results and made the table more appealing by using the gt library.\n\naccuracy(fit)\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      geography\n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    California\nmean\nTraining\n0.00\n0.27\n0.22\n−2.50\n12.94\n2.25\n2.02\n0.86\n    California\nNaive\nTraining\n0.00\n0.13\n0.10\n−0.03\n5.72\n1.00\n1.00\n−0.19\n    California\nDrift\nTraining\n0.00\n0.13\n0.10\n−0.31\n5.73\n1.00\n1.00\n−0.19\n    California\nLS\nTraining\n0.00\n0.25\n0.19\n−2.00\n11.34\n2.02\n1.84\n0.85"
  },
  {
    "objectID": "Benchmarks.html#leasons-learned",
    "href": "Benchmarks.html#leasons-learned",
    "title": "5  Model Benchmarks",
    "section": "5.9 Leasons Learned",
    "text": "5.9 Leasons Learned\nIn this module you have been introduced to the general procedure in forecasting time series. Particularly you have learned to:\n\nCreate forecasts with simple heuristics.\nAssess the fit of the model with accuracy measures.\nCreate a test set and train set to avoid over-fitting.\nPerform cross validation.\nSelect models with the AIC, AICc or BIC.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "Benchmarks.html#readings",
    "href": "Benchmarks.html#readings",
    "title": "5  Model Benchmarks",
    "section": "5.8 Readings",
    "text": "5.8 Readings\nHyndman (2021) Chapter 5 (The Forecaster’s Toolbox).\ngt package: https://gt.rstudio.com"
  },
  {
    "objectID": "Tools.html#r-basics",
    "href": "Tools.html#r-basics",
    "title": "1  Tools for Working With Simulation",
    "section": "1.1 R Basics",
    "text": "1.1 R Basics\nObjects, vectors and data frames are all important in the R programming language. They mainly help us store and manipulate data. An object is a piece of data that can be stored in a variable. It can be as simple as a single integer or as output of regression analysis. Vectors are one-dimensional arrays of data that cn be stored in an object. Vectors can contain elements of various data types, such as numerical values, character strings, and logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Lastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record."
  },
  {
    "objectID": "Tools.html#r-intermediate",
    "href": "Tools.html#r-intermediate",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 R Intermediate",
    "text": "1.2 R Intermediate\nLoops and conditionals\nrandom number generators"
  },
  {
    "objectID": "Tools.html#readings",
    "href": "Tools.html#readings",
    "title": "2  Tools for Working With Simulation",
    "section": "2.7 Readings",
    "text": "2.7 Readings\nThese reading will help you review the concepts and theory necessary for completing this module. Grolemund (2014) reviews the R basics needed to perform computer simulation, Jaggia and Kelly (2022) introduces probability distributions, Gelves (2022) has several applied problems (with solutions) in R to review the probability distributions, while Winston and Albright (2019) provides an application of the distributions to business simulation.\n\nGrolemund (2014) Chapter 1 (The Very Basics), Chapter 3 (R Objects), Chapter 7.2, 7.3 (Conditional Statements), Chapter 9.3, 9.4, 9.5 (Loops).\nJaggia and Kelly (2022) Chapter 5 (Discrete Probability Distributions) and Chapter 6 (Continuous Probability Distributions).\nGelves (2022) Chapter 10 (Discrete Random Variables), Chapter 11 (Continuous Random Variables). This is mainly review from your probability course. It is recommended you attempt the exercises in both chapters (solutions are provided at the end).\nWinston and Albright (2019) Chapter 10.1 (Introduction) and 10.2 (Probability Distributions for Input Variables). Pay special attention to the probability distributions and try to replicate the examples in R."
  },
  {
    "objectID": "Tools.html#storing-our-data-in-r",
    "href": "Tools.html#storing-our-data-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.1 Storing Our Data in R",
    "text": "2.1 Storing Our Data in R\nObjects, vectors, and data frames are all critical in the R programming language. They are helpful when storing and manipulating data in R. An object is often stored in a variable. It can be as simple as a single integer or as informative as the output in regression analysis. The code below creates an object x that stores the number \\(5\\).\n\nx&lt;-5\n\nVectors are one-dimensional data arrays that can be stored in an object. They can contain elements of various data types, such as numerical values, character, or logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Below, the vector Books stores the titles of \\(5\\) monster classics a bookstore plans to release.\n\nbooks&lt;-c(\"Frankenstein\",\"Dracula\",\"Moby Dick\",\n         \"War Of The Worlds\",\"Beowulf\")\n\nLastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record. Think of a data frame as a collection of related vectors. We can easily construct a data frame by combining one or more vectors using the data.frame() function in R.\n\ndata.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45))\n\n              Books Price\n1      Frankenstein  9.50\n2           Dracula  5.78\n3         Moby Dick  6.34\n4 War Of The Worlds  5.67\n5           Beowulf  2.45"
  },
  {
    "objectID": "Tools.html#automating-the-trial-process-in-r",
    "href": "Tools.html#automating-the-trial-process-in-r",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 Automating The Trial Process in R",
    "text": "1.2 Automating The Trial Process in R\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions. A loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true.\n\n\n\n\n\nConditionals, on the other hand, allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. By using loops and conditionals, you can create more complex and powerful programs in R."
  },
  {
    "objectID": "Tools.html#generating-random-numbers-in-r",
    "href": "Tools.html#generating-random-numbers-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.2 Generating Random Numbers in R",
    "text": "2.2 Generating Random Numbers in R\nSeveral functions are available in R that can be used to generate random numbers. These functions are based on a specific probability distribution. For instance, the rbinom() function generates random numbers based on the binomial distribution, while the runif() and rnorm() functions generate random numbers based on the uniform and normal distributions, respectively. The table below lists some functions that generate random numbers and their probability distribution.\n\n\n\nDistribution\nFamily\nPackage\nFunction\n\n\n\n\nUniform\nDiscrete\nextraDistr\nrdunif()\n\n\nBinomial\nDiscrete\nBase R\nrbinom()\n\n\nHypergeometric\nDiscrete\nBase R\nrhyper()\n\n\nPoisson\nDiscrete\nBase R\nrpois()\n\n\nUniform\nContinuous\nBase R\nrunif()\n\n\nNormal\nContinuous\nBase R\nrnorm()\n\n\nExponential\nContinuous\nBase R\nrexp()\n\n\nTriangle\nContinuous\nextraDistr\nrtriang()\n\n\n\nRecall that the binomial distribution illustrates the probability of x successes from an experiment with n trials. We can use the distribution to generate random numbers by providing the p (probability of success) and n (number of trials) parameters. Similarly, the uniform distribution shows the probability of a random variable within a minimum and maximum limit. Hence, we can generate random numbers from the distribution by providing the minimum and the maximum."
  },
  {
    "objectID": "Tools.html#company-x-want-your-help",
    "href": "Tools.html#company-x-want-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Want Your Help",
    "text": "1.4 Company X Want Your Help"
  },
  {
    "objectID": "Tools.html#leasons-learned-in-this-chapter",
    "href": "Tools.html#leasons-learned-in-this-chapter",
    "title": "1  Tools for Working With Simulation",
    "section": "1.5 Leasons Learned In This Chapter",
    "text": "1.5 Leasons Learned In This Chapter"
  },
  {
    "objectID": "Tools.html#using-loops-and-conditional-in-r",
    "href": "Tools.html#using-loops-and-conditional-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Using Loops and Conditional in R",
    "text": "2.3 Using Loops and Conditional in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to easily generate new variables for our model, or test different variations of our parameters to see how the model behaves.\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true. Below you can see the structure of the while loop.\n\n\n\n\n\nLet’s illustrate the syntax of the for loop by simulating a demand for each book and calculating the revenue generated by each book in the Monster Classic series.\n\nRevenue&lt;-c()\n\nfor (i in MS$Price) {\n  Revenue&lt;-c(Revenue,i*rnorm(1,50,2))\n  Revenue\n}\n\nConditionals allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. Below is the structure of the conditional statement.\n\n\n\n\n\nLet’s go back to the Monster Classic example and assume that the bookstore has gained additional insight on the demand of their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand&lt;-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand&lt;-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 87 88 76 80 74\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, then the random binomial number is drawn with the probability \\(0.9\\), if not the it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series one by one and adds a simulated demand. You can quickly realize that this becomes very efficient if the bookstore has a very large collection of books. Below is our data frame with the new simulated values.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     87\n2           Dracula  5.78     88\n3         Moby Dick  6.34     76\n4 War Of The Worlds  5.67     80\n5           Beowulf  2.45     74"
  },
  {
    "objectID": "Tools.html#company-x-wants-your-help",
    "href": "Tools.html#company-x-wants-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Wants Your Help",
    "text": "1.4 Company X Wants Your Help"
  },
  {
    "objectID": "Tools.html#lessons-learned-in-this-chapter",
    "href": "Tools.html#lessons-learned-in-this-chapter",
    "title": "2  Tools for Working With Simulation",
    "section": "2.8 Lessons Learned In This Chapter",
    "text": "2.8 Lessons Learned In This Chapter\n\nGenerate random numbers using R functions.\nUse Loops and Conditionals to simulate variables.\nApply objects, vectors, and data frames to store and manipulate data.\n\n\n\n\n\nGelves, J. Alejandro. 2022. “Business Statistics.” https://jagelves.github.io/BusinessStats/.\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/#license.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. “Business Statistics.”\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 The VA Deaprtment of Transportation Wants Your Services",
    "text": "2.4 The VA Deaprtment of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they schedule the expected staff and number of ferries to run.\nAssume that the VA Department of transportation shares three weeks of data. The table below records the number of vehicles that used the ferry service:\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\n\n\n\n\nMon\n1175\n1020\n1163\n\n\nTue\n1198\n1048\n1066\n\n\nWed\n1189\n1102\n1183\n\n\nThu\n1175\n1094\n1003\n\n\nFri\n1101\n1042\n1095\n\n\nSat\n1529\n1464\n1418\n\n\nSun\n1580\n1534\n1512\n\n\n\nWhat distribution would you use to simulate weekdays (Mon-Fri)? Would you simulate weekends (Sat and Sun) differently than weekdays? According to the data, what would be the minimum and maximum number of vehicles transported during weekdays (weekends)? Can you provide a sensible simulation for week 4?"
  },
  {
    "objectID": "Simulation.html#inputs-calculated-and-objective",
    "href": "Simulation.html#inputs-calculated-and-objective",
    "title": "2  Simulation in R",
    "section": "2.1 Inputs, Calculated, and Objective",
    "text": "2.1 Inputs, Calculated, and Objective"
  },
  {
    "objectID": "Simulation.html#law-of-large-numbers",
    "href": "Simulation.html#law-of-large-numbers",
    "title": "3  Simulation in R",
    "section": "3.4 Law of Large Numbers",
    "text": "3.4 Law of Large Numbers\nBefore we answer the question of how much fish to order, we must realize a couple of flaws of the model we have created. Mainly, we have run the simulation once and it is unlikely (although possible) that the attendance will be exactly the single value simulated by the rtriang() function (\\(44\\)). Instead, we want to provide the restaurant with a set of eventualities. Worst case scenarios (low attendance), best case scenarios (high attendance), and most likely outcome for their decision. This is only possible if we generate several attendance numbers, and see how the profit (output) behaves.\nAn additional problem is that if we tell the restaurant that the expected profit of ordering \\(x\\) amount of fish \\(y\\), we want to make sure that the average is not biased. Recall that as the sample size of a study increases, the average of the sample will converge towards the true population mean. In other words, as the number of simulations in our model increases, the estimate of the expected profit becomes more accurate. This is known as the Law of Large Numbers.\nThe code below repeats the simulation several times, allowing for different attendance scenarios to arise. Although, there is not a set number of times one should run a simulation to get a good estimate of the mean (or distribution), computers are powerful enough to run thousands if not millions of simulations. Below we run the simulation 10,000 times for illustration purposes.\n\nn&lt;-10000\nV_Order_Oz&lt;-rep(Order_Oz,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance&lt;-round(rtriang(n,20,50,30),0)\nConsumption&lt;-V_Attendance*V_Fish_Entitled_Oz\nAvailable&lt;-V_Order_Oz\n\nV_Profit&lt;-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\n\nFrom the simulation it seems like that the restaurant would make on average a profit of about \\(211.5\\) dollars if they order \\(160\\) ounces of fish. There are however, a couple of questions left unanswered. First, what are the other possible profits when ordering \\(160\\) ounces? Second, is there another amount of fish that would give them a higher expected profit?"
  },
  {
    "objectID": "Simulation.html#the-flaw-of-averages",
    "href": "Simulation.html#the-flaw-of-averages",
    "title": "2  Simulation in R",
    "section": "2.6 The Flaw of Averages",
    "text": "2.6 The Flaw of Averages\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population. For example, if a group of people has an average height of 5’10”, it does not mean that every individual in the group is exactly 5’10” tall. Some people may be shorter, while others may be taller. The flaw of averages can lead to inaccurate assumptions and incorrect conclusions, particularly when making predictions or decisions based on the average value. It is important to consider the distribution and variability of the characteristic within the population when making predictions or decisions, rather than relying solely on the average value."
  },
  {
    "objectID": "Simulation.html#the-newsvendor-problem",
    "href": "Simulation.html#the-newsvendor-problem",
    "title": "2  Simulation in R",
    "section": "2.5 The Newsvendor Problem",
    "text": "2.5 The Newsvendor Problem\nThe news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order and at what price to sell it. The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them. In the news vendor problem, there are two main factors that influence the decision: the demand for the product and the cost of the product. The demand for the product is uncertain, as it depends on various factors such as the weather, the competition, and the price of the product. The cost of the product is also uncertain, as it may vary based on factors such as production costs, transportation costs, and discounts. The goal of the news vendor is to maximize profits by ordering the optimal amount of the product and setting the optimal price. The news vendor must make this decision based on incomplete information about the demand and cost of the product, which makes the problem challenging and requires the use of probabilistic models and optimization techniques."
  },
  {
    "objectID": "Simulation.html#sensitivity-analysis",
    "href": "Simulation.html#sensitivity-analysis",
    "title": "3  Simulation in R",
    "section": "3.7 Sensitivity Analysis",
    "text": "3.7 Sensitivity Analysis\nWhat if the price of yellow-tail increases unexpectedly? What if the restaurant wishes to provide each guest with six ounces of fish instead of five? The model we have created above can accommodate these changes easily by just changing the input values. We can additionally, inform the restaurant how sensitive the results are to changes in key inputs or assumptions. This is the idea behind sensitivity analysis.\nSensitivity analysis assesses the robustness of a model or decision by evaluating the impact of changes in certain key input variables on the output of the model or decision. Sensitivity analysis helps to identify which variables are most important and how sensitive the output is to changes in those variables.\nLet’s consider providing each guest with six or seven ounces of fish. How does this change our recommendation? When running our model with different amounts of fish provided, you’ll notice that when each guest gets six ounces of fish, the restaurant should order $240$ ounces yielding an expected profit of $220$ dollars. When each guest gets seven ounces of fish, the restaurant should order $270$ ounces which generates an expected profit of $198$ dollars. Perhaps providing each guest with more fish is a bad idea as profits are highly sensitive to every ounce increase."
  },
  {
    "objectID": "Simulation.html#a-restaurant-needs-your-help",
    "href": "Simulation.html#a-restaurant-needs-your-help",
    "title": "2  Simulation in R",
    "section": "2.1 A Restaurant Needs Your Help",
    "text": "2.1 A Restaurant Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. They purchase the fish at the market and discard any unsold fish at the end of the day. However, they are unsure of how many people will attend the event. To ensure they have enough sushi and budget appropriately, they want to estimate the total cost and amount of sushi needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (which is divisible). Each guest at the festival is entitled to 5 ounces of sushi. If the sushi runs out, Sushi X has promised to serve their famous Miso soup, which cost them a total of 300 dollars for the event. Given that the festival charges 20 dollars per entry, how much yellow-tail would you recommend Sushi X purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#model-framework",
    "href": "Simulation.html#model-framework",
    "title": "3  Simulation in R",
    "section": "3.2 Model Framework",
    "text": "3.2 Model Framework\nThe restaurant provides you with a lot of information which might be overwhelming at first glance. To make the task less daunting, we should organize/classify the information so that we can create a model. For many business problems, the data can be classified into the following parts:\n\nThe inputs have given fixed values and provide the model’s basic structure. These are values that are most likely to be given and determined.\nThe decision variables are values the decision maker controls. We are usually interested in finding the optimal level of this variable.\nThe calculated values transform inputs and decision variables to other values that help describe our model. These make the model more informative and often are required to derive outputs.\nThe random variables are the primary source of uncertainty in the model. They are often modeled by sampling probability distributions.\nThe outputs are the ultimate values of interest; the inputs, decision variables, random variables, or calculated values determine them.\n\nBelow, you can see how we can classify and input the information in R.\n\nlibrary(extraDistr)\nOrder_Oz&lt;-160 # Decision Variable\n\nPrice_Fish_Oz&lt;-9/16 # Input\nPrice_Miso&lt;-300 # Input\nEntry_Fee&lt;-20 # Input\nFish_Entitled_Oz&lt;-5 #Input\n\nset.seed(20)\nAttendance&lt;-round(rtriang(1,20,50,30),0) # Random Variable\n\nConsumption&lt;-Attendance*Fish_Entitled_Oz # Calculated \nAvailable&lt;-Order_Oz # Calculated\n\nProfit&lt;-min(Consumption,Available)/Fish_Entitled_Oz*Entry_Fee-Order_Oz*Price_Fish_Oz-Price_Miso #Outcome\n\nIn the model above, you can see how the inputs have fixed values. These values were given to us by the restaurant and the restaurant has little or no control over them (the market price of the fish, the cost of making Miso soup, etc.). The random variable, captures the source of uncertainty (i.e., how many people attend the event). As you can see we have used here the rtriang() function from the extraDistr package to generate the attendance. We have chosen the triangle distribution since the restaurant has provided us with a lower limit (20), an upper limit (50), and a most likely case for attendance (30). Note also the use of the set.seed() function. This allows you to replicate the random numbers generated.\nThe calculated variables combine inputs, the decision variable, and the random variable to provide insight on how much is fish is expected to be both consumed and available. They also help us determine the profit (output), which is our main guide in knowing whether the decision of ordering \\(x\\) ounces of fish is the “best”."
  },
  {
    "objectID": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "href": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "title": "3  Simulation in R",
    "section": "3.1 Roll With It Sushi Needs Your Help",
    "text": "3.1 Roll With It Sushi Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. Every morning, the Sous-Chef purchases fish at the market and any unsold fish at the end of the day gets discarded.\nThe restaurant has contacted you because they are still determining how many people will attend the event and worry about how this will impact their business financially. To ensure they have enough sushi and budget appropriately, they want you to recommend the amount of fish needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people, and without your guidance they feel like this is the best guide in determining the fish needed.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (divisible). Each guest at the festival is entitled to 5 ounces of fish. If the sushi runs out, some customers will not be happy. However, the restaurant has promised to refund their entry fee. Additionally, they have promised to serve their famous Miso soup to every attendee. The cost of making a batch for up to 50 guests is 300 dollars.\nGiven that the festival charges 20 dollars per entry, how much yellow-tail would you recommend the restaurant to purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#the-news-vendor-problem",
    "href": "Simulation.html#the-news-vendor-problem",
    "title": "3  Simulation in R",
    "section": "3.3 The News Vendor Problem",
    "text": "3.3 The News Vendor Problem\nNote how the decision variable (Order_Oz) affects directly our outcome (Profit). We can see that it decreases the restaurant’s profit through costs, but also affects revenue through the amount of fish available. This is the heart of the problem. We don’t know how many people will attend, so if the restaurant orders too much fish their profits will go down because their costs are large. However, if they order too little then they will have to issue refunds, which decrease their revenue.\nAs you observe the Profit formula in the code above, you’ll notice the use of the min() function. This function returns the minimum of the Attendance and Consumption. The intuition here is that the restaurant can only collect entry fees for people who consumed sushi when the attendance is greater than the amount of sushi available. Likewise, their revenue will be capped at the total amount of people who attended, even if they ordered plenty of fish.\nThe problem illustrated above is called the news vendor problem. The news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order (and sometimes at what price to sell it). The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them."
  },
  {
    "objectID": "Simulation.html#recommendation",
    "href": "Simulation.html#recommendation",
    "title": "2  Simulation in R",
    "section": "2.5 Recommendation",
    "text": "2.5 Recommendation\nThere are a couple of things that are now evident. It seems likely that the restaurant would make on average a profit of about 234 dollars if they order 160 ounces of fish. We can now inform the restaurant that it is likely they will make a profit if they order 160 ounces. There are a couple of questions left unanswered. First, 234 dollars are expected of profits, what are the other possible outcomes when ordering 160 ounces? Secondly, is there another amount of fish that would give them a higher expected profit?\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering 160 ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to 250 dollars. So a better recommendation to the restaurant would be to inform them that when ordering 160 ounces, they will most likely get a profit of 250 dollars. There is a small risk of them making less that 100 dollars, but that they should not expect more than 250 dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is commonly known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population.\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz<-9/16 # Input\nPrice_Miso<-300 # Input\nEntry_Fee<-20 # Input\nFish_Entitled_Oz<-5 #Input\n\nProfits<-c()\n\nfor (i in Order_Oz){\nn<-10000\nV_Order_Oz<-rep(i,n)\nV_Price_Fish_Oz<-rep(Price_Fish_Oz,n)\nV_Price_Miso<-rep(Price_Miso,n)\nV_Entry_Fee<-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz<-rep(Fish_Entitled_Oz,n)\n\nV_Attendance<-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption<-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable<-V_Order_Oz # Calculated\n\nV_Profit<-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits<-c(Profits,mean(V_Profit))\n}\n\n(results<-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order Profits\n1   160 212.934\n2   170 225.413\n3   180 235.406\n4   190 241.335\n5   200 242.848\n6   210 243.837\n7   220 240.640\n8   230 234.759\n9   240 230.958"
  },
  {
    "objectID": "Simulation.html#flaw-of-averages",
    "href": "Simulation.html#flaw-of-averages",
    "title": "3  Simulation in R",
    "section": "3.5 Flaw Of Averages",
    "text": "3.5 Flaw Of Averages\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering \\(160\\) ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to \\(250\\) dollars. So a better recommendation to the restaurant would be to inform them that when ordering \\(160\\) ounces, they will most likely get a profit of \\(250\\) dollars. There is a small risk of them making less that \\(100\\) dollars in profit, but that they should not expect more than \\(250\\) dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population."
  },
  {
    "objectID": "Simulation.html#optimal-order-amount",
    "href": "Simulation.html#optimal-order-amount",
    "title": "3  Simulation in R",
    "section": "3.6 Optimal Order Amount",
    "text": "3.6 Optimal Order Amount\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a for loop. Below is the code:\n\nOrder_Oz=seq(160,300,10)\nPrice_Fish_Oz&lt;-9/16 # Input\nPrice_Miso&lt;-300 # Input\nEntry_Fee&lt;-20 # Input\nFish_Entitled_Oz&lt;-7 #Input\n\nProfits&lt;-c()\n\nfor (i in Order_Oz){\nn&lt;-100000\nV_Order_Oz&lt;-rep(i,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance&lt;-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption&lt;-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable&lt;-V_Order_Oz # Calculated\n\nV_Profit&lt;-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits&lt;-c(Profits,mean(V_Profit))\n}\n\n(results&lt;-data.frame(Order=Order_Oz,Profits=Profits))\n\n   Order   Profits\n1    160  66.62511\n2    170  88.31331\n3    180 108.85803\n4    190 127.86271\n5    200 144.83914\n6    210 159.74060\n7    220 171.74180\n8    230 181.43777\n9    240 188.65657\n10   250 193.78714\n11   260 196.97014\n12   270 198.27451\n13   280 198.15580\n14   290 196.51037\n15   300 193.80880\n\n\nThis table suggests that ordering \\(160\\) ounces is not optimal. Once again highlighting that the average attendance is not a good estimate of how much fish we should order. Instead, we can see that \\(210\\) ounces of fish should be ordered (this feeds about \\(42\\) people) to maximize the expected profits (\\(242\\) dollars)."
  },
  {
    "objectID": "Decisions.html#expected-value",
    "href": "Decisions.html#expected-value",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Expected Value",
    "text": "1.1 Expected Value\nImagine you are the CEO of a retail company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome.\nTo make an informed decision, you assess the probabilities of each outcome based on your market research, historical data, and/or expert opinions. For example, you might estimate a 60% chance of success and a 40% chance of failure.\nUsing this information, you can calculate the expected monetary value of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\n\nThe expected monetary value estimates the average monetary outcome you can expect from the decision. In this example, the expected value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EV\\) is the expected value, \\(p_{i}\\) is the probability of decision \\(i\\) and \\(x_{i}\\) is the monetary values resulting from decision \\(i\\). Notice that if we had other decisions, we could also calculate their expected value. Hence, the expected value allows us to rank and choose decisions that yield the highest monetary value on average."
  },
  {
    "objectID": "Decisions.html#decision-trees",
    "href": "Decisions.html#decision-trees",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.3 Decision Trees",
    "text": "1.3 Decision Trees\nDecision trees visually map the entire business decision process. Below you can see the decision tree for the decision to introduce a new production line.\n\n\n\n\ngraph LR\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( 0 )\n    C --&gt;|Success p=0.6| D( 5 )\n    C --&gt;|Failure p=0.4| E( -3 )\n\n\n\n\n\nDecision trees are read from left to right. Notice that there are two main branches stemming from the first decision node (i.e., the first square to the left). You can decide to Introduce or Not Introduce the new production line. If you Introduce, you reach a probability node (i.e., the circle that leads to the success and failure branches) where chance determines success or failure at the probabilities given. In the end, your choice of introducing the new product line yields an expected monetary value of 1.8 million, whereas not introducing the line yields no payoff.\nTo solve decision trees and find the optimal decision use backward induction. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision. Applying this procedure to our simple example, we would start with the probability node that leads to the success and failure branches and calculate the EMV of 1.8 million (refer to Section 1.2). Moving back to our initial decision, we now have the choice to Introduce or Not Introduce. In this case we should Introduce, since the resulting EMV (1.8) is higher than Not Introduce (0)."
  },
  {
    "objectID": "Decisions.html#bayes-theorem",
    "href": "Decisions.html#bayes-theorem",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.6 Bayes’ Theorem",
    "text": "1.6 Bayes’ Theorem\nNow we can use Bayes’ Theorem to update our probabilities given the recommendation from the consulting team. Recall that Bayes’ Theorem is a concept in probability that helps us update our beliefs or make better predictions when we get new information. Mathematically, Bayes’ Theorem states:\n\n\\(p(A|B)=\\frac{p(B|A)p(A)}{p(B)}\\)\n\nSubstituting values we can find the missing probabilities at the edge of the tree. In particular:\n\n\\(p(Success|+)=\\frac{p(+|Success)p(Success)}{p(+)}\\) \n\\(p(Success|+)=\\frac{0.95(0.6)}{0.63}\\) \n\\(p(Success|+)=0.90\\)\n\nThis implies that \\(p(Failure|+)=0.10\\). Similarly, the probabilities \\(p(Success|-)\\) and \\(p(Failure|-)\\) can be found using Bayes’ theorem (the reader should try to obtain these probabilities). The updated decision tree is given below:\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=0.93| D( 4.5 )\n    C --&gt;|Failure p=0.07| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=0.08| J( 4.5 )\n    H --&gt;|Failure p=0.92| K( -3.5 )"
  },
  {
    "objectID": "Decisions.html#readings",
    "href": "Decisions.html#readings",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.9 Readings",
    "text": "1.9 Readings\nReadings for this chapter are mainly from Winston and Albright (2019). Chapter 9 provides a good introduction to decision models with a couple of solved problems using excel. I would recommend using R to make calculations since we will be mainly using R. To construct decision trees in R you can use mermaid.\nSome concepts are important to review before you start reading the chapter. In particular, discrete random variables, expected value, conditional probability, probability rules, and Bayes’ theorem. These concepts are not explained in depth in the readings, so reviewing them beforehand could be helpful.\n\nWinston and Albright (2019) Chapters 9.1 (Introduction), 9.2 (Elements of Decision Analysis), 9.3 (Single-Stage Decision Problems) and 9.5 (Multistage Decision Problems). It is recommended you follow along in R as opposed to Excel (or Precision Tree Add-In).\nJaggia and Kelly (2022) Chapter 4.1 (Fundamental Probability Concepts), 4.2 (Rules of Probability), 4.3 (Contingency Tables and Probabilities), and 4.4 (The Total Probability Rule and Bayes’ Theorem)\nMermaid in Quarto Document: https://quarto.org/docs/authoring/diagrams.html\nMermaid in R Script: https://www.rdocumentation.org/packages/DiagrammeR/versions/1.0.10/topics/mermaid"
  },
  {
    "objectID": "Decisions.html#lessons-learned",
    "href": "Decisions.html#lessons-learned",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Lessons Learned",
    "text": "1.7 Lessons Learned\n\nUse the concept of Expected Monetary Value\nUse Decision Trees to map the business decision process\nApply the backward induction method to solve single-stage and multistage decision problems\n\n\n\n\n\nWinston, Wayne. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Simulation.html#lessons-learned-in-this-chapter",
    "href": "Simulation.html#lessons-learned-in-this-chapter",
    "title": "3  Simulation in R",
    "section": "3.9 Lessons Learned In This Chapter",
    "text": "3.9 Lessons Learned In This Chapter\n\nIdentify the parts of a simulation model.\nCreate a simulation model in R.\nIdentify the Flaw of Averages.\nFind optimal values in simulation models\n\n\n\n\n\nKwak, Young, and Lisa Ingall. 2007. “Exploring Monte Carlo Simulation Applications for Project Management.”\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Simulation.html#readings",
    "href": "Simulation.html#readings",
    "title": "3  Simulation in R",
    "section": "3.8 Readings",
    "text": "3.8 Readings\nThe readings for this chapter will introduce the application of simulation models to business. Winston and Albright (2019) mainly shows the technical application of the models using Excel. It is recommended that you follow in R instead. Kwak and Ingall (2007) motivates the use of Monte Carlo simulation for managing project risks and uncertainties.\n\nWinston and Albright (2019) Chapter 10.3 (Simulation and the Flaw of Averages), and 10.4 (Simulation with Built-In Excel Tools). Example 10.4 (Additional Uncertainty at Walton Bookstore) It is recommended you follow along using R instead of Excel.\nKwak and Ingall (2007)"
  },
  {
    "objectID": "Tools2.html#visualizing-the-data",
    "href": "Tools2.html#visualizing-the-data",
    "title": "4  Time Series Tools",
    "section": "4.5 Visualizing The Data",
    "text": "4.5 Visualizing The Data\nTo visualize the data, we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines the mapping of variables. The geom_line() function specifies the type of plot. In time series, we will use the line plot regularly. Labels for the graph are easily set with the labs function and there are plenty of themes available to customize your visualization. Below, the theme_classic() is displayed. To learn more about the ggplot package, you can refer to Wickham (2017) chapter 2. Below is the code to create a line plot of California’s average avocado price.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price, group=geography),color=\"black\") +\n  theme_classic() + \n  labs(x=\"\",\n       y=\"Average Price\", \n       title=\"Organic Avocado Price in California\",\n       subtitle=\"2015-2018\")  \n\n\n\n\nThe average price of avocados in California has been increasing during the period considered. It reached a maximum of about 2.6 in 2017 and was at a minimum in the spring of 2015. There is also a seasonal pattern with low prices at the beginning of the year and peaks mid-year. As we will see in upcoming chapters, these are patterns that can be extrapolated and used to forecast time series."
  },
  {
    "objectID": "Tools2.html#setting-up-the-date-variable",
    "href": "Tools2.html#setting-up-the-date-variable",
    "title": "4  Time Series Tools",
    "section": "4.3 Setting Up the Date Variable",
    "text": "4.3 Setting Up the Date Variable\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\""
  },
  {
    "objectID": "Tools2.html#tssibbles-and-decomposition",
    "href": "Tools2.html#tssibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.4 tssibbles and Decomposition",
    "text": "4.4 tssibbles and Decomposition\n\nlibrary(fpp3, quietly = T, warn.conflicts = F)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tsibble     1.1.3     ✔ feasts      0.3.0\n✔ tsibbledata 0.4.1     ✔ fable       0.3.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-31\")-> avocadots\n\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) -> calits\n\ncalits  %>% mutate(\n  `10-MA`=slider::slide_dbl(average_price,mean,\n                            .before=5, .after=4,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`10-MA`), col=\"red\")\n\nWarning: Removed 9 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits  %>% mutate(\n  `2X10-MA`=slider::slide_dbl(`10-MA`,mean,\n                           .before=1, .after=0,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`2X10-MA`), col=\"red\")\n\nWarning: Removed 10 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-31\") %>%\n  model(STL(average_price~trend(window=150)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()"
  },
  {
    "objectID": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "href": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "title": "4  Time Series Tools",
    "section": "4.3 Loading tidyverse and Inspecting the Data.",
    "text": "4.3 Loading tidyverse and Inspecting the Data.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado&lt;-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe code above imports the data as a tibble (a data structure similar to a data frame) and saves it in an object named avocado. The output informs us that three variables are classified as a character, while the rest are double. You can preview the data with either the spec() or glimpse() commands.\n\nspec(avocado)\n\ncols(\n  date = col_character(),\n  average_price = col_double(),\n  total_volume = col_double(),\n  `4046` = col_double(),\n  `4225` = col_double(),\n  `4770` = col_double(),\n  total_bags = col_double(),\n  small_bags = col_double(),\n  large_bags = col_double(),\n  xlarge_bags = col_double(),\n  type = col_character(),\n  year = col_double(),\n  geography = col_character()\n)\n\n\nYou will notice that the date variable is of type character. We can use the lubridate package to coerce this variable to a date. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function. You can learn more about this package in Wickham (2017) Chapter 19.\n\nlibrary(lubridate)\navocado$date&lt;-mdy(avocado$date)"
  },
  {
    "objectID": "Tools2.html#piping-and-dplyr",
    "href": "Tools2.html#piping-and-dplyr",
    "title": "4  Time Series Tools",
    "section": "4.4 Piping and dplyr",
    "text": "4.4 Piping and dplyr\ndplyr is commonly used with “piping”. Generally speaking, “piping” allows us to chain functions. “Piping” (%&gt;% or |&gt;) passes the object on the left of the pipe as the first argument in the function to the right of the pipe. We can illustrate this using the select() and arrange() functions.\n\navocado %&gt;% select(c(average_price,geography)) %&gt;%\n  arrange(desc(average_price)) %&gt;% head(5)\n\n# A tibble: 5 × 2\n  average_price geography           \n          &lt;dbl&gt; &lt;chr&gt;               \n1          3.25 San Francisco       \n2          3.17 Tampa               \n3          3.12 San Francisco       \n4          3.05 Miami/Ft. Lauderdale\n5          3.04 Raleigh/Greensboro  \n\n\nThere is a lot to explain in this line of code. Let’s start with the functions used. The select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data in ascending order. The desc() function is used to sort in descending order.\nLet’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%&gt;%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. The data frame is sorted in descending order so that the highest average avocado price is displayed first. Finally, the head() function is used to retrieve the top five entries.\nAs noted in Section 4.2, there is an additional date in the data set that is between weeks (2018-01-01). We can remove this observation by using the filter() function.\n\navocado %&gt;% filter(date!=ymd(\"2018-01-01\")) -&gt; avocado\n\nYou should notice that whereas the select() function chooses particular variables (i.e., columns), the filter() function chooses rows of the tibble that meet the conditions listed. Note also that the filtered data set is assigned (-&gt;) to avocado overwriting the older object.\nThe examples above highlight the use of dplyr functions to transform your data. There are plenty of other functions, but learning these are outside the scope of this book. To find out more, I recommend reading Wickham (2017) Chapter 4. For now, we will use one more data transformation technique to retrieve California’s average price of organic avocados for 2015-2018.\n\navocado %&gt;% \n  filter(geography==\"California\", type==\"organic\", year&lt;=2018) %&gt;% \n  select(date, average_price, geography) -&gt; cali"
  },
  {
    "objectID": "Tools2.html#tsibbles-and-decomposition",
    "href": "Tools2.html#tsibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.5 tsibbles and Decomposition",
    "text": "4.5 tsibbles and Decomposition\nAll though tibbles are a great data structure, when dealing with time series, there is a time component that is crucial when analyzing the data. As a consequence, we will be using a data structure called a tsibble (time series tibble). tsibbles are defined by a time index (i.e., the date), and some keys (i.e., some dimensions). In the avocado data set we are mainly interested in the average price of the avocados. You will note however that they are classified by location (geography) and type (e.g., organic and conventional). tsibbles, as well a variety of packages that help us analyze time series, are part of the fpp3 package. Below we load the package, and coerce our avocado tibble to a tsibble.\n\nlibrary(fpp3)\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-> avocadots\n\nNote that the as_tsibble() function was called with the parameter regular set at true indicating that the date has no gaps and occurs every week. The function filter_index() is called as it allows us to determine the window for analysis. As noted in section 5.1, there are some missing dates in December 2018. We limit the analysis to 2015-2018.\nRecall, that we are interested in the average price of avocados for califronia. We can specify the tsibble for analysis, by using dplyr.\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) -> calits"
  },
  {
    "objectID": "Tools2.html#decomposition",
    "href": "Tools2.html#decomposition",
    "title": "4  Time Series Tools",
    "section": "4.6 Decomposition",
    "text": "4.6 Decomposition\nAs mentioned above, the avocado data for California seems to have a trend and a seasonal pattern. There are methods available to tease out these components. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run in R by using the command below:\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-02\") %>%\n  model(STL(average_price~trend(window=200)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()\n\n\n\n\nAs shown above, the trend is increasing and the seasonal component confirms low levels at the beginning of the year and high levels in the summer.\nThe decomposition itself is constructed by first finding a moving average of the series to track the trend. As you can see the window in the trend is set to a high window so that moving average tracks the general trend and not the small fluctuations of the series. This trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly). The error is the remaining fluctuation of the series that is not explained by the trend or the seasonal component (Series-Trend-Seasonal=Error)."
  },
  {
    "objectID": "Benchmarks.html#predicting-the-avocado-data",
    "href": "Benchmarks.html#predicting-the-avocado-data",
    "title": "5  Model Benchmarks",
    "section": "5.2 Predicting the Avocado Data",
    "text": "5.2 Predicting the Avocado Data\nLet’s apply the forecasting methods to the average prices for avocados in California. Start by loading the fpp3 package and importing the data https://jagelves.github.io/Data/CaliforniaAvocado.csv.\n\nlibrary(tidyverse)\nlibrary(fpp3)\ncali<-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\nRecall, that we can create a tsibble from the csv file by using the as_tsibble() function.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) -> calits\n\nThe model() function can run the models discussed in section 5.1. We save this to an object called fit.\n\nfit <- model(calits,mean=MEAN(average_price),\n              Naive=NAIVE(average_price),\n              Drift=RW(average_price~drift()),\n              LS=TSLM(average_price~date))\n\nTo explore the model coefficients, we can use the coef() function. We have used the gt package to make the table visually appealing.\n\n\n\n\n\n\n  \n    \n      Model Coefficients For The Avocado Data\n    \n    \n  \n  \n    \n      geography\n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    California\nmean\nmean\n1.70\n0.02\n89.85\n0.00\n    California\nDrift\nb\n0.00\n0.01\n0.50\n0.62\n    California\nLS\n(Intercept)\n−2.94\n0.72\n−4.11\n0.00\n    California\nLS\ndate\n0.00\n0.00\n6.49\n0.00\n  \n  \n  \n\n\n\n\n\ncalits %>% autoplot(average_price) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %>% filter(`.model`==\"LS\"))"
  },
  {
    "objectID": "Tools2.html#tsibbles",
    "href": "Tools2.html#tsibbles",
    "title": "4  Time Series Tools",
    "section": "4.6 tsibbles",
    "text": "4.6 tsibbles\nWhen dealing with time series, time plays an important role. As a consequence, we will be using a data structure called a tsibble (time series tibble). tsibbles are defined by a time index (i.e., the date) that has a common interval (i.e., days, weeks, etc.), and keys (i.e., dimensions or observational units). In the avocado data set we are mainly interested in the average price of the avocados. You will note however that they are classified by location (geography) and type (organic and conventional). You can learn more about tsibbles here.\ntsibbles, as well a variety of packages that help us analyze time series, are part of the fpp3 package. Below we load the package, and coerce our avocado tibble to a tsibble.\n\nlibrary(fpp3)\navocado %&gt;%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-&gt; avocadots\n\nIn the code above, the as_tsibble() function was called with the parameter regular set at true indicating that the date has no gaps and occurs every week (the greatest common divisor of the index column). The function filter_index() is called as it allows us to determine the window for analysis. As noted in Section 4.2, there are some missing dates in December 2018. We limit the analysis for now to 2015-2018.\nRecall, that we are interested in the average price of avocados for California. We can specify the tsibble for analysis, by using dplyr.\n\navocadots %&gt;% filter(geography==\"California\", type==\"organic\") %&gt;%\n  select(date,geography,average_price) -&gt; calits"
  },
  {
    "objectID": "Tools2.html#time-series-decomposition",
    "href": "Tools2.html#time-series-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.7 Time Series Decomposition",
    "text": "4.7 Time Series Decomposition\nAs highlighted in Section 4.5, the average price of avocados in California seems to have a trend and a seasonal pattern. There are methods available to tease out these components and make them more apparent. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run this method in R by using the fable package.\nIn practice, the decomposition is constructed in several steps. First, a moving average of the series is calculated to track the trend. The trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly) for the de-trended series. The error is the remaining fluctuation of the series that is not explained by the trend or the seasonal component (\\(Series-Trend-Seasonal=Error\\)). In the end, each component can be graphed and displayed, as illustrated below.\n\ncalits %&gt;%\n  model(STL(average_price~trend(window=200)+\n              season(window=52), robust=TRUE)) %&gt;%\n  components() %&gt;% autoplot()+ theme_classic()\n\n\n\n\nfable allows us to construct this model easily by using the model() function. This function will allow us to estimate a variety of time series models, so we will be using it regularly. The particular model we are running is STL, hence the use of the STL() function within the model() function. We define the model by specifying the dependent variable (i.e., average_price) followed by a tilde (~) and the components. As you can see, the window argument in the trend() function is set to a relatively large value and the season() function specifies \\(52\\) weeks to capture the yearly seasonality. By doing this the moving average reflects the general direction of the series and avoids the small fluctuations of the data. The robust parameter is set to true to make the fitting process less sensitive to outliers or influential data points.\nAs shown above, the trend is increasing, and the seasonal component confirms low levels at the beginning of the year and high levels in the summer. These are two general patterns that determine the price of avocados in California and provide valuable insight to share with Chilango’s Kitchen."
  },
  {
    "objectID": "ETSARIMA.html#preliminaries",
    "href": "ETSARIMA.html#preliminaries",
    "title": "6  ETS and ARIMA",
    "section": "6.1 Preliminaries",
    "text": "6.1 Preliminaries\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and autocovariance, regardless of the time at which the series is observed.\nThe main reason for making the time series stationary is that it is required in many time series models (including the ARIMA model). These models make predictions about future values of the time series based on past values, and the statistical properties of the past values are used to inform these predictions. If the statistical properties of the time series are changing over time, then the models may not work well, as the assumptions underlying them would not be met.\nAdditionally, non-stationary time series may contain trends and/or seasonal patterns, which can make it difficult to model and forecast the series. By removing these patterns and making the series stationary, it becomes easier to model and forecast.\nIn general, before modeling and forecasting, we will check whether the series has a trend or homoskedasticity. To eliminate the trend in the series we will us the first difference of the series. We can do this in R by using the diff() function. For example consider Tesla’s quarterly vehicle deliveries from 2016-2022.\n\n\n\n\n\nDeliveries have mostly been in an upward trend, which makes sense as the company is currently growing. This series seems to not be stationary since it crosses the mean once and never revisits it. An easy way to make the series stationary (make it fluctuate around the mean) is to find the first difference. Below is the graph of the integrated series.\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% autoplot(difference(deliveries)) + theme_classic()\n\n\n\n\nNote how the series now fluctuates around the mean of zero. That is the change of the vehicles from quarter to quarter sometimes increases and sometimes decreases and on average their is no change. You will note that the series does exhibit some heteroskedasticity as the variance of the series seems to be low from the period of 2016-2018 and significanlty increases after. To normalize the variance of the series we can make a Box-Cox transormation.\n\nlambda <- deliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = guerrero) %>% pull(lambda_guerrero) \n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic()\n\n\n\n\nAs you can see this series is a bit more homoskedastic than the series without the transformation.\nTo test whether a series is stationary or not we can use the unitroot_kpss feature. In general, a low p-value allows us to reject the null of hypothesis of stationarity.\n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = c(unitroot_kpss)) %>% gt()\n\n\n\n\n\n  \n  \n    \n      kpss_stat\n      kpss_pvalue\n    \n  \n  \n    0.9465391\n0.01\n  \n  \n  \n\n\n\n\nThis confirms that Tesla deliveries are non-stationary.\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\nWe can use this metric to identify which periods are influential for our targeted forecast periods. As a consequence, we can illustrate a function of a series and it’s correlation with its lags to identify/quantify crucial periods. To construct an autocorrelation function (ACF) start by loading the data and coercing the period variable to a date.\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndeliveries<-read_csv(\"https://jagelves.github.io/Data/tsla_deliveries.csv\")\n\ndeliveries$period<-yearquarter(deliveries$period)\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% ACF(lag_max = 12, deliveries) %>% autoplot()+theme_classic()\n\n\n\n\nThe plot shows that the the correlation of the series with its first lag strongest, and that there is continuous decay as the lags get larger.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% PACF(lag_max = 12, deliveries) %>% autoplot()+theme_classic()\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag when we net the effect. In other words, if lag 2 or 3 seemed to have been correlated with the series (see ACF), it was mainly because the influence of lag 1 on the series."
  },
  {
    "objectID": "ETSARIMA.html#the-ar-model",
    "href": "ETSARIMA.html#the-ar-model",
    "title": "6  ETS and ARIMA",
    "section": "6.2 The ar model",
    "text": "6.2 The ar model"
  },
  {
    "objectID": "ETSARIMA.html#the-ma-model",
    "href": "ETSARIMA.html#the-ma-model",
    "title": "6  ETS and ARIMA",
    "section": "6.3 The ma model",
    "text": "6.3 The ma model"
  },
  {
    "objectID": "ETSARIMA.html#the-arima-model",
    "href": "ETSARIMA.html#the-arima-model",
    "title": "6  ETS and ARIMA",
    "section": "6.4 The arima model",
    "text": "6.4 The arima model"
  },
  {
    "objectID": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "href": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "title": "5  Model Benchmarks",
    "section": "5.2 Modeling the the Average Price of Avocados",
    "text": "5.2 Modeling the the Average Price of Avocados\nLet’s apply these four models to forecast the average price of avocados in California. We’ll start by loading the tidyverse and fpp3 packages and importing the data.\n\nlibrary(tidyverse)\nlibrary(fpp3)\ncali&lt;-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\nRecall that we can create a tsibble from the csv file using the as_tsibble() function. The index argument is set to the weekly date variable and the key argument to geography. The filter_index() function is used to focus our analysis for 2015-01-04~2018-06-02 with 2018-06-02 not being included.\n\ncali %&gt;%\n  as_tsibble(key=geography,index=date,regular=T) %&gt;%\n  filter_index(\"2015-01-04\"~\"2018-06-02\") -&gt; calits_train\n\nNow we can use the model() function to run the benchmarks discussed in Section 5.1. We have saved the models to an object called fit.\n\nfit &lt;- model(calits_train,mean=MEAN(average_price),\n              Naive=NAIVE(average_price),\n              Drift=RW(average_price~drift()),\n              LS=TSLM(average_price~date))\n\nThe fit object is saved as a mable (model table). The model()function specifies the four models to be estimated using their respective functions (i.e., MEAN(), NAIVE(), RW(), and TSLM()). To explore the coefficients of the models estimated, we use the coef() function with fit as its single argument. The output table has been enhanced visually by using the gt package.\n\nlibrary(gt)\ncoef(fit) %&gt;% \n  gt() %&gt;% \n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Coefficients For The Avocado Data**\")) %&gt;% \n  tab_style(\n    locations =cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(statistic,estimate,std.error,p.value),\n             decimals = 4)\n\n\n\n\n\n  \n    \n      Model Coefficients For The Avocado Data\n    \n    \n    \n      geography\n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    California\nmean\nmean\n1.6845\n0.0212\n79.3980\n0.0000\n    California\nDrift\nb\n0.0021\n0.0104\n0.2010\n0.8410\n    California\nLS\n(Intercept)\n−3.7322\n0.9230\n−4.0438\n0.0001\n    California\nLS\ndate\n0.0003\n0.0001\n5.8702\n0.0000\n  \n  \n  \n\n\n\n\nThe table records the estimates and p-values for all the benchmarks discussed in Section 5.1. The Naive model has no entry, as the forecast is created by using the previous period’s observed value. Note as well that the Drift and LS models select a positive slope to account for the trend. Below we illustrate the fit of the Mean model by a dashed blue line, the Least Squares model by the red line and the Naive model by the orange line.\n\ncalits_train %&gt;% autoplot(average_price) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %&gt;% filter(`.model`==\"LS\")) +\ngeom_line(aes(y = .fitted), col=\"orange\",\n            data = augment(fit) %&gt;% filter(`.model`==\"Naive\")) +\n  geom_line(aes(y = .fitted), col=\"blue\", linetype=\"dashed\",\n            data = augment(fit) %&gt;% filter(`.model`==\"mean\")) +\n  labs(y=\"\", title= \"California's Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - May 27, 2018\",\n       x=\"\")\n\n\n\n\nThe graph illustrates how closely the Naive model follows the data. This might seem like a good model, but consider how the heuristic makes a mistake every period. Since average prices are constantly changing every week, predicting the previous value always results in an error. Critically, the Naive prediction does not explain the series governing process. The LS model, on the other hand, provides some insight into a force that is influencing the data—a rising trend. We can use characteristics such as a trend or seasonality to forecast a series effectively."
  },
  {
    "objectID": "Benchmarks.html#model-fit",
    "href": "Benchmarks.html#model-fit",
    "title": "5  Model Benchmarks",
    "section": "5.3 Model Fit",
    "text": "5.3 Model Fit\nThe model fit will be assessed by comparing the fitted values against observed values. In general, a good fit is determined by how far the fitted values are from the observed ones. If we square all of the distances between actual points and predicted values (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE).\n\n\\(MSE = \\frac{ \\sum (\\hat{y}_t-y_t)^2}{T}\\)\n\nHow we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE’s but instead find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since all the accuracy metrics are the smallest. We highlighted these results and made the table more appealing using the gt library.\n\naccuracy(fit) %&gt;% \n  gt() %&gt;%\n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Fit**\")) %&gt;% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %&gt;% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"Naive\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n    \n      geography\n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    California\nmean\nTraining\n0.00\n0.28\n0.23\n−2.71\n13.59\n2.26\n2.05\n0.87\n    California\nNaive\nTraining\n0.00\n0.14\n0.10\n−0.19\n6.04\n1.00\n1.00\n−0.21\n    California\nDrift\nTraining\n0.00\n0.14\n0.10\n−0.31\n6.05\n1.00\n1.00\n−0.21\n    California\nLS\nTraining\n0.00\n0.26\n0.21\n−2.21\n12.23\n2.07\n1.87\n0.85"
  },
  {
    "objectID": "Benchmarks.html#forecast",
    "href": "Benchmarks.html#forecast",
    "title": "5  Model Benchmarks",
    "section": "5.4 Forecast",
    "text": "5.4 Forecast\nThe forecast of the series is obtained by using the forecast() function and specifying the number of periods (\\(h\\)) ahead to forecast. Below we forecast \\(27\\) weeks and save the result in an object called calits_fc.\n\ncalits_fc &lt;- fit %&gt;% forecast(h=27)\n\nThe autoplot() and autolayer() functions are used below to create a graph with the forecasts and the training set. The argument level is set to NULL to omit the prediction intervals.\n\ncalits_fc %&gt;% autoplot(level=NULL) + theme_classic() + \n  autolayer(calits_train, average_price) +\n  labs(y=\"\", title= \"California's Forecasted Average Price Of Avocados\",\n       subtitle = \"Jan 4, 2015 - Dec 2, 2018\",\n       x=\"\")\n\n\n\n\nNote how the Mean and Naive models predict that the series will continue without a trend. The LS and Drift models predicts that the series will continue its trend but, like all other methods, do not consider the seasonal pattern discussed in Section 4.5. In future chapters, we will look at models that account for both trend and seasonality."
  },
  {
    "objectID": "ETSARIMA.html#the-arp-model",
    "href": "ETSARIMA.html#the-arp-model",
    "title": "6  ETS and ARIMA",
    "section": "6.2 The AR(p) model",
    "text": "6.2 The AR(p) model\nIn the previous section we identified the deliveries of Tesla cars to have a decaying ACF and a single significant spike (at lag 1) in the PACF. This pattern can be generated by an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model just uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R:\n\ny<-c(0)\nphi<-0.7\nconst<-1\nnrep<-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% ACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the deliveries of Tesla. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% PACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nOnce again, if you compare the PACF to the one in the deliveries of Tesla you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero."
  },
  {
    "objectID": "ETSARIMA.html#the-maq-model",
    "href": "ETSARIMA.html#the-maq-model",
    "title": "6  ETS and ARIMA",
    "section": "6.3 The MA(q) model",
    "text": "6.3 The MA(q) model"
  },
  {
    "objectID": "ETSARIMA.html#the-arimapiq-model",
    "href": "ETSARIMA.html#the-arimapiq-model",
    "title": "6  ETS and ARIMA",
    "section": "6.4 The ARIMA(p,i,q) model",
    "text": "6.4 The ARIMA(p,i,q) model"
  },
  {
    "objectID": "Benchmarks.html#overfitting",
    "href": "Benchmarks.html#overfitting",
    "title": "5  Model Benchmarks",
    "section": "5.5 Overfitting",
    "text": "5.5 Overfitting\nOverfitting can happen when a model is overly flexible and complex. This can lead to the model fitting to the random fluctuations or noise in the data, rather than the underlying pattern.\nTo overcome this problem, we usually have a training set or subset of the data that we use to estimate the model’s parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the test set.\nRecall that our avocado price models were estimated for the period between 2015-01-04~2018-06-02. We will call this our training set. For our test set we’ll use the 2018-06-02~2018-12-02 period. The code below creates the test set.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) %>%\n  filter_index(\"2018-06-02\"~\"2018-12-02\")-> calits_test\n\nNow we can plot the training set, the forecast, and the test set with by using the code below.\n\ncalits_fc %>% autoplot(level=NULL) + theme_classic() + \n  autolayer(calits_train, average_price) + autolayer(calits_test, average_price)\n\n\n\n\nThe graph shows how the LS method does well with the test data. This can be confirmed by obtaining the accuracy measures against the test set.\n\n\n\n\naccuracy(calits_fc, calits) %>% gt() %>%\n  cols_align(\"center\") %>% \n  tab_header(title = md(\"**Model Fit**\")) %>% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %>% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"LS\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      .model\n      geography\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nCalifornia\nTest\n0.17\n0.21\n0.17\n9.24\n9.24\n1.74\n1.55\n0.52\n    LS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n1.37\n1.14\n0.52\n    mean\nCalifornia\nTest\n0.13\n0.18\n0.13\n6.72\n6.85\n1.31\n1.31\n0.50\n    Naive\nCalifornia\nTest\n0.20\n0.24\n0.20\n10.85\n10.85\n2.03\n1.74\n0.50\n  \n  \n  \n\n\n\n\nNote also that the Naive method although good, is no longer the best model. This is partly because it chases the series fluctuations closely and always makes a mistake when forecasting one period ahead."
  },
  {
    "objectID": "ARIMA.html#preliminaries",
    "href": "ARIMA.html#preliminaries",
    "title": "7  ARIMA",
    "section": "7.1 Preliminaries",
    "text": "7.1 Preliminaries\n\nWhite Noise\nIn time series, white noise refers to a sequence of random data points that have no correlation to each other. This process is used as a benchmark for other types of time series data that exhibit patterns or trends. By comparing a series with the white noise process, we can verify if the series has systematic components that can be modeled.\nWe can generate a white noise process by using the normal distribution with a mean of zero and a constant variance. Below we create a tsibble with the simulated data.\n\nlibrary(fpp3)\nset.seed(10)\nwn&lt;-tsibble(x=rnorm(100),period=seq(1:100),index=period)\n\nWe can now use the autoplot() function to observe the white noise process.\n\nwn %&gt;% autoplot(x) + theme_classic() + \n  labs(title=\"White Noise Process\",\n       subtitle=\"Mean=0 and Standard Deviation=1\",\nx=\"\",y=\"\") + \n  geom_hline(yintercept = 0, col=\"blue\", lwd=1, linetype=\"dashed\",\n             alpha=0.4)\n\n\n\n\nA very ragged pattern is shown in the graph above. The series behaves erratically, but it always fluctuates around a mean of 0 and keeps a standard deviation of one. Such a series is unpredictable, so the best one can do is to describe it by its mean and standard deviation.\n\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and auto-covariance, regardless of the time at which the series is observed.\nThe main reason for making the time series stationary is that it is required in many time series models (including the ARIMA model). These models make predictions about future values of the time series based on past values, and the statistical properties of the past values are used to inform these predictions. If the statistical properties of the time series are changing over time, then the models may not work well, as the assumptions underlying them would not be met.\nIn general, before modeling and forecasting, we will check whether the series is stationary (i.e., has no trend and is homoskedastic). To eliminate the trend in the series we will use the first difference of the series. We can do this in R by using the difference() function. For example consider Tesla’s quarterly vehicle deliveries from 2016-2022.\n\n\n\n\n\nDeliveries have mostly been in an upward trend, which makes sense as the company is currently scaling its production. This series seems to not be stationary since it crosses the mean (blue line) once and never revisits it. That is, the mean is not constant and changes with time. It is possible to make the series stationary by finding differences. Below is the graph of the first difference of the series.\n\ndeliveries %&gt;%\n  as_tsibble(index=period, regular=T) %&gt;% autoplot(difference(deliveries)) + theme_classic() +\n  labs(title=\"Change In Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\") + \n  geom_hline(yintercept = mean(difference(deliveries$deliveries), na.rm = TRUE), col=\"blue\", linetype=\"dashed\", lwd=1, alpha=0.4)\n\n\n\n\nThe series now fluctuates closer to the mean, but unlike the white noise process behaves less erratic. You will notice that in some periods the change in deliveries from quarter to quarter is high. For example, following the lows at the beginning of the year, deliveries seem to increase sharply. There seems to be correlations between the quarters or time dependencies. We will explore this more once we look at autocorrelations, partial autocorrelations, and seasonality.\nThe series exhibits heteroskedasticity (increasing variance), as the variance of the series seems to be low from the period of 2016-2018 while significantly higher for the period after. To normalize the variance of the series we can conduct a Box-Cox transformation.\n\nlambda &lt;- deliveries %&gt;% as_tsibble(index=period, regular=T) %&gt;%\n  features(deliveries, features = guerrero) %&gt;% pull(lambda_guerrero) \n\ndeliveries %&gt;% as_tsibble(index=period, regular=T) %&gt;%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic() +\n  labs(title=\"Box-Cox Transformation of Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\")\n\n\n\n\nThe transformation has made the series a bit more homoskedastic (variance is more uniform) than the series without the transformation. Note also that both transformations (differencing and box-cox) can be undone by using an inverse function, so that we can return to the delivery of vehicles.\nA couple of statistical features to determine the stationarity of a series are the unitroot_kpss and unitroot_ndiffs. In general, a low p-value allows us to reject the null of hypothesis of stationarity.\n\ndeliveries %&gt;% as_tsibble(index=period, regular=T) %&gt;%\n  features(deliveries, features = c(unitroot_kpss, unitroot_ndiffs)) %&gt;% gt() %&gt;%\n  cols_align(\"center\") %&gt;% \n  tab_header(title = \n               md(\"**Stationarity Tests**\")) %&gt;% tab_style(locations =                                                    cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(kpss_stat,kpss_pvalue),\n             decimals = 3)\n\n\n\n\n\n  \n    \n      Stationarity Tests\n    \n    \n    \n      kpss_stat\n      kpss_pvalue\n      ndiffs\n    \n  \n  \n    0.947\n0.010\n2\n  \n  \n  \n\n\n\n\nThe test reports a p-value of 0.01 when it is below 0.01 and 0.1 when it is above 0.1. Hence, the p-value confirms that Tesla deliveries are non-stationary and that two differences are required to make the data stationary. Two differences might be required as Tesla’s deliveries seem to be growing exponentially.\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\nWe can use this metric to identify which periods are influential for our targeted forecast periods. We can also illustrate a function of a series and it’s correlation with its lags to identify/quantify crucial periods. This time let’s inspect personal income growth in the state of California. Below we load the data and create the train and test sets.\n\nlibrary(fpp3)\nlibrary(tidyverse)\nPI&lt;-read_csv(\"https://jagelves.github.io/Data/PersonalIncome.csv\")\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(1970~2005) -&gt; PI_train\nPI %&gt;% as_tsibble(index=Date) %&gt;% \n  filter_index(2006~2021) -&gt; PI_test\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\nPI_train %&gt;%\n  ACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"\",\n                                 title=\"ACF Personal Income Growth in California\")\n\n\n\n\nThe plot shows that the correlation of the series with its first lag is strongest, and that there is continuous decay in the strength of the correlation as the lags get larger. The blue lines determine which autocorrelations are statistically different from zero (significant) at the 5% level. As you can see, lags 1-4 are positively correlated with the series and statistically significant.\nA white noise process on the other hand is expected to show no correlation with its lags since the series is constructed from independent draws from a normal distribution with constant variance. Below you can see the autocorrelation function of the white noise process.\n\nwn %&gt;% ACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"ACF\") +\nlabs(x=\"\", y=\"\", title=\"ACF White Noise Process\")\n\n\n\n\nInterestingly, lag 14 shows a positive correlation with the series. It is important to note that correlations can happen by chance even if we construct the series from a random process.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) (the correlation) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\nPI_train %&gt;%\n  PACF(lag_max = 12,PI_Growth) %&gt;% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF Personal Income Growth In California\")\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag. Specifically, lag 2, 3, and 4 seemed to have been correlated with the series (see ACF), but this was mainly because of the influence of lag 1.\nLet’s inspect the white noise process once more to confirm that there are no patterns.\n\nwn %&gt;% PACF(x) %&gt;% autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF White Noise Process\")\n\n\n\n\nIn sum, white noise processes are unpredictable and we can only describe them by their mean and standard deviation. Series that have patterns in their ACF or PACF can be modeled. Below we I illustrate how to model Personal Income in California with an AR(1) model."
  },
  {
    "objectID": "ARIMA.html#the-arp-model",
    "href": "ARIMA.html#the-arp-model",
    "title": "6  ARIMA",
    "section": "6.2 The AR(p) model",
    "text": "6.2 The AR(p) model\nIn the previous section, we identified the growth of personal income in California to have a decaying ACF and a single significant spike (at lag 1) in the PACF. These results can be generated with an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model just uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R.\n\ny<-c(0)\nphi<-0.7\nconst<-1\nnrep<-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like for a AR(1) process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% ACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the deliveries of Tesla. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% PACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nOnce again, if you compare the PACF to the one in personal income growth you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero. We can model personal consumption growth with an AR(1) process."
  },
  {
    "objectID": "ARIMA.html#the-maq-model",
    "href": "ARIMA.html#the-maq-model",
    "title": "6  ARIMA",
    "section": "6.5 The MA(q) model",
    "text": "6.5 The MA(q) model"
  },
  {
    "objectID": "ARIMA.html#the-arimapiq-model",
    "href": "ARIMA.html#the-arimapiq-model",
    "title": "6  ARIMA",
    "section": "6.6 The ARIMA(p,i,q) model",
    "text": "6.6 The ARIMA(p,i,q) model"
  },
  {
    "objectID": "ARIMA.html#forecast-and-residuals",
    "href": "ARIMA.html#forecast-and-residuals",
    "title": "6  ARIMA",
    "section": "6.3 Forecast and Residuals",
    "text": "6.3 Forecast and Residuals\nLet’s forecast the deliveries of Tesla using the AR(1) model. We can do this by first creating a train set and using the model() function.\n\ntesla_fit<-deliveries_train %>% \n  model(AR1 = AR(deliveries ~ order(1)),\n        LS=TSLM(deliveries~period),\n        Drift=RW(deliveries~drift()))\n\n\n\n\n\n\n\n  \n    \n      Model Coefficients For Tesla Deliveries\n    \n    \n  \n  \n    \n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    AR1\nar1\n1.15\n0.04\n29.53\n0.00\n    LS\n(Intercept)\n−1,913.59\n196.57\n−9.73\n0.00\n    LS\nperiod\n0.11\n0.01\n10.21\n0.00\n    Drift\nb\n12.77\n4.76\n2.68\n0.01\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    AR1\nTraining\n−0.12\n19.86\n14.64\n−4.05\n18.53\n0.34\n0.34\n−0.04\n    LS\nTraining\n0.00\n32.62\n24.91\n11.26\n53.66\n0.57\n0.56\n0.55\n    Drift\nTraining\n0.00\n22.35\n17.52\n−17.42\n28.78\n0.40\n0.38\n0.17\n  \n  \n  \n\n\n\n\n\ntesla_fit %>% forecast(new_data=deliveries_test)\n\nNew names:\nNew names:\nNew names:\nNew names:\n• `...1` -> `...4`\n\n\n# A fable: 12 x 5 [1Q]\n# Key:     .model [3]\n   .model  period   deliveries .mean  ...5\n   <chr>    <qtr>       <dist> <dbl> <dbl>\n 1 AR1    2022 Q1  N(356, 394)  356.    25\n 2 AR1    2022 Q2  N(411, 920)  411.    26\n 3 AR1    2022 Q3 N(475, 1621)  475.    27\n 4 AR1    2022 Q4 N(549, 2555)  549.    28\n 5 LS     2022 Q1 N(221, 1368)  221.    25\n 6 LS     2022 Q2 N(231, 1393)  231.    26\n 7 LS     2022 Q3 N(241, 1422)  241.    27\n 8 LS     2022 Q4 N(252, 1452)  252.    28\n 9 Drift  2022 Q1  N(321, 545)  321.    25\n10 Drift  2022 Q2 N(334, 1135)  334.    26\n11 Drift  2022 Q3 N(347, 1770)  347.    27\n12 Drift  2022 Q4 N(360, 2451)  360.    28"
  },
  {
    "objectID": "ARIMA.html#modeling-and-residuals",
    "href": "ARIMA.html#modeling-and-residuals",
    "title": "7  ARIMA",
    "section": "7.3 Modeling and Residuals",
    "text": "7.3 Modeling and Residuals\nLet’s model personal consumption using the AR(1) model. We’ll also estimate a Least Squares model to compare. Recall, that we can estimate these models by using the model() function and retrieve the coefficients with the coef() function.\n\nPI_fit&lt;-PI_train %&gt;% \n  model(AR1 = AR(PI_Growth ~ order(1)),\n        LS = TSLM(PI_Growth ~ trend()))\n\n\n\n\n\n\n\n  \n    \n      Model Coefficients For PI Growth\n    \n    \n    \n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    AR1\nconstant\n1.80\n0.82\n2.19\n0.04\n    AR1\nar1\n0.70\n0.12\n5.80\n0.00\n    LS\n(Intercept)\n9.19\n0.80\n11.50\n0.00\n    LS\ntrend()\n−0.17\n0.04\n−4.43\n0.00\n  \n  \n  \n\n\n\n\nNote how the estimated coefficient for the AR1 process resembles the one in our simulation. However, the estimation of this model suggest a constant of 1.8. If we changed the constant to 1.8, the simulation would resemble the PI growth data better.\nIf the AR(1) process correctly describes the series, the errors should behave like white noise. To inspect the errors we can use the augment() function. The ACF is displayed below.\n\nerrors_PI&lt;-augment(PI_fit)\n\nerrors_PI %&gt;% select(.resid) %&gt;% ACF(.resid) %&gt;% \n  autoplot() + theme_bw()\n\n\n\n\nNote how the errors of the AR(1) model resemble white noise. This suggests that we have identified the systematic component of the series as being an AR(1) process. In other words, there is nothing left to model since the errors are completely random. This is not the case for the LS model, since we still observe some significant spikes (lag 1 and lag 13) in the ACF function.\n\nerrors_PI %&gt;% select(.resid) %&gt;% PACF(.resid) %&gt;% \n  autoplot() + theme_bw()\n\n\n\n\nThe PACF once again shows no pattern for the residuals of the AR(1) model and some significant lags for the LS model."
  },
  {
    "objectID": "Benchmarks.html#cross-validation",
    "href": "Benchmarks.html#cross-validation",
    "title": "5  Model Benchmarks",
    "section": "5.6 Cross Validation",
    "text": "5.6 Cross Validation\nInstead of selecting a single training set and test set, we can create several. Specifically, we could take the first three observations of our time series and define them as the training set. We can then estimate a model and forecast the fourth (or nth) observation. The forecast error is recorded and the training set is changed so that now the first four observations are used to estimate the model and forecast the fifth (or nth) observation. This procedure is repeated as many times as the data allows. Below we create a table that enables us to follow the cross-validation of our benchmarks.\n\navocado_cv &lt;- calits %&gt;% \n  select(-geography, -total_volume) %&gt;% \n  stretch_tsibble(.init = 3, .step = 1)\n\nstretch_tsibble() is a handy function that creates a variable called id that is initialized with the .init argument. In this case, the first three observations are given \\(id=1\\). The id then changes with a step of \\(.step=1\\). That is, \\(id=2\\) for the first four observations, then \\(id=3\\) for the first five observations, and so on. Below is a sample of the tsibble.\n\n\n\n\n\n\n  \n    \n      CV tsibble\n    \n    \n    \n      date\n      average_price\n      .id\n    \n  \n  \n    2015-01-04\n1.24\n1\n    2015-01-11\n1.10\n1\n    2015-01-18\n1.24\n1\n    2015-01-04\n1.24\n2\n    2015-01-11\n1.10\n2\n    2015-01-18\n1.24\n2\n    2015-01-25\n1.30\n2\n    2015-01-04\n1.24\n3\n  \n  \n  \n\n\n\n\nUsing this new tsibble, the benchmarks are estimated for each id and forecasts are generated for four periods ahead (\\(h=1\\)). The accuracy is measured and averaged across all iterations for each model. Results are shown in the table below.\n\navocado_cv %&gt;%\n  model(Mean=MEAN(average_price),\n        Naive=RW(average_price),\n        Drift=RW(average_price ~ drift()),\n        LS=TSLM(average_price~date)) %&gt;%\n  forecast(h = 1) %&gt;% accuracy(calits) %&gt;% \n  gt() %&gt;%\n  cols_align(\"center\") %&gt;% \n  tab_header(title = md(\"**Model Fit Cross Validation**\")) %&gt;% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %&gt;% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %&gt;% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"Naive\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit Cross Validation\n    \n    \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nTest\n0.00\n0.14\n0.10\n−0.42\n5.77\n1.01\n1.01\n−0.19\n    LS\nTest\n−0.05\n0.26\n0.20\n−4.59\n11.63\n2.03\n1.91\n0.84\n    Mean\nTest\n0.12\n0.27\n0.20\n5.51\n11.11\n2.10\n2.05\n0.84\n    Naive\nTest\n0.00\n0.13\n0.10\n−0.02\n5.65\n1.00\n1.00\n−0.19\n  \n  \n  \n\n\n\n\nThe Naive method performs the best when forecasting one period ahead. However, we note once again that the Naive method will provide the same forecast for one, two, three or more periods ahead. You can confirm that this model would lose its appeal when predicting three or four periods ahead. Most importantly, there is no formal model telling us how data is generated."
  },
  {
    "objectID": "Benchmarks.html#sec-Bench",
    "href": "Benchmarks.html#sec-Bench",
    "title": "5  Model Benchmarks",
    "section": "5.1 Benchmarks",
    "text": "5.1 Benchmarks\nOne of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A naive prediction sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T\\)\n\nwhere \\(\\hat y_{T+h}\\) is the predicted value for \\(h\\) periods ahead, and \\(y_T\\) is the value observed at the current time period \\(T\\). We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would “drift” the naive prediction upward. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T+h(\\frac{y_t-y_1}{T-1})\\)\n\nwhere \\(h(\\frac{y_t-y_1}{T-1})\\) can be thought as the average increase of \\(y\\) from period \\(1\\) to the current period \\(T\\). One could also predict weight by observing weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:\n\n\\(\\hat y_{T+h}=\\frac{(y_1+y_2+...+y_T)}{T}\\)\n\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:\n\n\\(\\hat y_{T+h}=b_0+b_1(T+h)\\)"
  },
  {
    "objectID": "Benchmarks.html#over-fitting",
    "href": "Benchmarks.html#over-fitting",
    "title": "5  Model Benchmarks",
    "section": "5.5 Over-Fitting",
    "text": "5.5 Over-Fitting\nOver-fitting can happen when a model is overly flexible. This can make the model fit to the random fluctuations or noise in the data, rather than the underlying pattern. This is a major failing in modeling as it ignores the systematic pattern that governs the time series.\nTo overcome this problem, we usually have a training set or subset of the data that we use to estimate the model’s parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the test set. A model that over-fits to the training data, will often perform poorly when forecasting the test set.\nRecall that benchmarks were estimated for the period between 2015-01-04~2018-06-02. We will call this our training set. For our test set, we’ll use the 2018-06-02~2018-12-02 period. The code below creates the test set using the filter_index() function.\n\ncali %&gt;%\n  as_tsibble(key=geography,\n             index=date, regular=T) %&gt;%\n  filter_index(\"2018-06-02\"~\"2018-12-02\") -&gt; calits_test\n\nNow we can plot the training set, the forecast, and the test set by using the code below.\n\ncalits_fc %&gt;% autoplot(level=NULL) + \n  theme_classic() + \n  autolayer(calits_train, average_price) + \n  autolayer(calits_test, average_price)\n\n\n\n\nThe graph shows how the LS method does well with the test data and a long forecast period. This can be confirmed by obtaining the accuracy measures against the test set. The code below uses the accuracy() function to generate the main table.\n\naccuracy(calits_fc, calits_test)\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n    \n      .model\n      geography\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nCalifornia\nTest\n0.17\n0.21\n0.17\n9.24\n9.24\nNaN\nNaN\n0.52\n    LS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\nNaN\nNaN\n0.52\n    Naive\nCalifornia\nTest\n0.20\n0.24\n0.20\n10.85\n10.85\nNaN\nNaN\n0.50\n    mean\nCalifornia\nTest\n0.13\n0.18\n0.13\n6.72\n6.85\nNaN\nNaN\n0.50\n  \n  \n  \n\n\n\n\nInterestingly, the Naive method is no longer the best model since it will always predict the series’ previous value regardless of how many periods we forecast. On the other hand, the LS model correctly uses the deterministic trend to forecast the future. Trends are useful in predicting time series."
  },
  {
    "objectID": "Tools2.html#sec-Avocado",
    "href": "Tools2.html#sec-Avocado",
    "title": "4  Time Series Tools",
    "section": "4.2 The Avocado Data Set",
    "text": "4.2 The Avocado Data Set\nThe avocado data set is weekly retail scan data for U.S retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. Other avocados (e.g. greenskins) are not included in this data. You can find the data here.\nNote: When inspecting the data you will notice that each entry is recorded weekly. However, there is an entry on 01/01/2018, that is right after 12/31/2017. This is a single observation that is not weekly. You will also note that there are missing dates from 12/02/2018-12/31/2018."
  },
  {
    "objectID": "Tools2.html#sec-Visual",
    "href": "Tools2.html#sec-Visual",
    "title": "4  Time Series Tools",
    "section": "4.5 Visualizing The Data",
    "text": "4.5 Visualizing The Data\nTo visualize the data, we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines the mapping of variables. The geom_line() function specifies the type of plot. In time series, we will use the line plot regularly. Labels for the graph are easily set with the labs() function and there are plenty of themes available to customize your visualization. Below, the theme_classic() is displayed. To learn more about the ggplot package, you can refer to Wickham (2017) Chapter 2. Below is the code to create a line plot of California’s average avocado price.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price, group=geography),color=\"black\") +\n  theme_classic() + \n  labs(x=\"\",\n       y=\"Average Price\", \n       title=\"Organic Avocado Price in California\",\n       subtitle=\"2015-2018\")  \n\n\n\n\nThe average price of avocados in California has been increasing during the period considered. It reached a maximum of about 2.6 in 2017 and was at a minimum in the spring of 2015. There is also a seasonal pattern with low prices at the beginning of the year and peaks mid-year. As we will see in upcoming chapters, these are patterns that can be extrapolated and used to forecast time series."
  },
  {
    "objectID": "ARIMA.html#the-ar1-model",
    "href": "ARIMA.html#the-ar1-model",
    "title": "7  ARIMA",
    "section": "7.2 The AR(1) model",
    "text": "7.2 The AR(1) model\nIn the previous section, we identified the growth of personal income in California to have a decaying ACF and a single significant spike (at lag 1) in the PACF. These patterns can be generated with an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R.\n\ny&lt;-c(0)\nphi&lt;-0.7\nconst&lt;-1\nnrep&lt;-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like for a AR(1) process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% ACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the personal income. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %&gt;% PACF(lag_max = 12, y) %&gt;% autoplot()+theme_bw()+labs(x=\"\",y=\"PACF\",                         title=\"PACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\nOnce again, if you compare the PACF to the one in personal income growth you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero. This allows us to confirm that we can model personal consumption growth with an AR(1) process."
  },
  {
    "objectID": "ARIMA.html#model-selection",
    "href": "ARIMA.html#model-selection",
    "title": "7  ARIMA",
    "section": "7.4 Model Selection",
    "text": "7.4 Model Selection\nWe can further choose between these two models by looking at the AIC, AICc, or BIC.\n\nglance(PI_fit) %&gt;% arrange(AICc) %&gt;% select(.model:BIC)\n\n\n\n\n\n\n\n  \n    \n      Model Fit Measures\n    \n    \n    \n      .model\n      sigma2\n      AIC\n      AICc\n      BIC\n    \n  \n  \n    AR1\n4.30\n−20.25\n−19.89\n−17.09\n    LS\n5.51\n65.35\n66.10\n70.10\n  \n  \n  \n\n\n\n\nHere we note that the AR(1) model performs better in all of the metrics as they are significantly lower than those for the LS. The accuracy on the test set shown below, once more confirms that the AR(1) model performs better than the LS model.\n\nPI_fc&lt;-PI_fit %&gt;% forecast(new_data = PI_test)\nPI_fc  %&gt;% accuracy(PI_test) \n\n\n\n\n\n\n\n  \n    \n      Accuracy Measures\n    \n    \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    AR1\nTest\n−1.51\n3.47\n2.45\n−115.69\n156.61\nNaN\nNaN\n0.33\n    LS\nTest\n2.60\n4.43\n3.87\n22.50\n106.69\nNaN\nNaN\n0.48\n  \n  \n  \n\n\n\n\nThe graph below shows the test set along with the forecast of the AR(1) model. Prediction confidence intervals are shown to highlight the uncertainty of the prediction. The blue line indicates the mean of the predictions which are assumed to follow a normal distribution.\n\nPI_fc %&gt;% filter(.model==\"AR1\") %&gt;% autoplot() + theme_classic() +\n  autolayer(PI_train, PI_Growth) +\n  autolayer(PI_test, PI_Growth) + \n  labs(title=\"Personal Income Growth AR(1) Forecast Accuracy\",\nsubtitle=\"1970-2021\", y=\"\",x=\"\")"
  },
  {
    "objectID": "Decisions.html#adding-uncertainty",
    "href": "Decisions.html#adding-uncertainty",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Adding Uncertainty",
    "text": "1.4 Adding Uncertainty\nTo solve decision trees with uncertainty use the folding-back procedure. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision."
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company-ceo",
    "href": "Decisions.html#shopaholic-retail-company-ceo",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Shopaholic Retail Company CEO",
    "text": "1.1 Shopaholic Retail Company CEO\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome."
  },
  {
    "objectID": "Decisions.html#expected-monetary-value",
    "href": "Decisions.html#expected-monetary-value",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Expected Monetary Value",
    "text": "1.2 Expected Monetary Value\nAccording to your estimates, you believe that there is a \\(60\\)% chance of success and a \\(40\\)% chance of failure. Using this information, you can calculate the expected monetary value (EMV) of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\nThe expected monetary value estimates the average monetary outcome you can expect from a decision involving uncertainty. In this example, the value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EMV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EMV\\) is the expected monetary value, \\(p_{i}\\) is the probability of outcome \\(i\\), and \\(x_{i}\\) is the monetary value resulting from outcome \\(i\\). Notice that if we had other decisions involving uncertainty, we could also evaluate them using the EMV. Hence, the EMV allows us to rank and choose best decisions when uncertainty is present."
  },
  {
    "objectID": "Decisions.html#lessons-learned-in-this-chapter",
    "href": "Decisions.html#lessons-learned-in-this-chapter",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.10 Lessons Learned In This Chapter",
    "text": "1.10 Lessons Learned In This Chapter\n\nUse the concept of Expected Monetary Value.\nUse Decision Trees to map the business decision process.\nApply the backward induction method to solve decision problems.\nApply Bayes’ Theorem to update probabilities given new information.\nDetermine the price of information.\n\n\n\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. “Business Statistics.”\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "ARIMA.html#readings",
    "href": "ARIMA.html#readings",
    "title": "7  ARIMA",
    "section": "7.5 Readings",
    "text": "7.5 Readings\nHyndman (2021) Chapter 9 (ARIMA Models)."
  },
  {
    "objectID": "ARIMA.html#leasons-learned",
    "href": "ARIMA.html#leasons-learned",
    "title": "7  ARIMA",
    "section": "7.6 Leasons Learned",
    "text": "7.6 Leasons Learned\nIn this module you have been introduced to ARIMA model. Particularly you have learned to:\n\nUse the autocorrelations and partial autocorrelations to analyze time series.\nIdentify the AR(1) process using autocorrelations.\nModel the ARIMA process using the ARIMA() function.\nProvide forecasts of the ARIMA process in R.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "ETS.html#readings",
    "href": "ETS.html#readings",
    "title": "6  ETS",
    "section": "6.7 Readings",
    "text": "6.7 Readings\nThe main reading for ETS models comes from Chapter 8 of Hyndman (2021). These readings provide a bit more detail on the mathematical background behind each model and a few more applications.\nHyndman (2021) Chapter 8 (Exponential Smoothing)."
  },
  {
    "objectID": "ETS.html#leasons-learned",
    "href": "ETS.html#leasons-learned",
    "title": "6  ETS",
    "section": "6.8 Leasons Learned",
    "text": "6.8 Leasons Learned\nIn this module you have been introduced to ETS model. Particularly you have learned to:\n\nUse the model() and ETS() functions to estimate the model.\nIdentify when ETS model is superior to other model by using the cross validation and information criterion.\nForecast time series with the ETS model.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company",
    "href": "Decisions.html#shopaholic-retail-company",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Shopaholic Retail Company",
    "text": "1.1 Shopaholic Retail Company\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome. Determining how to adequately assess the risks involved in this decision and the potential outcomes is of importance."
  },
  {
    "objectID": "Decisions.html#consulting-team",
    "href": "Decisions.html#consulting-team",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Consulting Team",
    "text": "1.4 Consulting Team\nConsider now the option of hiring a consulting team that promises to give you a bit more certainty. The team provides a recommendation based on market research, historical data, and their expertise for $500,000. However, it is known that the team has a false positive rate (i.e., recommending to go with a project when the project would fail) of 15% and a false negative rate of 5% (i.e., recommending not going with a project and when the project would succeed). Below you can see the updated decision tree with the results of the recommendation (i.e., Positive, and Negative).\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=?| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=?| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )\n\n\n\n\n\nNote that even though the consultants might recommend not introducing the production line, the CEO can still decide to go against their recommendation. Also, some probabilities are now unknown and must be calculated (they are highlighted in the decision tree with \\(?\\)). For example, given that the recommendation is positive, the probability that the introduction would succeed is unknown \\(p(Success|+)\\). We will use probability theory to uncover the missing probabilities in the upcoming sections. Finally, the final payoffs have all been adjusted to reflect the cost of hiring the team of consultants.\nUltimately, we would like gain more certainty on our decision to introduce the production line. Additionally, knowing if we should hire consultants to advise is equally essential."
  },
  {
    "objectID": "Decisions.html#updating-probabilities",
    "href": "Decisions.html#updating-probabilities",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Updating Probabilities",
    "text": "1.5 Updating Probabilities\nThe Law of total probability is useful in determining the probabilities that we get a positive recommendation \\(p(+)\\) or a negative recommendation \\(p(-)\\). In sum the law states:\n\n\\(p(A)=p(A|B)p(B)+p(A|B^c)p(B^c)\\)\n\nSubstituting the values provided in our problem we obtain:\n\n\\(p(+)=p(+|Failure)p(Failure)+p(+|Success)p(Success)\\) \n\\(p(+)=0.15(0.4)+0.95(0.6)\\) \n\\(p(+)=0.63\\)\n\nHence, the probability of obtaining a recommendation of introducing the new product line is \\(63\\)% and of not recommending the introduction is \\(p(-)=37\\)%. The updated decision tree is now:\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )"
  },
  {
    "objectID": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.6 The VA Department of Transportation Wants Your Services",
    "text": "2.6 The VA Department of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they schedule the expected staff and number of ferries to run.\nAssume that the VA Department of transportation shares three weeks of data. The table below records the number of vehicles that used the ferry service:\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\n\n\n\n\nMon\n1175\n1020\n1163\n\n\nTue\n1198\n1048\n1066\n\n\nWed\n1189\n1102\n1183\n\n\nThu\n1175\n1094\n1003\n\n\nFri\n1101\n1042\n1095\n\n\nSat\n1529\n1464\n1418\n\n\nSun\n1580\n1534\n1512\n\n\n\nWhat distribution would you use to simulate weekdays (Mon-Fri)? Would you simulate weekends (Sat and Sun) differently than weekdays? According to the data, what would be the minimum and maximum number of vehicles transported during weekdays (weekends)? Can you provide a sensible simulation for week 4?"
  },
  {
    "objectID": "Tools.html#using-loops-in-r",
    "href": "Tools.html#using-loops-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 Using Loops in R",
    "text": "2.4 Using Loops in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to quickly generate new variables for our model or test different variations of our parameters to see how the model behaves.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a specific condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a particular condition is true. Let us illustrate the syntax of the for loop by simulating demand for each book Monster Classics series and calculating the total revenue.\n\nRevenue&lt;-c()\n\nfor (i in MS$Price) {\n  Revenue&lt;-c(Revenue,i*rbinom(1,100,0.7))\n  print(Revenue)\n}\n\n[1] 741\n[1] 741.00 421.94\n[1] 741.00 421.94 456.48\n[1] 741.00 421.94 456.48 419.58\n[1] 741.00 421.94 456.48 419.58 186.20\n\n\nThe code above starts by creating an empty vector to store the revenue generated by each book. A for loop is then used to simulate the revenue for each book. The process starts by taking the first price in the MS$Price vector and multiplying it with a single random number drawn from the binomial distribution with \\(100\\) trials and probability \\(0.7\\) (our simulated demand). Note how the code combines the Revenue vector with the revenue generated by the simulation in the code c(Revenue, i*rbinom(1,100,0.7). This process is repeated for every number in the MS$Price vector, leading to a final vector with each book’s revenues."
  },
  {
    "objectID": "Tools.html#using-conditionals-in-r",
    "href": "Tools.html#using-conditionals-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.5 Using Conditionals in R",
    "text": "2.5 Using Conditionals in R\nConditionals allow you to execute different code blocks based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false.\nLet us go back to the Monster Classic example and assume that the bookstore has gained additional insight into the demand for their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand&lt;-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand&lt;-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 86 87 70 72 71\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, the random binomial number is drawn with the probability \\(0.9\\). If not, it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series and adds a simulated demand. You can quickly realize that this becomes very efficient if the bookstore has an extensive collection of books. Below is our data frame with the new simulated values.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     86\n2           Dracula  5.78     87\n3         Moby Dick  6.34     70\n4 War Of The Worlds  5.67     72\n5           Beowulf  2.45     71"
  },
  {
    "objectID": "Decisions.html#optimal-decision",
    "href": "Decisions.html#optimal-decision",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Optimal Decision",
    "text": "1.7 Optimal Decision\nNow that we have calculated the probabilities we can finally decide whether we should hire the consulting team, and most importantly whether we should introduce the new product line. Calculating EMV’s the decision tree is:\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( ))\n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt; D(EMV=3.94)\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt; J(EMV=-2.86)\n\n\n\n\n\nIt is important to notice here that if the CEO hires the consultants, he/she should follow the recommendation. If the recommendation is positive, then the EMV resulting from introduction of the product line (\\(3.94\\)) is greater than the loss of not introducing the product line (\\(-0.5\\)). Similarly, if the recommendation is negative, the EMV from the introduction (\\(-2.86\\)) is less than the loss of not introducing the product line (\\(-0.5\\)). As a consequence, the recommendation is aligned with the CEO’s best decision."
  },
  {
    "objectID": "Tools.html#nightmare-reads-simulates-demand",
    "href": "Tools.html#nightmare-reads-simulates-demand",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Nightmare Reads Simulates Demand",
    "text": "2.3 Nightmare Reads Simulates Demand\nNightmare Reads bookstore wishes to determine how many customers will buy their Monster Classic Series. They plan to send \\(100\\) catalogs by mail to potential customers. Before they send the catalogs, they decide to get an estimate on demand. Past data reveals that a customer will buy a book from the catalog with a probability of \\(0.70\\). Using R, they simulate the demand generated by their catalog.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=rbinom(5,100,0.7)))\n\n              Books Price Demand\n1      Frankenstein  9.50     68\n2           Dracula  5.78     67\n3         Moby Dick  6.34     72\n4 War Of The Worlds  5.67     71\n5           Beowulf  2.45     72\n\n\nThe demand for the book is generated using the rbinom() function. The first input of the rbinom() function specifies how many random numbers are needed (\\(5\\)), the second one specifies the number of trials in the experiment (\\(100\\)), and the last one specifies the probability of success (\\(0.7\\)). The bookstore can now assess the Monster Series’s revenues with these demands."
  },
  {
    "objectID": "Decisions.html#the-value-of-information",
    "href": "Decisions.html#the-value-of-information",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.8 The Value of Information",
    "text": "1.8 The Value of Information\nWe are now faced with an important question. What is the value of the information provided by the consulting team? In other words, should the CEO hire the consultants? To answer this question, let’s start by calculating expected monetary value of the tree in Section 1.7:\n\n\\(EMV=0.63(3.94)+0.37(-0.5)\\) \n\\(EMV=2.3\\) \n\nThis EMV is higher than the EMV of making the decision without the consultants (1.8 million). This highlights that the information provided by the consulting team is valuable to the CEO. Moreover, the CEO should be willing to pay up to $1,000,000 for the consulting service."
  },
  {
    "objectID": "Tools2.html#chilangos-kitchen-wants-you-to-forecast-avocado-prices",
    "href": "Tools2.html#chilangos-kitchen-wants-you-to-forecast-avocado-prices",
    "title": "4  Time Series Tools",
    "section": "4.1 Chilango’s Kitchen Wants You to Forecast Avocado Prices",
    "text": "4.1 Chilango’s Kitchen Wants You to Forecast Avocado Prices\nChilango’s Kitchen specializes in tacos and burritos that are made to order in front of the customer. Guacamole is the perfect pairing to their delicious food and one of the restaurant’s best sellers. Their guac uses just six ingredients: avocados, lime juice, cilantro, red onion, jalapeño, and kosher salt. Because of its popularity, each restaurant goes through approximately five cases of avocados daily, amounting to more than \\(44,000\\) pounds annually. Chilango’s Kitchen wants you to provide insights on the price of avocados in California."
  },
  {
    "objectID": "ETS.html#predicting-teslas-deliveries",
    "href": "ETS.html#predicting-teslas-deliveries",
    "title": "6  ETS",
    "section": "6.2 Predicting Tesla’s Deliveries",
    "text": "6.2 Predicting Tesla’s Deliveries\nDeliveries are a carefully watched number by Tesla shareholders and are the closest approximation of sales disclosed by the company. Additionally, Tesla’s deliveries are closely followed due to their impact on financial markets, the EV industry, innovation and disruption, production efficiency, and the growth of the EV market. The numbers serve as a key performance indicator for Tesla’s success and provide insights into the broader trends in the electric vehicle industry. Can we use the ETS model to forecast Tesla’s deliveries?"
  },
  {
    "objectID": "ETS.html#the-data",
    "href": "ETS.html#the-data",
    "title": "6  ETS",
    "section": "6.3 The Data",
    "text": "6.3 The Data\nThe data can be found here Tesla. Below is code that inputs the data as a tsibble in R.\n\nlibrary(fpp3)\n\n# Create tsibble\ntesla&lt;-tsibble(\n  period=yearquarter(c(\"2016:Q1\",\"2016:Q2\",\"2016:Q3\",\"2016:Q4\",\n                       \"2017:Q1\",\"2017:Q2\",\"2017:Q3\",\"2017:Q4\",\n                       \"2018:Q1\",\"2018:Q2\",\"2018:Q3\",\"2018:Q4\",\n                       \"2019:Q1\",\"2019:Q2\",\"2019:Q3\",\"2019:Q4\",\n                       \"2020:Q1\",\"2020:Q2\",\"2020:Q3\",\"2020:Q4\",\n                       \"2021:Q1\",\"2021:Q2\",\"2021:Q3\",\"2021:Q4\",\n                       \"2022:Q1\",\"2022:Q2\",\"2022:Q3\",\"2022:Q4\",\n                       \"2023:Q1\",\"2023:Q2\")),\n  deliveries=c(14.8,14.4,24.5,22.2,\n               25,22,26.2,29.9,\n               30,40.7,83.5,90.7,\n               63,95.2,97,112,\n               88.4,90.7,139.3,180.6,\n               184.82,201.25,241.3,308.6,\n               310.5,254.7,343.8,405.3,\n               422.9,466.1),\n  index=period     # This is the time variable\n)\n\nAs you can see the tsibble is created with the tsibble() function included in the fpp3 package. The yearquarter() function from the lubridate package is used to coerce the period data to a date. The time variable is then specified via the index parameter. The code below creates the plot of Tesla’s deliveries using the autoplot() function.\n\ntesla %&gt;% autoplot(.vars=deliveries) + theme_classic() +\n  labs(title= \"Tesla Car Deliveries\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")\n\n\n\n\nThe most striking aspect of Tesla’s deliveries is the exponential trend. There also seems to be a seasonal component, with relatively higher production in Q2 and Q3 versus the other quarters. These characteristics will be adopted by the ETS model to forecast the series. Below we can see the STL decomposition that confirm these characteristics.\n\ntesla %&gt;%\n  model(STL(deliveries~trend(window=6)+\n              season(window=4), robust=TRUE)) %&gt;%\n  components() %&gt;% autoplot()+ theme_classic()"
  },
  {
    "objectID": "ETS.html#models",
    "href": "ETS.html#models",
    "title": "6  ETS",
    "section": "6.4 Models",
    "text": "6.4 Models\nTo model the data and create the appropriate forecasts, we start by generating test and training sets from the available data.\n\ntrain_tesla&lt;-filter_index(.data=tesla,\"2016 Q1\"~\"2021 Q4\")\ntest_tesla&lt;-filter_index(.data=tesla,\"2022 Q1\"~\"2023 Q2\")\n\nThere is no fixed rule for determining the length of the train and test sets. In this example, it is important to allocate a sufficiently large portion of the data to the training set to capture the underlying seasonality and trend of Tesla’s deliveries. The sets are easily created using the filter_index() function.\nFive models will be estimated based on ETS. The first one is the Simple Exponential Smoothing model with additive errors (SES), the Holt model that includes an additive trend (HOLT), a dampened trend model (DAMPED), a damped model with seasonality (DAMPEDS), and finally an algorithmic function that attempts to select the best ETS model (see Hyndman (2021), Chapter 8). Along with these five models two more models are set forth. The first one is a simple least squares model (LS) and the second one is a quadratic model with seasonality dummies (LSS).\nModel selection will be done via cross validation. Recall, that the the stretch_tsibble() function reshapes the tsibble to accommodate for cross validation. The .init parameter sets the first eight observations to estimate our initial model and the .step argument increases the training set by four. Cross validation is done four periods ahead (a year) and accuracy measures are created by comparing forecasts to the test set.\nEach component of the ETS model can be included as either multiplicative (\\(M\\)) or additive (\\(A\\)). The trend component can be assigned to be damped (\\(Ad\\) or \\(Md\\)). If the component is to be omitted from the model, None (\\(N\\)) is specified. Below is the code to estimate the models and the results of the cross validation.\n\nlibrary(gt)\ntrain_tesla %&gt;% stretch_tsibble(.init = 8, .step=4) %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season()))%&gt;%\n  forecast(h = 4) %&gt;%\n  accuracy(tesla) %&gt;% select(-\"ACF1\") \n\n\n\n\n\n\n\n  \n    \n      Cross Validation Models\n    \n    \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n    \n  \n  \n    ALGO\nTest\n40.231\n57.608\n45.297\n23.474\n28.654\n1.042\n0.990\n    DAMPED\nTest\n18.701\n55.497\n44.900\n8.033\n32.803\n1.033\n0.954\n    DAMPEDS\nTest\n21.011\n47.198\n38.590\n9.399\n23.120\n0.888\n0.811\n    HOLT\nTest\n15.122\n57.200\n46.728\n4.127\n34.811\n1.075\n0.983\n    LS\nTest\n11.657\n43.208\n37.200\n2.527\n28.375\n0.856\n0.742\n    LSS\nTest\n9.739\n41.890\n36.739\n1.641\n28.748\n0.845\n0.720\n    SES\nTest\n40.991\n58.160\n45.750\n23.988\n28.824\n1.053\n0.999\n  \n  \n  \n\n\n\n\nThe accuracy measures reveal that the DAMPEDS and LSS models perform consistently well. Below, we will continue with the DAMPEDS and LSS models as the trend seems to be exponential and there seems to be evidence of seasonality. These model are estimated and saved into an object called fit below.\n\nfit &lt;- tesla %&gt;%\n  model(\n    DAMPEDS = ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\n    LSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())\n  )\n\nIf one is interested in retrieving the model coefficients, one can use the tidy() (or coef()) function. Below the function is used along with the fit object to retrieve the coefficients of the Least Squares model with seasonality:\n\ntidy(fit) %&gt;% filter(.model==\"LSS\") %&gt;%\n  select(-\".model\")\n\n\n\n\n\n\n\n  \n    \n      LSS Model Coefficients\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n36.43\n13.97\n2.61\n0.02\n    trend()\n−7.33\n1.92\n−3.82\n0.00\n    I(trend()^2)\n0.70\n0.06\n11.62\n0.00\n    season()year2\n−8.63\n10.91\n−0.79\n0.44\n    season()year3\n8.07\n11.35\n0.71\n0.48\n    season()year4\n21.40\n11.35\n1.88\n0.07\n  \n  \n  \n\n\n\n\nThe output above, reveals that the seasonal dummy for Q4 is statistically significant at the \\(10\\)% confirming the seasonal pattern found in the decomposition (Section 6.3). The plot below shows the fit of the models with the blue line representing the LSS model and the red line the DAMPEDS model.\n\ntesla %&gt;% autoplot(deliveries, lwd=1.2, alpha=0.5) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"blue\",\n            data = augment(fit)  %&gt;% filter(`.model`==\"LSS\")) +\n              geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %&gt;% filter(`.model`==\"DAMPEDS\")) + \n  labs(title= \"Tesla Car Deliveries Fitted Values\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")"
  },
  {
    "objectID": "ETS.html#forecast",
    "href": "ETS.html#forecast",
    "title": "6  ETS",
    "section": "6.4 Forecast",
    "text": "6.4 Forecast\nForecasts are created by using the fit object. We will forecast four quarters ahead using the forecast() function. The code below generate a table with the forecasts.\n\nlibrary(gt)\nfit %&gt;%\n  forecast(h = 4) %&gt;% select(-\".model\") -&gt; deliveries_fc\ndeliveries_fc\n\n\n\n\n\n\n\n  \n    \n      ETS Forecast\n    \n    \n    \n      period\n      deliveries\n      .mean\n      .model\n    \n  \n  \n    2023 Q3\nN(516, 854)\n515.90\nDAMPEDS\n    2023 Q4\nN(563, 1681)\n562.94\nDAMPEDS\n    2024 Q1\nN(579, 2895)\n578.73\nDAMPEDS\n    2024 Q2\nN(602, 4563)\n601.50\nDAMPEDS\n    2023 Q3\nN(482, 745)\n482.03\nLS\n    2023 Q4\nN(518, 801)\n518.07\nLS\n    2024 Q1\nN(555, 870)\n555.46\nLS\n    2024 Q2\nN(594, 953)\n594.20\nLS\n    2023 Q3\nN(489, 709)\n488.95\nLSS\n    2023 Q4\nN(539, 754)\n538.99\nLSS\n    2024 Q1\nN(556, 782)\n555.69\nLSS\n    2024 Q2\nN(587, 844)\n586.56\nLSS\n  \n  \n  \n\n\n\n\nForecasts for the four quarters are shown above, with the corresponding mean. In general, the ETS model predicts Tesla will continue its trend and increase its deliveries every quarter. For the third quarter of 2023, Tesla expects to deliver about \\(516,000\\) cars on average with a standard deviation \\(854\\) cars. If we recall the 68-95-99.7 rule, Tesla deliveries for Q3 of 2023 will likely be between \\(513000\\) and \\(519000\\). The increasing standard deviation for future periods reminds us that longer-period forecasts have even more uncertainty. The plot below illustrates how the model expects Tesla to continue its upward trend.\n\nfit %&gt;%\n  forecast(h = 4) %&gt;%\n  autoplot(tesla, level=95)+\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries\",\n       subtitle = \"Q1 2017 to Q2 2023\") + theme_classic()"
  },
  {
    "objectID": "Decisions.html#sec-EMV",
    "href": "Decisions.html#sec-EMV",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Expected Monetary Value",
    "text": "1.2 Expected Monetary Value\nAccording to your estimates, you believe that there is a \\(60\\)% chance of success and a \\(40\\)% chance of failure. Using this information, you can calculate the expected monetary value (EMV) of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\nThe expected monetary value estimates the average monetary outcome you can expect from a decision involving uncertainty. In this example, the value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EMV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EMV\\) is the expected monetary value, \\(p_{i}\\) is the probability of outcome \\(i\\), and \\(x_{i}\\) is the monetary value resulting from outcome \\(i\\). Notice that if we had other decisions involving uncertainty, we could also evaluate them using the EMV. Hence, the EMV allows us to rank and choose best decisions when uncertainty is present."
  },
  {
    "objectID": "Decisions.html#optimal-decision-sec-optimal",
    "href": "Decisions.html#optimal-decision-sec-optimal",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Optimal Decision {?sec-Optimal}",
    "text": "1.7 Optimal Decision {?sec-Optimal}\nNow that we have calculated the probabilities we can finally decide whether we should hire the consulting team, and most importantly whether we should introduce the new product line. Summarizing the decision tree by calculating the EMV’s for each probability node yields:\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( ))\n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt; D(EMV=3.94)\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt; J(EMV=-2.86)\n\n\n\n\n\nIt is important to notice here that if the CEO hires the consultants, he/she should follow the recommendation. If the recommendation is positive, then the EMV resulting from introduction of the product line (\\(3.94\\)) is greater than the loss of not introducing the product line (\\(-0.5\\)). Similarly, if the recommendation is negative, the EMV from the introduction (\\(-2.86\\)) is less than the loss of not introducing the product line (\\(-0.5\\)). As a consequence, the recommendation is aligned with the CEO’s best decision."
  },
  {
    "objectID": "Decisions.html#sec-Optimal",
    "href": "Decisions.html#sec-Optimal",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Optimal Decision",
    "text": "1.7 Optimal Decision\nNow that we have calculated the probabilities we can finally decide whether we should hire the consulting team, and most importantly whether we should introduce the new product line. Summarizing the decision tree by calculating the EMV’s for each probability node yields:\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=0.63| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( ))\n    A[ ] --&gt;|Don't Introduce| B( -0.5 )\n    C --&gt; D(EMV=3.94)\n    \n    F(( ))--&gt; |Negative p=0.37| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( -0.5 )\n    H --&gt; J(EMV=-2.86)\n\n\n\n\n\nIt is important to notice here that if the CEO hires the consultants, he/she should follow the recommendation. If the recommendation is positive, then the EMV resulting from introduction of the product line (\\(3.94\\)) is greater than the loss of not introducing the product line (\\(-0.5\\)). Similarly, if the recommendation is negative, the EMV from the introduction (\\(-2.86\\)) is less than the loss of not introducing the product line (\\(-0.5\\)). As a consequence, the recommendation is aligned with the CEO’s best decision."
  },
  {
    "objectID": "Benchmarks.html#other-accuracy-measures",
    "href": "Benchmarks.html#other-accuracy-measures",
    "title": "5  Model Benchmarks",
    "section": "5.7 Other Accuracy Measures",
    "text": "5.7 Other Accuracy Measures\nAIC (Akaike Information Criterion), AICc (corrected AIC), and BIC (Bayesian Information Criterion) are commonly used measures of model accuracy or goodness of fit in statistical modeling. They are used to compare different models and select the one that best balances model complexity (number of parameters estimated) and fit.\n\nAIC is a measure that penalizes model complexity. It balances the trade-off between model fit and the number of parameters in the model. The AIC value is calculated using the formula:\n\n\n\\(AIC = -2 * log-likelihood + 2 * k\\)\n\nIn this formula, the log-likelihood represents how well the model fits the data, and the number of parameters (\\(k\\)) accounts for the complexity of the model. The lower the AIC value, the better the model.\n\nAICc is an adjustment to the AIC measure, particularly for smaller sample sizes. AIC tends to overestimate the complexity penalty when the number of data points is relatively small. AICc adds a correction factor to account for this and is calculated using the formula:\n\n\\(AICc = AIC + (2 * k * (k + 1)) / (n - k - 1)\\)\n\n\nHere, \\(k\\) represents the number of parameters, and \\(n\\) is the sample size. AICc provides a more accurate measure of model fit in situations where the sample size is small. Similarly, lower AICc values indicate better model fit.\n\nBIC, also known as Schwarz Information Criterion (SIC), is another measure that penalizes model complexity. BIC is based on Bayesian principles and provides a stronger penalty for model complexity compared to AIC. The BIC value is calculated using the formula:\n\n\\(BIC = -2 * log-likelihood + log(n) * k\\)\n\n\nIn this formula, log-likelihood represents the model fit, \\(n\\) is the sample size, and \\(k\\) is the number of parameters. BIC puts a greater emphasis on simplicity compared to AIC.\nThese measures can be easily calculated in R using the report() function. The code below estimates the ETS and ARIMA models (which we will learn in the upcoming modules) for illustration purposes since the Naive and Mean models are non-parametric and do not provide us with an AIC, AICc, or BIC.\n\ncalits_train %&gt;%\n  model(LS=TSLM(average_price~trend()),\n        ETS=ETS(average_price),\n        LS2=ARIMA(average_price))%&gt;% \n  report() %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n\n\n\n\n\n\n  \n    \n      Model Fit Information Criterion\n    \n    \n    \n      .model\n      AIC\n      AICc\n      BIC\n    \n  \n  \n    LS\n−476.14\n−476.01\n−466.60\n    ETS\n203.26\n203.39\n212.80\n    ARIMA\n−205.83\n−205.76\n−199.48\n  \n  \n  \n\n\n\n\nThe model with the lowest AIC (AICc or BIC) is the simple Least Squares model that only has two parameters to estimate (slope and intercept). These results indicate that LS provides a good fit relative to it’s complexity."
  },
  {
    "objectID": "ETS.html#information-criterion",
    "href": "ETS.html#information-criterion",
    "title": "6  ETS",
    "section": "6.5 Information Criterion",
    "text": "6.5 Information Criterion\nWe can also attempt to select our models via the AIC, AICc, or BIC. The code below summarizes these measure for the models considered.\n\ntrain_tesla %&gt;%\n  model(\n    SES=ETS(deliveries ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    HOLT=ETS(deliveries ~ error(\"A\") + trend(\"A\") + season(\"N\")),\n    DAMPED=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"N\")),\n    DAMPEDS=ETS(deliveries ~ error(\"A\") + trend(\"Ad\") + season(\"A\")),\nALGO=ETS(deliveries),\nLS = TSLM(deliveries ~ trend()+I(trend()^2)),\nLSS = TSLM(deliveries ~ trend()+I(trend()^2)+season())) %&gt;% \n  report()  %&gt;%\n  select('.model',\"AIC\",\"AICc\",\"BIC\")\n\n\n\n\n\n\n\n  \n    \n      Model Fit Information Criterion\n    \n    \n    \n      .model\n      AIC\n      AICc\n      BIC\n    \n  \n  \n    SES\n237.16\n238.36\n240.69\n    HOLT\n233.99\n237.32\n239.88\n    DAMPED\n236.37\n241.31\n243.44\n    DAMPEDS\n233.92\n250.84\n245.70\n    ALGO\n223.19\n224.39\n226.73\n    LS\n150.12\n152.22\n154.83\n    LSS\n147.17\n154.17\n155.41\n  \n  \n  \n\n\n\n\nHere, once again the LSS model seems to perform the best as it provides the lowest values. Among the ETS models, the ALGO model now stands out. This should be of no surprise, as the ALGO model is designed to choose ETS components that minimize the AIC."
  },
  {
    "objectID": "ETS.html#forecasts",
    "href": "ETS.html#forecasts",
    "title": "6  ETS",
    "section": "6.6 Forecasts",
    "text": "6.6 Forecasts\nForecasts are created by using the fit object. We will forecast four quarters ahead using the forecast() function. The code below generates a table with the forecasts.\n\nlibrary(gt)\nfit %&gt;%\n  forecast(h = 4) %&gt;% select(-\".model\") -&gt; deliveries_fc\ndeliveries_fc\n\n\n\n\n\n\n\n  \n    \n      Forecasts\n    \n    \n    \n      period\n      deliveries\n      .mean\n      .model\n    \n  \n  \n    2023 Q3\nN(516, 854)\n515.90\nDAMPEDS\n    2023 Q4\nN(563, 1681)\n562.94\nDAMPEDS\n    2024 Q1\nN(579, 2895)\n578.73\nDAMPEDS\n    2024 Q2\nN(602, 4563)\n601.50\nDAMPEDS\n    2023 Q3\nN(489, 709)\n488.95\nLSS\n    2023 Q4\nN(539, 754)\n538.99\nLSS\n    2024 Q1\nN(556, 782)\n555.69\nLSS\n    2024 Q2\nN(587, 844)\n586.56\nLSS\n  \n  \n  \n\n\n\n\nForecasts for the four quarters are shown above, with the corresponding mean. In general, both models predict Tesla will continue its trend and increase its deliveries every quarter. According to the DAMPEDS model, Tesla is expected to deliver about \\(516,000\\) cars on average with a standard deviation \\(854\\) cars. If we recall the 68-95-99.7 rule, Tesla deliveries for Q3 of 2023 will likely be between \\(513000\\) and \\(519000\\). The increasing standard deviation for future periods reminds us that longer-period forecasts have even more uncertainty. The plot below illustrates the forecasts for both models along with the \\(95\\)% prediction intervals.\n\nfit %&gt;%\n  forecast(h = 4) %&gt;%\n  autoplot(tesla, level=95)+\n  labs(x=\"Quarter\", y=\"\",\n       title = \"Tesla Car Deliveries Forecasts\",\n       subtitle = \"Q1 2017 to Q2 2023\") + theme_classic()"
  },
  {
    "objectID": "ETS.html#ets-components",
    "href": "ETS.html#ets-components",
    "title": "6  ETS",
    "section": "6.1 ETS Components",
    "text": "6.1 ETS Components\nETS models build on simple exponential smoothing (SES). The basic idea behind SES is to assign more weight to recent observations and gradually decrease the weights as the observations become older. The model emphasizes the most recent data points and gives less importance to older observations.\nMathematically, the simple exponential smoothing model can be defined as:\n\n\\(l_{t}=\\alpha y_t + \\alpha(1-\\alpha)l_{t-1}\\)\n\nwhere \\(l_{t}\\) is smoothed value of the series at time \\(t\\), \\(y_t\\) is the value observed at the current time period \\(t\\) and \\(\\alpha\\) is the smoothing parameter. The SES model is useful when forecasting series that have no trend or seasonality.\nThe SES model can easily be modified to account for trend and seasonality by adding additional components. For example, the Holt’s linear trend method adds a component to account for a linear trend, the damped trend methods flatten the trend some time into the future, and the Holt-Winters model accounts for seasonality. The collection of models generated by adding different components are summarized as Error, Trend, and Seasonality (ETS) models. We apply the ETS model to the deliveries of the electric car company Tesla in the sections below."
  },
  {
    "objectID": "ETS.html#teslas-deliveries",
    "href": "ETS.html#teslas-deliveries",
    "title": "6  ETS",
    "section": "6.2 Tesla’s Deliveries",
    "text": "6.2 Tesla’s Deliveries\nDeliveries are a carefully watched number by Tesla shareholders and are the closest approximation of sales disclosed by the company. Additionally, Tesla’s deliveries are closely followed due to their impact on financial markets, the EV industry, innovation and disruption, production efficiency, and the growth of the EV market. The numbers serve as a key performance indicator for Tesla’s success and provide insights into the broader trends in the electric vehicle industry. Can we use the ETS model to forecast Tesla’s deliveries?"
  },
  {
    "objectID": "ETS.html#sec-data",
    "href": "ETS.html#sec-data",
    "title": "6  ETS",
    "section": "6.3 The Data",
    "text": "6.3 The Data\nThe data can be found here Tesla. Below is code that inputs the data as a tsibble in R.\n\nlibrary(fpp3)\n\n# Create tsibble\ntesla&lt;-tsibble(\n  period=yearquarter(c(\"2016:Q1\",\"2016:Q2\",\"2016:Q3\",\"2016:Q4\",\n                       \"2017:Q1\",\"2017:Q2\",\"2017:Q3\",\"2017:Q4\",\n                       \"2018:Q1\",\"2018:Q2\",\"2018:Q3\",\"2018:Q4\",\n                       \"2019:Q1\",\"2019:Q2\",\"2019:Q3\",\"2019:Q4\",\n                       \"2020:Q1\",\"2020:Q2\",\"2020:Q3\",\"2020:Q4\",\n                       \"2021:Q1\",\"2021:Q2\",\"2021:Q3\",\"2021:Q4\",\n                       \"2022:Q1\",\"2022:Q2\",\"2022:Q3\",\"2022:Q4\",\n                       \"2023:Q1\",\"2023:Q2\")),\n  deliveries=c(14.8,14.4,24.5,22.2,\n               25,22,26.2,29.9,\n               30,40.7,83.5,90.7,\n               63,95.2,97,112,\n               88.4,90.7,139.3,180.6,\n               184.82,201.25,241.3,308.6,\n               310.5,254.7,343.8,405.3,\n               422.9,466.1),\n  index=period     # This is the time variable\n)\n\nAs you can see the tsibble is created with the tsibble() function included in the fpp3 package. The yearquarter() function from the lubridate package is used to coerce the period data to a date. The time variable is then specified via the index parameter. The code below creates the plot of Tesla’s deliveries using the autoplot() function.\n\ntesla %&gt;% autoplot(.vars=deliveries) + theme_classic() +\n  labs(title= \"Tesla Car Deliveries\", \n       subtitle = \"Q1 2017 to Q2 2023\") +\n  xlab(\"Quarter\") + ylab(\" \")\n\n\n\n\nThe most striking aspect of Tesla’s deliveries is the exponential trend. There also seems to be a seasonal component, with relatively higher production Q4 versus the other quarters. These characteristics will be adopted by the ETS model to forecast the series. Below we can see the STL decomposition that confirm these characteristics.\n\ntesla %&gt;%\n  model(STL(deliveries~trend(window=6)+\n              season(window=4), robust=TRUE)) %&gt;%\n  components() %&gt;% autoplot()+ theme_classic()"
  }
]