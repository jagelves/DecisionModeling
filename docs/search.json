[
  {
    "objectID": "Tools2.html#data-wrangling-with-tidyverse.",
    "href": "Tools2.html#data-wrangling-with-tidyverse.",
    "title": "4  Time Series Tools",
    "section": "4.2 Data Wrangling With tidyverse.",
    "text": "4.2 Data Wrangling With tidyverse.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive to the user. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado<-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function imports the data as a tibble (a data structure similar to a data frame). There are three variables that are classified as character, while the rest are double. At this point you can preview the data with either the spec() or glimpse() commands.\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\"\n\n\nWhen using dplyr it’s always helpful to use piping. Generally speaking, piping allows us to chain functions. Piping (%>%) passes the object on the left of the pipe as the first argument to the right of the pipe. We can illustrate this by using the select() and arrange() functions.\n\navocado %>% select(c(average_price,geography)) %>%\n  arrange(desc(average_price)) \n\n# A tibble: 33,045 × 2\n   average_price geography           \n           <dbl> <chr>               \n 1          3.25 San Francisco       \n 2          3.17 Tampa               \n 3          3.12 San Francisco       \n 4          3.05 Miami/Ft. Lauderdale\n 5          3.04 Raleigh/Greensboro  \n 6          3.03 Las Vegas           \n 7          3    San Francisco       \n 8          3    Raleigh/Greensboro  \n 9          2.99 San Francisco       \n10          2.99 Jacksonville        \n# … with 33,035 more rows\n\n\n\navocado %>% filter(date!=ymd(\"2018-01-01\")) -> avocado\n\nThere is a lot to unpack in this line of code. Let’s start with the functions used. Both the select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data. By default it will sort in ascending order, hence we have used the desc() function to use descending order.\nNow, let’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%>%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. That data frame is sorted in descending order so that the highest average avocado price is displayed first.\nThis example highlights the use of dplyr functions to transform your data. There are plenty of other functions you can use, but learning these are outside the scope of this book. To find out more we recommend reading Wickham (2017) chapter 4. For now we will use one more data transformation technique to retrieve average price of organic avocados for California.\n\navocado %>% \n  filter(geography==\"California\", type==\"organic\",\n         year<=2018) %>%\n  select(date, average_price, geography) -> cali\n\nWhereas the select() function chooses particular variables, the filter() function chooses rows of the tibble that meet the conditions listed."
  },
  {
    "objectID": "Tools2.html#the-avocado-data-set",
    "href": "Tools2.html#the-avocado-data-set",
    "title": "4  Time Series Tools",
    "section": "4.1 The Avocado Data Set",
    "text": "4.1 The Avocado Data Set\nTo demonstrate these tools, we will be using the avocado data set. This data is weekly retail scan data for U.S retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLUs) in the data are only for Hass avocados. Other avocados (e.g. greenskins) are not included in this data.\nNote: When inspecting the data you will notice that each entry is recorded weekly. However, there is an entry for 01/01/2018, that is right after 12/31/2017. This is a single observation that is not weekly. You will also note that there are missing dates from 12/02/2018-12/31/2018."
  },
  {
    "objectID": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "href": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "title": "4  Time Series Tools",
    "section": "4.1 Chipotle Wants You to Forecast Avocado Prices",
    "text": "4.1 Chipotle Wants You to Forecast Avocado Prices\nChipotle is an American chain specializing in tacos and burritos that are made to order in front of the customer. Guacamole is the perfect pairing to their delicious food and one of Chipotle’s best sellers. Their guac uses just six ingredients: avocados, lime juice, cilantro, red onion, jalapeño, and kosher salt. Because of its popularity, each restaurant goes through approximately five cases of avocados a day, amounting to more than 44,000 pounds of avocados annually. Chipotle wants you to develop a model to forecast the price of avocados. This model will allow the company to understand the cost of one of it’s most essential product."
  },
  {
    "objectID": "Tools2.html#lessons-learned-in-this-chapter",
    "href": "Tools2.html#lessons-learned-in-this-chapter",
    "title": "4  Time Series Tools",
    "section": "4.9 Lessons Learned in This Chapter",
    "text": "4.9 Lessons Learned in This Chapter\nIn this module you have been introduced to data wrangling, plotting, tsibbles and time series decomposition. You have learned how to:\n\nManipulate dates with lubridate.\nSelect and filter variables using dplyr.\nPlot time series using ggplot.\nApply tsibbles in time series analysis.\nDecompose a series using the model() function in fable.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz."
  },
  {
    "objectID": "Tools2.html#readings",
    "href": "Tools2.html#readings",
    "title": "4  Time Series Tools",
    "section": "4.8 Readings",
    "text": "4.8 Readings\nHyndman (2021) Chapter 1, and Chapter 2.\nWickham (2017) Chapter 2, Chapter 4, and Chapter 19.\ntsibble: https://tsibble.tidyverts.org\nfable: https://fable.tidyverts.org"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Modeling",
    "section": "",
    "text": "Preface\nThis book is based on three topics in Decision Modeling. Mainly, decision tree models, business simulation models and time series models. Special attention is given to business applications.\nComments are welcomed at jagelves@wm.edu"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Gelves, J. Alejandro. 2022. “Business Statistics.” https://jagelves.github.io/BusinessStatistics/.\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/#license.\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.”\nhttps://otexts.com/fpp3/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz.\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical\nManagement Science.”"
  },
  {
    "objectID": "Benchmarks.html#benchmarks",
    "href": "Benchmarks.html#benchmarks",
    "title": "5  Model Benchmarks",
    "section": "5.1 Benchmarks",
    "text": "5.1 Benchmarks\nOne of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A naive prediction sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T\\)\n\nwhere \\(\\hat y_{T+h}\\) is the predicted value for \\(h\\) periods ahead, and \\(y_T\\) is the value observed at the current time period \\(T\\). We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would “drift” the naive prediction upward. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T+h(\\frac{y_t-y_1}{T-1})\\)\n\nwhere \\(h(\\frac{y_t-y_1}{T-1})\\) can be thought as the average increase of \\(y\\) from period \\(1\\) to the current period \\(T\\). One could also predict weight by observing weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:\n\n\\(\\hat y_{T+h}=\\frac{(y_1+y_2+...+y_T)}{T}\\)\n\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:\n\n\\(\\hat y_{T+h}=b_0+b_1(T+h)\\)"
  },
  {
    "objectID": "Benchmarks.html#accuracy",
    "href": "Benchmarks.html#accuracy",
    "title": "5  Model Benchmarks",
    "section": "5.3 Accuracy",
    "text": "5.3 Accuracy\nWe will assess the fit of the benchmarks by comparing the fitted values against actual values. Generally, a good fit is determined by how far the fitted values are from the actual ones. If we square all of the distances (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE). Formally:\nHow we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE’s but then find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since the measures are the smallest. We have highlighted these results and made the table more appealing by using the gt library.\n\naccuracy(fit)\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      geography\n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    California\nmean\nTraining\n0.00\n0.27\n0.22\n−2.50\n12.94\n2.25\n2.02\n0.86\n    California\nNaive\nTraining\n0.00\n0.13\n0.10\n−0.03\n5.72\n1.00\n1.00\n−0.19\n    California\nDrift\nTraining\n0.00\n0.13\n0.10\n−0.31\n5.73\n1.00\n1.00\n−0.19\n    California\nLS\nTraining\n0.00\n0.25\n0.19\n−2.00\n11.34\n2.02\n1.84\n0.85"
  },
  {
    "objectID": "Benchmarks.html#leasons-learned",
    "href": "Benchmarks.html#leasons-learned",
    "title": "5  Model Benchmarks",
    "section": "5.8 Leasons Learned",
    "text": "5.8 Leasons Learned\nIn this module you have been introduced to the general procedure in forecasting time series. Particularly you have learned to:\n\nCreate forecasts with simple heuristics.\nAssess the fit of the model with accuracy measures.\nCreate a test set and train set to avoid over-fitting.\nPerform cross validation.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "Benchmarks.html#readings",
    "href": "Benchmarks.html#readings",
    "title": "5  Model Benchmarks",
    "section": "5.7 Readings",
    "text": "5.7 Readings\nHyndman (2021) Chapter 5 (The Forcaster’s Toolbox).\ngt package: https://gt.rstudio.com"
  },
  {
    "objectID": "Tools.html#r-basics",
    "href": "Tools.html#r-basics",
    "title": "1  Tools for Working With Simulation",
    "section": "1.1 R Basics",
    "text": "1.1 R Basics\nObjects, vectors and data frames are all important in the R programming language. They mainly help us store and manipulate data. An object is a piece of data that can be stored in a variable. It can be as simple as a single integer or as output of regression analysis. Vectors are one-dimensional arrays of data that cn be stored in an object. Vectors can contain elements of various data types, such as numerical values, character strings, and logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Lastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record."
  },
  {
    "objectID": "Tools.html#r-intermediate",
    "href": "Tools.html#r-intermediate",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 R Intermediate",
    "text": "1.2 R Intermediate\nLoops and conditionals\nrandom number generators"
  },
  {
    "objectID": "Tools.html#readings",
    "href": "Tools.html#readings",
    "title": "2  Tools for Working With Simulation",
    "section": "2.6 Readings",
    "text": "2.6 Readings\nThese reading will help you review the concepts and theory necessary for completing this module. Grolemund (2014) reviews the R basics needed to perform computer simulation, Gelves (2022) has several applied problems in R to review the probability concepts necessary to understand the different random number generators, while Winston and Albright (2019) provides an application of the distributions to business simulation.\nGrolemund (2014) Chapter 1 (The Very Basics), Chapter 3 (R Objects), Chapter 7.2, 7.3 (Conditional Statements), Chapter 9.3, 9.4, 9.5 (Loops).\nGelves (2022) Chapter 10 (Discrete Random Variables), Chapter 11 (Continuous Random Variables). This is mainly review from your probability course. It is recommended you attempt the exercises in both chapters (solutions are provided at the end).\nWinston and Albright (2019) Chapter 10.1 (Introduction) and 10.2 (Probability Distributions for Input Variables). Pay special attention to the probability distributions and try to replicate the examples in R.\nJaggia and Kelly (2022) Chapter 5 (Discrete Probability Distributions) and Chapter 6 (Continuous Probability Distributions)."
  },
  {
    "objectID": "Tools.html#storing-our-data-in-r",
    "href": "Tools.html#storing-our-data-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.1 Storing Our Data in R",
    "text": "2.1 Storing Our Data in R\nObjects, vectors and data frames are all important in the R programming language. They are useful when storing and manipulating data in R. An object is a piece of data that can be stored in a variable. It can be as simple as a single integer or as informative as the output in regression analysis. The code below creates an object x that stores the number \\(5\\).\n\nx&lt;-5\n\nVectors are one-dimensional arrays of data that can be stored in an object. They can contain elements of various data types, such as numerical values, character, or logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Below, the vector Books stores the titles of \\(5\\) monster classics a bookstore plans to release as characters.\n\nbooks&lt;-c(\"Frankenstein\",\"Dracula\",\"Moby Dick\",\n         \"War Of The Worlds\",\"Beowulf\")\n\nLastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record. You can think of a data frame as a collection of vectors that are related to each other. We can easily construct a data frame by combining one or more vectors using the data.frame() function in R.\n\ndata.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45))\n\n              Books Price\n1      Frankenstein  9.50\n2           Dracula  5.78\n3         Moby Dick  6.34\n4 War Of The Worlds  5.67\n5           Beowulf  2.45"
  },
  {
    "objectID": "Tools.html#automating-the-trial-process-in-r",
    "href": "Tools.html#automating-the-trial-process-in-r",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 Automating The Trial Process in R",
    "text": "1.2 Automating The Trial Process in R\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions. A loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true.\n\n\n\n\n\nConditionals, on the other hand, allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. By using loops and conditionals, you can create more complex and powerful programs in R."
  },
  {
    "objectID": "Tools.html#generating-random-numbers-in-r",
    "href": "Tools.html#generating-random-numbers-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.2 Generating Random Numbers in R",
    "text": "2.2 Generating Random Numbers in R\nThere are several functions available in R that can be used to generate random numbers. These functions are based on a specific probability distribution. For instance, the rbinom() function generates random numbers based on the binomial distribution, while the rnorm() function generates random numbers based on the normal distribution. By using these functions, we can generate random numbers that follow a specific probability distribution. For example, the binomial distribution may be useful in estimating the probability that a certain number of customers respond to a marketing campaign. However, as we will see below, we can also use the distribution to generate random customers who either responded or not responded to the campaign.\nAssume the bookstore is unsure on how many customers will buy their Monster Classic Series at list price. Their plan is to send \\(100\\) catalogs by mail to potential customers. Before they send the catalogs, they decide to get an estimate on demand. Past data reveals that the probability a customer would buy any of the titles at the given prices is \\(0.70\\). Let’s modify our data frame by simulating demand with the rbinom() function.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=rbinom(5,100,0.7)))\n\n              Books Price Demand\n1      Frankenstein  9.50     74\n2           Dracula  5.78     71\n3         Moby Dick  6.34     69\n4 War Of The Worlds  5.67     62\n5           Beowulf  2.45     71\n\n\nAs you can see, the rbinom() function has yielded \\(5\\) simulated outcomes from the binomial experiment with a probability of \\(0.7\\). With these demands the bookstore can prepare for different demand scenarios as well as assessing the profitability of the Monster Series. Below you can see a table that summarizes some common distributions and their respective functions in R.\n\n\n\nDistribution\nFamily\nPackage\nFunction\n\n\n\n\nUniform\nDiscrete\nextraDistr\nrdunif()\n\n\nBinomial\nDiscrete\nBase R\nrbinom()\n\n\nHypergeometric\nDiscrete\nBase R\nrhyper()\n\n\nPoisson\nDiscrete\nBase R\nrpois()\n\n\nUniform\nContinuous\nBase R\nrunif()\n\n\nNormal\nContinuous\nBase R\nrnorm()\n\n\nExponential\nContinuous\nBase R\nrexp()\n\n\nTriangle\nContinuous\nextraDistr\nrtriang()"
  },
  {
    "objectID": "Tools.html#company-x-want-your-help",
    "href": "Tools.html#company-x-want-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Want Your Help",
    "text": "1.4 Company X Want Your Help"
  },
  {
    "objectID": "Tools.html#leasons-learned-in-this-chapter",
    "href": "Tools.html#leasons-learned-in-this-chapter",
    "title": "1  Tools for Working With Simulation",
    "section": "1.5 Leasons Learned In This Chapter",
    "text": "1.5 Leasons Learned In This Chapter"
  },
  {
    "objectID": "Tools.html#using-loops-and-conditional-in-r",
    "href": "Tools.html#using-loops-and-conditional-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Using Loops and Conditional in R",
    "text": "2.3 Using Loops and Conditional in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to easily generate new variables for our model, or test different variations of our parameters to see how the model behaves.\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true. Below you can see the structure of the while loop.\n\n\n\n\n\nLet’s illustrate the syntax of the for loop by simulating a demand for each book and calculating the revenue generated by each book in the Monster Classic series.\n\nRevenue&lt;-c()\n\nfor (i in MS$Price) {\n  Revenue&lt;-c(Revenue,i*rnorm(1,50,2))\n  Revenue\n}\n\nConditionals allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. Below is the structure of the conditional statement.\n\n\n\n\n\nLet’s go back to the Monster Classic example and assume that the bookstore has gained additional insight on the demand of their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand&lt;-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand&lt;-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 87 88 76 80 74\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, then the random binomial number is drawn with the probability \\(0.9\\), if not the it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series one by one and adds a simulated demand. You can quickly realize that this becomes very efficient if the bookstore has a very large collection of books. Below is our data frame with the new simulated values.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     87\n2           Dracula  5.78     88\n3         Moby Dick  6.34     76\n4 War Of The Worlds  5.67     80\n5           Beowulf  2.45     74"
  },
  {
    "objectID": "Tools.html#company-x-wants-your-help",
    "href": "Tools.html#company-x-wants-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Wants Your Help",
    "text": "1.4 Company X Wants Your Help"
  },
  {
    "objectID": "Tools.html#lessons-learned-in-this-chapter",
    "href": "Tools.html#lessons-learned-in-this-chapter",
    "title": "2  Tools for Working With Simulation",
    "section": "2.7 Lessons Learned In This Chapter",
    "text": "2.7 Lessons Learned In This Chapter\n\nGenerate random numbers using R functions.\nUse Loops and Conditionals to simulate variables.\nApply objects, vectors, and data frames to store and manipulate data.\n\n\n\n\n\nGelves, J. Alejandro. 2022. “Business Statistics.” https://jagelves.github.io/BusinessStatistics/.\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/#license.\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. “Business Statistics.”\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 The VA Deaprtment of Transportation Wants Your Services",
    "text": "2.4 The VA Deaprtment of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they schedule the expected staff and number of ferries to run.\nAssume that the VA Department of transportation shares three weeks of data. The table below records the number of vehicles that used the ferry service:\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\n\n\n\n\nMon\n1175\n1020\n1163\n\n\nTue\n1198\n1048\n1066\n\n\nWed\n1189\n1102\n1183\n\n\nThu\n1175\n1094\n1003\n\n\nFri\n1101\n1042\n1095\n\n\nSat\n1529\n1464\n1418\n\n\nSun\n1580\n1534\n1512\n\n\n\nWhat distribution would you use to simulate weekdays (Mon-Fri)? Would you simulate weekends (Sat and Sun) differently than weekdays? According to the data, what would be the minimum and maximum number of vehicles transported during weekdays (weekends)? Can you provide a sensible simulation for week 4?"
  },
  {
    "objectID": "Simulation.html#inputs-calculated-and-objective",
    "href": "Simulation.html#inputs-calculated-and-objective",
    "title": "2  Simulation in R",
    "section": "2.1 Inputs, Calculated, and Objective",
    "text": "2.1 Inputs, Calculated, and Objective"
  },
  {
    "objectID": "Simulation.html#law-of-large-numbers",
    "href": "Simulation.html#law-of-large-numbers",
    "title": "3  Simulation in R",
    "section": "3.4 Law of Large Numbers",
    "text": "3.4 Law of Large Numbers\nBefore we answer the question of how much fish to order, we must realize a couple of flaws of the model we have created. Mainly, we have run the simulation once and it is unlikely (although possible) that the attendance will be exactly \\(44\\). Instead, we want to provide the restaurant with a set of eventualities. Worst case scenarios (low attendance), best case scenarios (high attendance), and most likely outcome for their decision. This is only possible if we generate several attendance numbers, and see how the output behaves.\nAn additional problem is that if we provide the average profit of our model, we want to make sure that the average is not biased. Recall that as the sample size of a study increases, the average of the sample will converge towards the true population mean. In other words, as the number of simulations in our model increases, the estimate of the expected profit becomes more and more accurate. This is known as the Law of Large Numbers.\nThe code below repeats the simulation not once, or twice, but several times. Although, there is not a set number of times one should run a simulation to get a good estimate of the mean (or distribution), computers are powerful enough to run thousands if not millions of simulations. Below we run the simulation 10,000 times for illustration purposes.\n\nn&lt;-10000\nV_Order_Oz&lt;-rep(Order_Oz,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance&lt;-round(rtriang(n,20,50,30),0)\nConsumption&lt;-V_Attendance*V_Fish_Entitled_Oz\nAvailable&lt;-V_Order_Oz\n\nV_Profit&lt;-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\n\nFrom the simulation it seems like that the restaurant would make on average a profit of about 211.5 dollars if they order 160 ounces of fish. There are however, a couple of questions left unanswered. First, what are the other possible profit when ordering 160 ounces? Second, is there another amount of fish that would give them a higher expected profit?"
  },
  {
    "objectID": "Simulation.html#the-flaw-of-averages",
    "href": "Simulation.html#the-flaw-of-averages",
    "title": "2  Simulation in R",
    "section": "2.6 The Flaw of Averages",
    "text": "2.6 The Flaw of Averages\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population. For example, if a group of people has an average height of 5’10”, it does not mean that every individual in the group is exactly 5’10” tall. Some people may be shorter, while others may be taller. The flaw of averages can lead to inaccurate assumptions and incorrect conclusions, particularly when making predictions or decisions based on the average value. It is important to consider the distribution and variability of the characteristic within the population when making predictions or decisions, rather than relying solely on the average value."
  },
  {
    "objectID": "Simulation.html#the-newsvendor-problem",
    "href": "Simulation.html#the-newsvendor-problem",
    "title": "2  Simulation in R",
    "section": "2.5 The Newsvendor Problem",
    "text": "2.5 The Newsvendor Problem\nThe news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order and at what price to sell it. The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them. In the news vendor problem, there are two main factors that influence the decision: the demand for the product and the cost of the product. The demand for the product is uncertain, as it depends on various factors such as the weather, the competition, and the price of the product. The cost of the product is also uncertain, as it may vary based on factors such as production costs, transportation costs, and discounts. The goal of the news vendor is to maximize profits by ordering the optimal amount of the product and setting the optimal price. The news vendor must make this decision based on incomplete information about the demand and cost of the product, which makes the problem challenging and requires the use of probabilistic models and optimization techniques."
  },
  {
    "objectID": "Simulation.html#sensitivity-analysis",
    "href": "Simulation.html#sensitivity-analysis",
    "title": "3  Simulation in R",
    "section": "3.7 Sensitivity Analysis",
    "text": "3.7 Sensitivity Analysis\nSensitivity analysis is a tool used in decision-making to assess the robustness of a model or decision by evaluating the impact of changes in certain key input variables on the output of the model or decision. Sensitivity analysis helps to identify which variables are most important and how sensitive the output is to changes in those variables. It is often used in financial, economic, and engineering contexts to evaluate the feasibility and risk of different scenarios or to identify potential areas of improvement."
  },
  {
    "objectID": "Simulation.html#a-restaurant-needs-your-help",
    "href": "Simulation.html#a-restaurant-needs-your-help",
    "title": "2  Simulation in R",
    "section": "2.1 A Restaurant Needs Your Help",
    "text": "2.1 A Restaurant Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. They purchase the fish at the market and discard any unsold fish at the end of the day. However, they are unsure of how many people will attend the event. To ensure they have enough sushi and budget appropriately, they want to estimate the total cost and amount of sushi needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (which is divisible). Each guest at the festival is entitled to 5 ounces of sushi. If the sushi runs out, Sushi X has promised to serve their famous Miso soup, which cost them a total of 300 dollars for the event. Given that the festival charges 20 dollars per entry, how much yellow-tail would you recommend Sushi X purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#model-framework",
    "href": "Simulation.html#model-framework",
    "title": "3  Simulation in R",
    "section": "3.2 Model Framework",
    "text": "3.2 Model Framework\nThe restaurant provides you with a lot of information which might be overwhelming at first glance. To make the task less daunting, we should organize/classify the information so that we can create a model. In general, for many business problems, the data can be classified into the following parts:\n\nThe inputs have given fixed values and provide the model’s basic structure. These are values that are most likely to be given and determined.\nThe decision variables are values the decision maker controls. We are usually interested in finding the optimal level of this variable.\nThe calculated values transform inputs and decision variables to other values that help describe our model. These make the model more informative and often are required to derive outputs.\nThe random variables are the primary source of uncertainty in the model. They are often modeled by sampling probability distributions.\nThe outputs are the ultimate values of interest; the inputs, decision variables, random variables, or calculated values determine them.\n\nBelow, you can see how we can classify and input the information in R.\n\nlibrary(extraDistr)\nOrder_Oz&lt;-160 # Decision Variable\n\nPrice_Fish_Oz&lt;-9/16 # Input\nPrice_Miso&lt;-300 # Input\nEntry_Fee&lt;-20 # Input\nFish_Entitled_Oz&lt;-5 #Input\n\nset.seed(20)\nAttendance&lt;-round(rtriang(1,20,50,30),0) # Random Variable\n\nConsumption&lt;-Attendance*Fish_Entitled_Oz # Calculated \nAvailable&lt;-Order_Oz # Calculated\n\nProfit&lt;-min(Consumption,Available)/Fish_Entitled_Oz*Entry_Fee-Order_Oz*Price_Fish_Oz-Price_Miso #Outcome\n\nIn the model above, you can see how the inputs have fixed values. These values were given to us by the restaurant and it seems like the they have little or no control over them (the market price of the fish, the cost of making Miso soup, etc.). The random variable, captures the source of uncertainty (i.e., how many people attend the event). As you can see we have used here the rtriang() function from the extraDistr package to generate the attendance. We have chosen the triangle distribution since the restaurant has provided us with a lower limit, an upper limit, and a most likely case for attendance. Note also the use of the set.seed() function. This allows you to replicate the random numbers generated.\nThe calculated variables combine inputs, the decision variable, and the random variable to provide insight on how much is fish is expected to be both consumed and available. They also help us determine the output, which is our main guide in knowing whether the decision of ordering \\(x\\) ounces of fish is the “best”."
  },
  {
    "objectID": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "href": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "title": "3  Simulation in R",
    "section": "3.1 Roll With It Sushi Needs Your Help",
    "text": "3.1 Roll With It Sushi Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. Every morning, the Sous-Chef purchases fish at the market and any unsold fish at the end of the day gets discarded.\nThe restaurant has contacted you because they are still determining how many people will attend the event and worry about how this will impact their business financially. To ensure they have enough sushi and budget appropriately, they want you to recommend the amount of fish needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people, and without your guidance they feel like this is the best guide in determining the fish needed.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (divisible). Each guest at the festival is entitled to 5 ounces of fish. If the sushi runs out, some customers will not be happy. However, the restaurant has promised to refund their entry fee. Additionally, they have promised to serve their famous Miso soup to every attendee. The cost of making a batch for up to 50 guests is 300 dollars.\nGiven that the festival charges 20 dollars per entry, how much yellow-tail would you recommend the restaurant to purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#the-news-vendor-problem",
    "href": "Simulation.html#the-news-vendor-problem",
    "title": "3  Simulation in R",
    "section": "3.3 The News Vendor Problem",
    "text": "3.3 The News Vendor Problem\nNote how the decision variable (Order_Oz) affects directly our outcome (Profit). We can see that it decreases the restaurant’s profit through costs, but also affects revenue through the amount of fish available. This is the heart of the problem. We don’t know how many people will attend, so if the restaurant orders too much fish their profits will go down because their costs are large. However, if they order too little then they will have to issue refunds, which decrease their revenue.\nAs you observe the Profit formula in the code above, you’ll notice the use of the min() function. This function returns the minimum of the Attendance and Consumption. The intuition here is that the restaurant can only collect entry fees for the people who consumed the sushi when the attendance is greater than the amount of sushi available. Likewise, their revenue will be capped at the total amount of people who attended, even if they ordered plenty of fish.\nThe problem illustrated above is called the news vendor problem. The news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order (and sometimes at what price to sell it). The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them."
  },
  {
    "objectID": "Simulation.html#recommendation",
    "href": "Simulation.html#recommendation",
    "title": "2  Simulation in R",
    "section": "2.5 Recommendation",
    "text": "2.5 Recommendation\nThere are a couple of things that are now evident. It seems likely that the restaurant would make on average a profit of about 234 dollars if they order 160 ounces of fish. We can now inform the restaurant that it is likely they will make a profit if they order 160 ounces. There are a couple of questions left unanswered. First, 234 dollars are expected of profits, what are the other possible outcomes when ordering 160 ounces? Secondly, is there another amount of fish that would give them a higher expected profit?\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering 160 ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to 250 dollars. So a better recommendation to the restaurant would be to inform them that when ordering 160 ounces, they will most likely get a profit of 250 dollars. There is a small risk of them making less that 100 dollars, but that they should not expect more than 250 dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is commonly known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population.\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz<-9/16 # Input\nPrice_Miso<-300 # Input\nEntry_Fee<-20 # Input\nFish_Entitled_Oz<-5 #Input\n\nProfits<-c()\n\nfor (i in Order_Oz){\nn<-10000\nV_Order_Oz<-rep(i,n)\nV_Price_Fish_Oz<-rep(Price_Fish_Oz,n)\nV_Price_Miso<-rep(Price_Miso,n)\nV_Entry_Fee<-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz<-rep(Fish_Entitled_Oz,n)\n\nV_Attendance<-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption<-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable<-V_Order_Oz # Calculated\n\nV_Profit<-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits<-c(Profits,mean(V_Profit))\n}\n\n(results<-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order Profits\n1   160 212.934\n2   170 225.413\n3   180 235.406\n4   190 241.335\n5   200 242.848\n6   210 243.837\n7   220 240.640\n8   230 234.759\n9   240 230.958"
  },
  {
    "objectID": "Simulation.html#flaw-of-averages",
    "href": "Simulation.html#flaw-of-averages",
    "title": "3  Simulation in R",
    "section": "3.5 Flaw Of Averages",
    "text": "3.5 Flaw Of Averages\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering 160 ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to 250 dollars. So a better recommendation to the restaurant would be to inform them that when ordering 160 ounces, they will most likely get a profit of 250 dollars. There is a small risk of them making less that 100 dollars, but that they should not expect more than 250 dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population."
  },
  {
    "objectID": "Simulation.html#optimal-order-amount",
    "href": "Simulation.html#optimal-order-amount",
    "title": "3  Simulation in R",
    "section": "3.6 Optimal Order Amount",
    "text": "3.6 Optimal Order Amount\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz&lt;-9/16 # Input\nPrice_Miso&lt;-300 # Input\nEntry_Fee&lt;-20 # Input\nFish_Entitled_Oz&lt;-5 #Input\n\nProfits&lt;-c()\n\nfor (i in Order_Oz){\nn&lt;-10000\nV_Order_Oz&lt;-rep(i,n)\nV_Price_Fish_Oz&lt;-rep(Price_Fish_Oz,n)\nV_Price_Miso&lt;-rep(Price_Miso,n)\nV_Entry_Fee&lt;-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz&lt;-rep(Fish_Entitled_Oz,n)\n\nV_Attendance&lt;-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption&lt;-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable&lt;-V_Order_Oz # Calculated\n\nV_Profit&lt;-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits&lt;-c(Profits,mean(V_Profit))\n}\n\n(results&lt;-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order Profits\n1   160 212.934\n2   170 225.413\n3   180 235.406\n4   190 241.335\n5   200 242.848\n6   210 243.837\n7   220 240.640\n8   230 234.759\n9   240 230.958\n\n\nThis table suggests that 160 ounces is not optimal. Once again highlighting that the average attendance is not a good estimate of how much fish we should order. Instead, we can see that 210 ounces of fish should be ordered (this feeds about 42 people) to maximize the expected profits."
  },
  {
    "objectID": "Decisions.html#expected-value",
    "href": "Decisions.html#expected-value",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Expected Value",
    "text": "1.1 Expected Value\nImagine you are the CEO of a retail company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome.\nTo make an informed decision, you assess the probabilities of each outcome based on your market research, historical data, and/or expert opinions. For example, you might estimate a 60% chance of success and a 40% chance of failure.\nUsing this information, you can calculate the expected monetary value of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\n\nThe expected monetary value estimates the average monetary outcome you can expect from the decision. In this example, the expected value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EV\\) is the expected value, \\(p_{i}\\) is the probability of decision \\(i\\) and \\(x_{i}\\) is the monetary values resulting from decision \\(i\\). Notice that if we had other decisions, we could also calculate their expected value. Hence, the expected value allows us to rank and choose decisions that yield the highest monetary value on average."
  },
  {
    "objectID": "Decisions.html#decision-trees",
    "href": "Decisions.html#decision-trees",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.3 Decision Trees",
    "text": "1.3 Decision Trees\nDecision trees map the entire decision process. Below you can see the decision process for the new production line.\n\n\n\n\ngraph LR\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( 0 )\n    C --&gt;|Success p=0.6| D( 5 )\n    C --&gt;|Failure p=0.4| E( -3 )\n\n\n\n\n\nThe decision tree is read from left to right. Notice that there are two main branches stemming from the first decision node (i.e., the first square to the left). You can decide to Introduce or Not Introduce. If you Introduce, you reach a probability node (i.e., the circle that leads to the success and failure branches) where chance determines success or failure. In the end, your choice of introducing the new product line yields an expected monetary value of 1.8 million, whereas not introducing the line yields no payoff.\nTo solve decision trees and find the optimal decision use backward induction. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision. Applying this procedure to our simple example, we would start with the probability node that leads to the success and failure branches and calculate the EMV of 1.8 million. Moving back to our initial decision, we now have the choice to Introduce or Not Introduce. In this case we should Introduce, since the EMV (1.8) is higher than Not Introduce (0)."
  },
  {
    "objectID": "Decisions.html#bayes-theorem",
    "href": "Decisions.html#bayes-theorem",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Bayes’ Theorem",
    "text": "1.5 Bayes’ Theorem"
  },
  {
    "objectID": "Decisions.html#readings",
    "href": "Decisions.html#readings",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.6 Readings",
    "text": "1.6 Readings\nReadings for this chapter are mainly from Winston and Albright (2019). Chapter 9 provides a good introduction to decision models with a couple of solved problems included using excel. I would recommend using R to make calculations instead. To construct decision trees you can use mermaid.\nSome concepts are important to review before you start reading the chapter. In particular, discrete random variables, expected value, conditional probability, probability rules, and Bayes’ theorem. These concepts are not explained in depth in the readings, so reviewing them before reading the chapter could be helpful.\nWinston and Albright (2019) Chapters 9.1 (Introduction), 9.2 (Elements of Decision Analysis), 9.3 (Single-Stage Decision Problems) and 9.5 (Multistage Decision Problems). It is recommended you follow along in R as opposed to Excel (or Precision Tree Add-In).\nJaggia and Kelly (2022) Chapter 4.1 (Fundamental Probability Concepts), 4.2 (Rules of Probability), 4.3 (Contingency Tables and Probabilities), and 4.4 (The Total Probability Rule and Bayes’ Theorem)\nhttps://quarto.org/docs/authoring/diagrams.html"
  },
  {
    "objectID": "Decisions.html#lessons-learned",
    "href": "Decisions.html#lessons-learned",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Lessons Learned",
    "text": "1.7 Lessons Learned\n\nUse the concept of Expected Monetary Value\nUse Decision Trees to map the business decision process\nApply the backward induction method to solve single-stage and multistage decision problems\n\n\n\n\n\nWinston, Wayne. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Simulation.html#lessons-learned-in-this-chapter",
    "href": "Simulation.html#lessons-learned-in-this-chapter",
    "title": "3  Simulation in R",
    "section": "3.9 Lessons Learned In This Chapter",
    "text": "3.9 Lessons Learned In This Chapter\n\nIdentify the parts of a simulation model.\nCreate a simulation model in R.\nIdentify the Flaw of Averages.\nFind optimal values in simulation models\n\n\n\n\n\nWinston, Wayne. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "Simulation.html#readings",
    "href": "Simulation.html#readings",
    "title": "3  Simulation in R",
    "section": "3.8 Readings",
    "text": "3.8 Readings\nWinston (2019) Chapter 10.3 (Simulation and the Flaw of Averages), and 10.4 (Simulation with Built-In Excel Tools). Example 10.4 (Additional Uncertainty at Walton Bookstore) It is recommended you follow along using R instead of Excel."
  },
  {
    "objectID": "Tools2.html#visualizing-the-data",
    "href": "Tools2.html#visualizing-the-data",
    "title": "4  Time Series Tools",
    "section": "4.5 Visualizing The Data",
    "text": "4.5 Visualizing The Data\nTo visualize the data, we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines the mapping of variables. The geom_line() function specifies the type of plot. In time series, we will use the line plot regularly. Labels for the graph are easily set with the labs function and there are plenty of themes available to customize your visualization. Below, the theme_classic() is displayed. To learn more about the ggplot package, you can refer to Wickham (2017) chapter 2. Below is the code to create a line plot of California’s average avocado price.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price, group=geography),color=\"black\") +\n  theme_classic() + \n  labs(x=\"\",\n       y=\"Average Price\", \n       title=\"Organic Avocado Price in California\",\n       subtitle=\"2015-2018\")  \n\n\n\n\nThe average price of avocados in California has been increasing during the period considered. It reached a maximum of about 2.6 in 2017 and was at a minimum in the spring of 2015. There is also a seasonal pattern with low prices at the beginning of the year and peaks mid-year. As we will see in upcoming chapters, these are patterns that can be extrapolated and used to forecast time series."
  },
  {
    "objectID": "Tools2.html#setting-up-the-date-variable",
    "href": "Tools2.html#setting-up-the-date-variable",
    "title": "4  Time Series Tools",
    "section": "4.3 Setting Up the Date Variable",
    "text": "4.3 Setting Up the Date Variable\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\""
  },
  {
    "objectID": "Tools2.html#tssibbles-and-decomposition",
    "href": "Tools2.html#tssibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.4 tssibbles and Decomposition",
    "text": "4.4 tssibbles and Decomposition\n\nlibrary(fpp3, quietly = T, warn.conflicts = F)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tsibble     1.1.3     ✔ feasts      0.3.0\n✔ tsibbledata 0.4.1     ✔ fable       0.3.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-31\")-> avocadots\n\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) -> calits\n\ncalits  %>% mutate(\n  `10-MA`=slider::slide_dbl(average_price,mean,\n                            .before=5, .after=4,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`10-MA`), col=\"red\")\n\nWarning: Removed 9 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits  %>% mutate(\n  `2X10-MA`=slider::slide_dbl(`10-MA`,mean,\n                           .before=1, .after=0,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`2X10-MA`), col=\"red\")\n\nWarning: Removed 10 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-31\") %>%\n  model(STL(average_price~trend(window=150)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()"
  },
  {
    "objectID": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "href": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "title": "4  Time Series Tools",
    "section": "4.3 Loading tidyverse and Inspecting the Data.",
    "text": "4.3 Loading tidyverse and Inspecting the Data.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive to the user. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado<-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function imports the data as a tibble (a data structure similar to a data frame). The output informs us that three variables are classified as a character, while the rest are double. You can preview the data with either the spec() or glimpse() commands.\n\nspec(avocado)\n\ncols(\n  date = col_character(),\n  average_price = col_double(),\n  total_volume = col_double(),\n  `4046` = col_double(),\n  `4225` = col_double(),\n  `4770` = col_double(),\n  total_bags = col_double(),\n  small_bags = col_double(),\n  large_bags = col_double(),\n  xlarge_bags = col_double(),\n  type = col_character(),\n  year = col_double(),\n  geography = col_character()\n)\n\n\nYou will notice that the date variable is of type character. We can use the lubridate package to coerce this variable to a date. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function. You can learn more about this package in Wickham (2017) Chapter 19.\n\nlibrary(lubridate)\navocado$date<-mdy(avocado$date)"
  },
  {
    "objectID": "Tools2.html#piping-and-dplyr",
    "href": "Tools2.html#piping-and-dplyr",
    "title": "4  Time Series Tools",
    "section": "4.4 Piping and dplyr",
    "text": "4.4 Piping and dplyr\ndplyr is commonly used with “piping”. Generally speaking, “piping” allows us to chain functions. “Piping” (%>%) passes the object on the left of the pipe as the first argument in the function to the right of the pipe. We can illustrate this using the select() and arrange() functions.\n\navocado %>% select(c(average_price,geography)) %>%\n  arrange(desc(average_price)) %>% head(5)\n\n# A tibble: 5 × 2\n  average_price geography           \n          <dbl> <chr>               \n1          3.25 San Francisco       \n2          3.17 Tampa               \n3          3.12 San Francisco       \n4          3.05 Miami/Ft. Lauderdale\n5          3.04 Raleigh/Greensboro  \n\n\nThere is a lot to unpack in this line of code. Let’s start with the functions used. The select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data in ascending order. The desc() function is used to sort in descending order.\nLet’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%>%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. The data frame is sorted in descending order so that the highest average avocado price is displayed first. Finally, the head() function is used to retrieve the top five entries.\nAs noted in Section 4.2, there is an additional date in the data set that is between weeks (2018-01-01). We can remove this observation by using the filter() function.\n\navocado %>% filter(date!=ymd(\"2018-01-01\")) -> avocado\n\nYou should notice that whereas the select() function chooses particular variables (i.e., columns), the filter() function chooses rows of the tibble that meet the conditions listed. Note also that the filtered data set is assigned (->) to avocado overwriting the older object.\nThe examples above highlight the use of dplyr functions to transform your data. There are plenty of other functions, but learning these are outside the scope of this book. To find out more, I recommend reading Wickham (2017) Chapter 4. For now, we will use one more data transformation technique to retrieve California’s average price of organic avocados for 2015-2018.\n\navocado %>% \n  filter(geography==\"California\", type==\"organic\", \n         year<=2018) %>%\n  select(date, average_price, geography) -> cali"
  },
  {
    "objectID": "Tools2.html#tsibbles-and-decomposition",
    "href": "Tools2.html#tsibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.5 tsibbles and Decomposition",
    "text": "4.5 tsibbles and Decomposition\nAll though tibbles are a great data structure, when dealing with time series, there is a time component that is crucial when analyzing the data. As a consequence, we will be using a data structure called a tsibble (time series tibble). tsibbles are defined by a time index (i.e., the date), and some keys (i.e., some dimensions). In the avocado data set we are mainly interested in the average price of the avocados. You will note however that they are classified by location (geography) and type (e.g., organic and conventional). tsibbles, as well a variety of packages that help us analyze time series, are part of the fpp3 package. Below we load the package, and coerce our avocado tibble to a tsibble.\n\nlibrary(fpp3)\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-> avocadots\n\nNote that the as_tsibble() function was called with the parameter regular set at true indicating that the date has no gaps and occurs every week. The function filter_index() is called as it allows us to determine the window for analysis. As noted in section 5.1, there are some missing dates in December 2018. We limit the analysis to 2015-2018.\nRecall, that we are interested in the average price of avocados for califronia. We can specify the tsibble for analysis, by using dplyr.\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) -> calits"
  },
  {
    "objectID": "Tools2.html#decomposition",
    "href": "Tools2.html#decomposition",
    "title": "4  Time Series Tools",
    "section": "4.6 Decomposition",
    "text": "4.6 Decomposition\nAs mentioned above, the avocado data for California seems to have a trend and a seasonal pattern. There are methods available to tease out these components. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run in R by using the command below:\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-02\") %>%\n  model(STL(average_price~trend(window=200)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()\n\n\n\n\nAs shown above, the trend is increasing and the seasonal component confirms low levels at the beginning of the year and high levels in the summer.\nThe decomposition itself is constructed by first finding a moving average of the series to track the trend. As you can see the window in the trend is set to a high window so that moving average tracks the general trend and not the small fluctuations of the series. This trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly). The error is the remaining fluctuation of the series that is not explained by the trend or the seasonal component (Series-Trend-Seasonal=Error)."
  },
  {
    "objectID": "Benchmarks.html#predicting-the-avocado-data",
    "href": "Benchmarks.html#predicting-the-avocado-data",
    "title": "5  Model Benchmarks",
    "section": "5.2 Predicting the Avocado Data",
    "text": "5.2 Predicting the Avocado Data\nLet’s apply the forecasting methods to the average prices for avocados in California. Start by loading the fpp3 package and importing the data https://jagelves.github.io/Data/CaliforniaAvocado.csv.\n\nlibrary(tidyverse)\nlibrary(fpp3)\ncali<-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\nRecall, that we can create a tsibble from the csv file by using the as_tsibble() function.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) -> calits\n\nThe model() function can run the models discussed in section 5.1. We save this to an object called fit.\n\nfit <- model(calits,mean=MEAN(average_price),\n              Naive=NAIVE(average_price),\n              Drift=RW(average_price~drift()),\n              LS=TSLM(average_price~date))\n\nTo explore the model coefficients, we can use the coef() function. We have used the gt package to make the table visually appealing.\n\n\n\n\n\n\n  \n    \n      Model Coefficients For The Avocado Data\n    \n    \n  \n  \n    \n      geography\n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    California\nmean\nmean\n1.70\n0.02\n89.85\n0.00\n    California\nDrift\nb\n0.00\n0.01\n0.50\n0.62\n    California\nLS\n(Intercept)\n−2.94\n0.72\n−4.11\n0.00\n    California\nLS\ndate\n0.00\n0.00\n6.49\n0.00\n  \n  \n  \n\n\n\n\n\ncalits %>% autoplot(average_price) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %>% filter(`.model`==\"LS\"))"
  },
  {
    "objectID": "Tools2.html#tsibbles",
    "href": "Tools2.html#tsibbles",
    "title": "4  Time Series Tools",
    "section": "4.6 tsibbles",
    "text": "4.6 tsibbles\nWhen dealing with time series, time plays an important role. As a consequence, we will be using a data structure called a tsibble (time series tibble). tsibbles are defined by a time index (i.e., the date) that has a common interval (i.e., days, weeks, etc.), and some keys (i.e., some dimensions or observational units). In the avocado data set we are mainly interested in the average price of the avocados. You will note however that they are classified by location (geography) and type (organic and conventional). You can learn more about tsibbles here.\ntsibbles, as well a variety of packages that help us analyze time series, are part of the fpp3 package. Below we load the package, and coerce our avocado tibble to a tsibble.\n\nlibrary(fpp3)\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-> avocadots\n\nIn the code above, the as_tsibble() function was called with the parameter regular set at true indicating that the date has no gaps and occurs every week (the greatest common divisor of the index column). The function filter_index() is called as it allows us to determine the window for analysis. As noted in Section 4.2, there are some missing dates in December 2018. We limit the analysis to 2015-2018.\nRecall, that we are interested in the average price of avocados for California. We can specify the tsibble for analysis, by using dplyr.\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price) -> calits"
  },
  {
    "objectID": "Tools2.html#time-series-decomposition",
    "href": "Tools2.html#time-series-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.7 Time Series Decomposition",
    "text": "4.7 Time Series Decomposition\nAs highlighted in Section 4.5, the average price of avocados in California seems to have a trend and a seasonal pattern. There are methods available to tease out these components and make them more apparent. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run this method in R by using the fable package.\n\ncalits %>%\n  model(STL(average_price~trend(window=200)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()\n\n\n\n\nAs shown above, the trend is increasing and the seasonal component confirms low levels at the beginning of the year and high levels in the summer.\nThe decomposition is constructed in several steps. First, a moving average of the series is calculated to track the trend. As you can see, the window argument in the trend() function is set to a relatively large value. By doing this the moving average reflects the general direction of the series and not the small fluctuations. The trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly) for the de-trended series. The error is the remaining fluctuation of the series that is not explained by the trend or the seasonal component (Series-Trend-Seasonal=Error). In the end, each component can be graphed and displayed, as illustrated above."
  },
  {
    "objectID": "ETSARIMA.html#preliminaries",
    "href": "ETSARIMA.html#preliminaries",
    "title": "6  ETS and ARIMA",
    "section": "6.1 Preliminaries",
    "text": "6.1 Preliminaries\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and autocovariance, regardless of the time at which the series is observed.\nThe main reason for making the time series stationary is that it is required in many time series models (including the ARIMA model). These models make predictions about future values of the time series based on past values, and the statistical properties of the past values are used to inform these predictions. If the statistical properties of the time series are changing over time, then the models may not work well, as the assumptions underlying them would not be met.\nAdditionally, non-stationary time series may contain trends and/or seasonal patterns, which can make it difficult to model and forecast the series. By removing these patterns and making the series stationary, it becomes easier to model and forecast.\nIn general, before modeling and forecasting, we will check whether the series has a trend or homoskedasticity. To eliminate the trend in the series we will us the first difference of the series. We can do this in R by using the diff() function. For example consider Tesla’s quarterly vehicle deliveries from 2016-2022.\n\n\n\n\n\nDeliveries have mostly been in an upward trend, which makes sense as the company is currently growing. This series seems to not be stationary since it crosses the mean once and never revisits it. An easy way to make the series stationary (make it fluctuate around the mean) is to find the first difference. Below is the graph of the integrated series.\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% autoplot(difference(deliveries)) + theme_classic()\n\n\n\n\nNote how the series now fluctuates around the mean of zero. That is the change of the vehicles from quarter to quarter sometimes increases and sometimes decreases and on average their is no change. You will note that the series does exhibit some heteroskedasticity as the variance of the series seems to be low from the period of 2016-2018 and significanlty increases after. To normalize the variance of the series we can make a Box-Cox transormation.\n\nlambda <- deliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = guerrero) %>% pull(lambda_guerrero) \n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic()\n\n\n\n\nAs you can see this series is a bit more homoskedastic than the series without the transformation.\nTo test whether a series is stationary or not we can use the unitroot_kpss feature. In general, a low p-value allows us to reject the null of hypothesis of stationarity.\n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = c(unitroot_kpss)) %>% gt()\n\n\n\n\n\n  \n  \n    \n      kpss_stat\n      kpss_pvalue\n    \n  \n  \n    0.9465391\n0.01\n  \n  \n  \n\n\n\n\nThis confirms that Tesla deliveries are non-stationary.\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\nWe can use this metric to identify which periods are influential for our targeted forecast periods. As a consequence, we can illustrate a function of a series and it’s correlation with its lags to identify/quantify crucial periods. To construct an autocorrelation function (ACF) start by loading the data and coercing the period variable to a date.\n\nlibrary(fpp3)\nlibrary(tidyverse)\ndeliveries<-read_csv(\"https://jagelves.github.io/Data/tsla_deliveries.csv\")\n\ndeliveries$period<-yearquarter(deliveries$period)\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% ACF(lag_max = 12, deliveries) %>% autoplot()+theme_classic()\n\n\n\n\nThe plot shows that the the correlation of the series with its first lag strongest, and that there is continuous decay as the lags get larger.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% PACF(lag_max = 12, deliveries) %>% autoplot()+theme_classic()\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag when we net the effect. In other words, if lag 2 or 3 seemed to have been correlated with the series (see ACF), it was mainly because the influence of lag 1 on the series."
  },
  {
    "objectID": "ETSARIMA.html#the-ar-model",
    "href": "ETSARIMA.html#the-ar-model",
    "title": "6  ETS and ARIMA",
    "section": "6.2 The ar model",
    "text": "6.2 The ar model"
  },
  {
    "objectID": "ETSARIMA.html#the-ma-model",
    "href": "ETSARIMA.html#the-ma-model",
    "title": "6  ETS and ARIMA",
    "section": "6.3 The ma model",
    "text": "6.3 The ma model"
  },
  {
    "objectID": "ETSARIMA.html#the-arima-model",
    "href": "ETSARIMA.html#the-arima-model",
    "title": "6  ETS and ARIMA",
    "section": "6.4 The arima model",
    "text": "6.4 The arima model"
  },
  {
    "objectID": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "href": "Benchmarks.html#modeling-the-the-average-price-of-avocados",
    "title": "5  Model Benchmarks",
    "section": "5.2 Modeling the the Average Price of Avocados",
    "text": "5.2 Modeling the the Average Price of Avocados\nLet’s apply these four models to forecast the average price of avocados in California. We’ll start by loading the tidyverse and fpp3 packages and importing the data.\n\nlibrary(tidyverse)\nlibrary(fpp3)\ncali<-read_csv(\"https://jagelves.github.io/Data/CaliforniaAvocado.csv\")\n\nRecall, that we can create a tsibble from the csv file by using the as_tsibble() function. The index argument is set to the weekly date variable. The filter_index() function is used so that we can focus our analysis for the period of 2015-01-04~2018-06-02.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-06-02\")-> calits_train\n\nNow we can use the model() function to run the benchmarks discussed in Section 5.1. We have saved the models to an object called fit.\n\nfit <- model(calits_train,mean=MEAN(average_price),\n              Naive=NAIVE(average_price),\n              Drift=RW(average_price~drift()),\n              LS=TSLM(average_price~date))\n\nThe fit object is saved as a mable (model table). To explore the coefficients of the models estimated, we use the coef() function with fit as its single argument. The output table has been enhanced visually by using the gt package.\n\nlibrary(gt)\ncoef(fit) %>% gt() %>% \n  cols_align(\"center\") %>% \n  tab_header(title = \n               md(\"**Model Coefficients For The Avocado Data**\")) %>% tab_style(locations =                                                    cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(statistic,estimate,std.error,p.value),\n             decimals = 2)\n\n\n\n\n\n  \n    \n      Model Coefficients For The Avocado Data\n    \n    \n    \n      geography\n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    California\nmean\nmean\n1.68\n0.02\n79.40\n0.00\n    California\nDrift\nb\n0.00\n0.01\n0.20\n0.84\n    California\nLS\n(Intercept)\n−3.73\n0.92\n−4.04\n0.00\n    California\nLS\ndate\n0.00\n0.00\n5.87\n0.00\n  \n  \n  \n\n\n\n\nThe table records the estimates for all the benchmarks discussed in Section 5.1. The naive method has no estimate, as it is simply the previous period’s observed value. Below we illustrate the fit of the least squares model by the red line and the Naive model by the orange line.\n\ncalits_train %>% autoplot(average_price) + theme_classic() + \n  geom_line(aes(y = .fitted), col=\"red\",\n            data = augment(fit) %>% filter(`.model`==\"LS\")) +\ngeom_line(aes(y = .fitted), col=\"orange\",\n            data = augment(fit) %>% filter(`.model`==\"Naive\")) +\n  labs(y=\"\", title= \"California's Average Price Of Avocados\",\n       subtitle = \"2015-2018\",\n       x=\"\")\n\n\n\n\nThe graph illustrates how closely the Naive method follows the data. This might seem like a good model, but consider how every period the heuristic is making a mistake since average prices are constantly changing every week. Consider as well how the Naive prediction provides no explanation of the series governing process. The LS model, on the other hand, does provide some insight to a force that is influencing the data. Mainly, a rising trend. We can use characteristics such as a trend or seasonality to forecast a series."
  },
  {
    "objectID": "Benchmarks.html#model-fit",
    "href": "Benchmarks.html#model-fit",
    "title": "5  Model Benchmarks",
    "section": "5.3 Model Fit",
    "text": "5.3 Model Fit\nThe model fit will be assessed by comparing the fitted values against actual values. In general, a good fit is determined by how far the fitted values are from the observed ones. If we square all of the distances between actual points and predicted values (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE).\n\n\\(MSE = \\frac{ \\sum (\\hat{y}_t-y_t)^2}{T}\\)\n\nHow we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE’s but we then find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since all the accuracy metrics are the smallest. We highlighted these results and once again made the table more appealing by using the gt library.\n\naccuracy(fit)\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n    \n      geography\n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    California\nmean\nTraining\n0.00\n0.28\n0.23\n−2.71\n13.59\n2.26\n2.05\n0.87\n    California\nNaive\nTraining\n0.00\n0.14\n0.10\n−0.19\n6.04\n1.00\n1.00\n−0.21\n    California\nDrift\nTraining\n0.00\n0.14\n0.10\n−0.31\n6.05\n1.00\n1.00\n−0.21\n    California\nLS\nTraining\n0.00\n0.26\n0.21\n−2.21\n12.23\n2.07\n1.87\n0.85"
  },
  {
    "objectID": "Benchmarks.html#forecast",
    "href": "Benchmarks.html#forecast",
    "title": "5  Model Benchmarks",
    "section": "5.4 Forecast",
    "text": "5.4 Forecast\nThe forecast of the series is obtained by using the forecast() function and specifying the number of periods (\\(h\\)) ahead we need to forecast. We can easily forecast 30 weeks of data in R with the code below.\n\ncalits_fc<- fit %>% forecast(h=30)\n\nThe autoplot() and autolayer() functions are used below to create a graph with the forecasts and the training set.\n\ncalits_fc %>% autoplot(level=NULL) + theme_classic() + \n  autolayer(calits_train, average_price)\n\n\n\n\nNote how the Mean and Naive models forecast that the series will continue without a trend. The LS model predicts that the series will continue its trend but as all of the other methods, does not take into account the seasonal pattern discussed in Section 4.5. In future chapters, we will be looking at models that account for trend and seasonality."
  },
  {
    "objectID": "ETSARIMA.html#the-arp-model",
    "href": "ETSARIMA.html#the-arp-model",
    "title": "6  ETS and ARIMA",
    "section": "6.2 The AR(p) model",
    "text": "6.2 The AR(p) model\nIn the previous section we identified the deliveries of Tesla cars to have a decaying ACF and a single significant spike (at lag 1) in the PACF. This pattern can be generated by an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model just uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R:\n\ny<-c(0)\nphi<-0.7\nconst<-1\nnrep<-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% ACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the deliveries of Tesla. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% PACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nOnce again, if you compare the PACF to the one in the deliveries of Tesla you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero."
  },
  {
    "objectID": "ETSARIMA.html#the-maq-model",
    "href": "ETSARIMA.html#the-maq-model",
    "title": "6  ETS and ARIMA",
    "section": "6.3 The MA(q) model",
    "text": "6.3 The MA(q) model"
  },
  {
    "objectID": "ETSARIMA.html#the-arimapiq-model",
    "href": "ETSARIMA.html#the-arimapiq-model",
    "title": "6  ETS and ARIMA",
    "section": "6.4 The ARIMA(p,i,q) model",
    "text": "6.4 The ARIMA(p,i,q) model"
  },
  {
    "objectID": "Benchmarks.html#overfitting",
    "href": "Benchmarks.html#overfitting",
    "title": "5  Model Benchmarks",
    "section": "5.5 Overfitting",
    "text": "5.5 Overfitting\nOverfitting can happen when a model is overly flexible and complex. This can lead to the model fitting to the random fluctuations or noise in the data, rather than the underlying pattern.\nTo overcome this problem, we usually have a training set or subset of the data that we use to estimate the model’s parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the test set.\nRecall that our avocado price models were estimated for the period between 2015-01-04~2018-06-02. We will call this our training set. For our test set we’ll use the 2018-06-02~2018-12-02 period. The code below creates the test set.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) %>%\n  filter_index(\"2018-06-02\"~\"2018-12-02\")-> calits_test\n\nNow we can plot the training set, the forecast, and the test set with by using the code below.\n\ncalits_fc %>% autoplot(level=NULL) + theme_classic() + \n  autolayer(calits_train, average_price) + autolayer(calits_test, average_price)\n\n\n\n\nThe graph shows how the LS method does well with the test data. This can be confirmed by obtaining the accuracy measures against the test set.\n\n\n\n\naccuracy(calits_fc, calits) %>% gt() %>%\n  cols_align(\"center\") %>% \n  tab_header(title = md(\"**Model Fit**\")) %>% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %>% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"LS\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      .model\n      geography\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nCalifornia\nTest\n0.17\n0.21\n0.17\n9.24\n9.24\n1.74\n1.55\n0.52\n    LS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n1.37\n1.14\n0.52\n    mean\nCalifornia\nTest\n0.13\n0.18\n0.13\n6.72\n6.85\n1.31\n1.31\n0.50\n    Naive\nCalifornia\nTest\n0.20\n0.24\n0.20\n10.85\n10.85\n2.03\n1.74\n0.50\n  \n  \n  \n\n\n\n\nNote also that the Naive method although good, is no longer the best model. This is partly because it chases the series fluctuations closely and always makes a mistake when forecasting one period ahead."
  },
  {
    "objectID": "ARIMA.html#preliminaries",
    "href": "ARIMA.html#preliminaries",
    "title": "6  ARIMA",
    "section": "6.1 Preliminaries",
    "text": "6.1 Preliminaries\n\nWhite Noise\nIn time series, white noise refers to a sequence of random data points that have no correlation to each other. This process is used as a benchmark for other types of time series data that exhibit patterns or trends. By comparing a series with the white noise process, we can verify if the series has systematic components that can be modeled.\nWe can generate a white noise process by using the normal distribution with a mean of zero and a constant variance. Below we create a tsibble with the simulated data.\n\nlibrary(fpp3)\nset.seed(10)\nwn<-tsibble(x=rnorm(100),period=seq(1:100),index=period)\n\nWe can now use the autoplot() function to observe the white noise process.\n\nwn %>% autoplot(x) + theme_classic() + \n  labs(title=\"White Noise Process\",\n       subtitle=\"Mean=0 and Standard Deviation=1\",\nx=\"\",y=\"\") + \n  geom_hline(yintercept = 0, col=\"blue\", lwd=1, linetype=\"dashed\",\n             alpha=0.4)\n\n\n\n\nA very ragged pattern is shown in the graph above. The series behaves erratically, but it always fluctuates around a mean of 0 and keeps a standard deviation of one. Such a series is unpredictable, so the best one can do is to describe it by its mean and standard deviation.\n\n\nStationarity\nA time series is said to be stationary if its statistical properties do not change over time. In other words, a stationary time series has a constant mean, variance, and auto-covariance, regardless of the time at which the series is observed.\nThe main reason for making the time series stationary is that it is required in many time series models (including the ARIMA model). These models make predictions about future values of the time series based on past values, and the statistical properties of the past values are used to inform these predictions. If the statistical properties of the time series are changing over time, then the models may not work well, as the assumptions underlying them would not be met.\nIn general, before modeling and forecasting, we will check whether the series is stationary (i.e., has no trend or homoskedasticity). To eliminate the trend in the series we will use the first difference of the series. We can do this in R by using the difference() function. For example consider Tesla’s quarterly vehicle deliveries from 2016-2022.\n\n\n\n\n\nDeliveries have mostly been in an upward trend, which makes sense as the company is currently scaling its production. This series seems to not be stationary since it crosses the mean (blue line) once and never revisits it. That is the mean is not constant and changes with time. It is possible to make the series stationary by finding differences. Below is the graph of the first difference of the series.\n\ndeliveries %>%\n  as_tsibble(index=period, regular=T) %>% autoplot(difference(deliveries)) + theme_classic() +\n  labs(title=\"Change In Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\") + \n  geom_hline(yintercept = mean(difference(deliveries$deliveries), na.rm = TRUE), col=\"blue\", linetype=\"dashed\", lwd=1, alpha=0.4)\n\n\n\n\nThe series now fluctuates closer to the mean, but unlike the white noise process behaves less erratic. You will notice that in some periods the change in deliveries from quarter to quarter is high. For example, following the lows at the beginning of the year, deliveries seem to increase sharply. There seems to be correlations between the quarters or time dependencies. We will explore this more once we look at autocorrelations, partial autocorrelations, and seasonality.\nThe series exhibits heteroskedasticity (increasing variance), as the variance of the series seems to be low from the period of 2016-2018 while significantly higher for the period after. To normalize the variance of the series we can conduct a Box-Cox transformation.\n\nlambda <- deliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = guerrero) %>% pull(lambda_guerrero) \n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  autoplot(box_cox(difference(deliveries), lambda)) +\n  labs(y = \"\")+ theme_classic() +\n  labs(title=\"Box-Cox Transformation of Tesla's Vehicle Deliveries\",\n       subtitle = \"Q1 2016 - Q4 2022\",x=\"\",y=\"\")\n\n\n\n\nThe transformation has made the series a bit more homoskedastic (variance is more uniform) than the series without the transformation. Note also that both transformations can be undone by using an inverse function, so that we can return to the delivery of vehicles.\nA couple of statistical features to determine the stationarity of a series are the unitroot_kpss and unitroot_ndiffs. In general, a low p-value allows us to reject the null of hypothesis of stationarity.\n\n\n\n\ndeliveries %>% as_tsibble(index=period, regular=T) %>%\n  features(deliveries, features = c(unitroot_kpss, unitroot_ndiffs)) %>% gt() %>%\n  cols_align(\"center\") %>% \n  tab_header(title = \n               md(\"**Stationarity Tests**\")) %>% tab_style(locations =                                                    cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(kpss_stat,kpss_pvalue),\n             decimals = 3)\n\n\n\n\n\n  \n    \n      Stationarity Tests\n    \n    \n    \n      kpss_stat\n      kpss_pvalue\n      ndiffs\n    \n  \n  \n    0.947\n0.010\n2\n  \n  \n  \n\n\n\n\nThe test reports a p-value of 0.01 when it is below 0.01 and 0.1 when it is above 0.1. Hence, the p-value confirms that Tesla deliveries are non-stationary and that two differences are required to make the data stationary.\n\n\nThe autocorrelation function\nAutocorrelations are essential in time series analysis since they indicate the degree of similarity between a time series and a lagged version of itself (a previous period). They help identify patterns and trends in the data allowing us to predict future values of the series. For example, suppose a time series exhibits a strong positive autocorrelation at a lag of \\(k\\) periods. In such a case, the value at time \\(t+k\\) will likely be similar to that at time \\(t\\). Formally we can write the autocorrelation as:\n\n\\(\\rho_{y_t,y_{t-k}}=\\frac{cov(y_t,y_{y-k})}{sd(y_t)sd(y_{t-k})}\\)\n\nWe can use this metric to identify which periods are influential for our targeted forecast periods. We can also illustrate a function of a series and it’s correlation with its lags to identify/quantify crucial periods. This time let’s inspect personal income growth in the state of California. Below we load the data and create the train and test sets.\n\nlibrary(fpp3)\nlibrary(tidyverse)\nPI<-read_csv(\"https://jagelves.github.io/Data/PersonalIncome.csv\")\nPI %>% as_tsibble(index=Date) %>% \n  filter_index(1970~2005) -> PI_train\nPI %>% as_tsibble(index=Date) %>% \n  filter_index(2006~2021) -> PI_test\n\nThe autocorrelation function can now be constructed by using the ACF() function and plotting it with autoplot() as shown below.\n\nPI_train %>%\n  ACF(lag_max = 12,PI_Growth) %>% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"\",\n                                 title=\"ACF Personal Income Growth in California\")\n\n\n\n\nThe plot shows that the correlation of the series with its first lag is strongest, and that there is continuous decay in the strength of the correlation as the lags get larger. The blue lines determine which autocorrelations are statistically different from zero (significant) at the 5% level. As you can see, lags 1-4 are positively correlated with the series and statistically significant.\nA white noise process on the other hand is expected to show no correlation with its lags since the series is constructed from independent draws from a normal distribution with constant variance. Below you can see the autocorrelation function of the white noise process.\n\nwn %>% ACF(x) %>% autoplot() + theme_bw() + labs(x=\"\", y=\"ACF\") +\nlabs(x=\"\", y=\"\", title=\"ACF White Noise Process\")\n\n\n\n\nInterestingly, lag 14 shows a positive correlation with the series. It is important to note that correlations can happen by chance even if we construct the series from a random process.\n\n\nThe partial autocorrelation function\nAs with the ACF, the partial autocorrelation function (PACF) summarizes the relationships between a series and its lags. However, the relationships of intervening lags are removed. The sample partial autocorrelation at lag \\(k\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nFormally speaking, when we calculate the autocorrelation between \\(y_t\\) and \\(y_{t+k}\\), information flows from \\(t\\) to \\(t+k\\), so that indirectly \\(\\rho_k\\) (the correlation) accounts for the contribution of lags between \\(t\\) and \\(t+k\\). A series of regressions would allow us to calculate the PACF. Luckily, R calculates these easily for us as shown below:\n\nPI_train %>%\n  PACF(lag_max = 12,PI_Growth) %>% \n  autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF Personal Income Growth In California\")\n\n\n\n\nThe graph shows that the series has a strong correlation only with its first lag. Specifically, lag 2, 3, and 4 seemed to have been correlated with the series (see ACF), but this was mainly because of the influence of lag 1.\nLet’s inspect the white noise process once more to confirm that there are no patterns.\n\nwn %>% PACF(x) %>% autoplot() + theme_bw() + labs(x=\"\", y=\"PACF\") +\nlabs(x=\"\", y=\"\", title=\"PACF White Noise Process\")\n\n\n\n\nIn sum, white noise processes are unpredictable and we can only describe them by their mean and standard deviation. Series that have patterns in their ACF or PACF can be modeled. Below we I illustrate how to model Personal Income in California with an AR(1) model."
  },
  {
    "objectID": "ARIMA.html#the-arp-model",
    "href": "ARIMA.html#the-arp-model",
    "title": "6  ARIMA",
    "section": "6.2 The AR(p) model",
    "text": "6.2 The AR(p) model\nIn the previous section, we identified the growth of personal income in California to have a decaying ACF and a single significant spike (at lag 1) in the PACF. These results can be generated with an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model just uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R.\n\ny<-c(0)\nphi<-0.7\nconst<-1\nnrep<-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like for a AR(1) process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% ACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the deliveries of Tesla. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% PACF(lag_max = 12, y) %>% autoplot()+theme_classic()\n\n\n\n\nOnce again, if you compare the PACF to the one in personal income growth you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero. We can model personal consumption growth with an AR(1) process."
  },
  {
    "objectID": "ARIMA.html#the-maq-model",
    "href": "ARIMA.html#the-maq-model",
    "title": "6  ARIMA",
    "section": "6.5 The MA(q) model",
    "text": "6.5 The MA(q) model"
  },
  {
    "objectID": "ARIMA.html#the-arimapiq-model",
    "href": "ARIMA.html#the-arimapiq-model",
    "title": "6  ARIMA",
    "section": "6.6 The ARIMA(p,i,q) model",
    "text": "6.6 The ARIMA(p,i,q) model"
  },
  {
    "objectID": "ARIMA.html#forecast-and-residuals",
    "href": "ARIMA.html#forecast-and-residuals",
    "title": "6  ARIMA",
    "section": "6.3 Forecast and Residuals",
    "text": "6.3 Forecast and Residuals\nLet’s forecast the deliveries of Tesla using the AR(1) model. We can do this by first creating a train set and using the model() function.\n\ntesla_fit<-deliveries_train %>% \n  model(AR1 = AR(deliveries ~ order(1)),\n        LS=TSLM(deliveries~period),\n        Drift=RW(deliveries~drift()))\n\n\n\n\n\n\n\n  \n    \n      Model Coefficients For Tesla Deliveries\n    \n    \n  \n  \n    \n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    AR1\nar1\n1.15\n0.04\n29.53\n0.00\n    LS\n(Intercept)\n−1,913.59\n196.57\n−9.73\n0.00\n    LS\nperiod\n0.11\n0.01\n10.21\n0.00\n    Drift\nb\n12.77\n4.76\n2.68\n0.01\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n  \n  \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    AR1\nTraining\n−0.12\n19.86\n14.64\n−4.05\n18.53\n0.34\n0.34\n−0.04\n    LS\nTraining\n0.00\n32.62\n24.91\n11.26\n53.66\n0.57\n0.56\n0.55\n    Drift\nTraining\n0.00\n22.35\n17.52\n−17.42\n28.78\n0.40\n0.38\n0.17\n  \n  \n  \n\n\n\n\n\ntesla_fit %>% forecast(new_data=deliveries_test)\n\nNew names:\nNew names:\nNew names:\nNew names:\n• `...1` -> `...4`\n\n\n# A fable: 12 x 5 [1Q]\n# Key:     .model [3]\n   .model  period   deliveries .mean  ...5\n   <chr>    <qtr>       <dist> <dbl> <dbl>\n 1 AR1    2022 Q1  N(356, 394)  356.    25\n 2 AR1    2022 Q2  N(411, 920)  411.    26\n 3 AR1    2022 Q3 N(475, 1621)  475.    27\n 4 AR1    2022 Q4 N(549, 2555)  549.    28\n 5 LS     2022 Q1 N(221, 1368)  221.    25\n 6 LS     2022 Q2 N(231, 1393)  231.    26\n 7 LS     2022 Q3 N(241, 1422)  241.    27\n 8 LS     2022 Q4 N(252, 1452)  252.    28\n 9 Drift  2022 Q1  N(321, 545)  321.    25\n10 Drift  2022 Q2 N(334, 1135)  334.    26\n11 Drift  2022 Q3 N(347, 1770)  347.    27\n12 Drift  2022 Q4 N(360, 2451)  360.    28"
  },
  {
    "objectID": "ARIMA.html#modeling-and-residuals",
    "href": "ARIMA.html#modeling-and-residuals",
    "title": "6  ARIMA",
    "section": "6.3 Modeling and Residuals",
    "text": "6.3 Modeling and Residuals\nLet’s model personal consumption using the AR(1) model. We’ll also estimate a Least Squares model to compare. Recall, that we can estimate these models by using the model() function and retrieve the coefficients with the coef() function.\n\nPI_fit<-PI_train %>% \n  model(AR1 = AR(PI_Growth ~ order(1)),\n        LS = TSLM(PI_Growth ~ trend()))\n\n\n\n\n\n\n\n  \n    \n      Model Coefficients For PI Growth\n    \n    \n    \n      .model\n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    AR1\nconstant\n1.80\n0.82\n2.19\n0.04\n    AR1\nar1\n0.70\n0.12\n5.80\n0.00\n    LS\n(Intercept)\n9.19\n0.80\n11.50\n0.00\n    LS\ntrend()\n−0.17\n0.04\n−4.43\n0.00\n  \n  \n  \n\n\n\n\nNote how the estimated coefficient for the AR1 process resembles the one in our simulation. However, the estimation of this model suggest a constant of 1.8. If we changed the constant to 1.8, the simulation would resemble the PI growth data better.\nIf the AR(1) process correctly describes the series, the errors should behave like white noise. To inspect the errors we can use the augment() function. The ACF is displayed below.\n\nerrors_PI<-augment(PI_fit)\n\nerrors_PI %>% select(.resid) %>% ACF(.resid) %>% \n  autoplot() + theme_bw()\n\n\n\n\nNote how the errors of the AR(1) model resemble white noise. This suggests that we have identified the systematic component of the series as being an AR(1) process. In other words, there is nothing left to model since the errors are completely random. This is not the case for the LS model, since we still observe some significant spikes (lag 1 and lag 13) in the ACF function.\n\nerrors_PI %>% select(.resid) %>% PACF(.resid) %>% \n  autoplot() + theme_bw()\n\n\n\n\nThe PACF once again shows no pattern for the residuals of the AR(1) model and some significant lags for the LS model."
  },
  {
    "objectID": "Benchmarks.html#cross-validation",
    "href": "Benchmarks.html#cross-validation",
    "title": "5  Model Benchmarks",
    "section": "5.6 Cross Validation",
    "text": "5.6 Cross Validation\nInstead of selecting a single training set and test set, we can create several. Specifically, we could take the first three observations and define them as the training set. We can then estimate a model and forecast the fourth observation. The forecast error is recorded and the training set is changed so that now the first four observations are used to estimate the model and forecast the fifth observation. This procedure is repeated as many times as the data allows. Below we create a table that enables us to follow the cross-validation of our benchmarks.\n\navocado_cv <- calits_train %>% \n  select(-geography, -total_volume) %>% \n  stretch_tsibble(.init = 3, .step = 1)\n\nThe stretch_tsibble() is a handy function that creates a variable called id that is initialized with the .init argument. In this case, the first three observations are given \\(id=1\\). The id then changes with a step of \\(.step=1\\). That is, \\(id=2\\) for the first four observations, then \\(id=3\\) for the first five observations, and so on. Below is a sample of the tsibble.\n\n\n\n\n\n\n  \n    \n      CV tsibble\n    \n    \n    \n      date\n      average_price\n      .id\n    \n  \n  \n    2015-01-04\n1.24\n1\n    2015-01-11\n1.10\n1\n    2015-01-18\n1.24\n1\n    2015-01-04\n1.24\n2\n    2015-01-11\n1.10\n2\n    2015-01-18\n1.24\n2\n    2015-01-25\n1.30\n2\n    2015-01-04\n1.24\n3\n  \n  \n  \n\n\n\n\nUsing this new tsibble, the benchmarks are estimated for each id and forecasts are generated for one period ahead (\\(h=1\\)). The accuracy is measured and averaged across all iterations for each model. Results are shown in the table below.\n\navocado_cv %>%\n  model(Mean=MEAN(average_price),\n        Naive=RW(average_price),\n        Drift=RW(average_price ~ drift()),\n        LS=TSLM(average_price~date)) %>%\n  forecast(h = 1) %>% accuracy(calits) %>% gt() %>%\n  cols_align(\"center\") %>% \n  tab_header(title = md(\"**Model Fit Cross Validation**\")) %>% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %>% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"Naive\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit Cross Validation\n    \n    \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nTest\n0.00\n0.14\n0.10\n−0.58\n6.09\n1.01\n1.01\n−0.21\n    LS\nTest\n−0.05\n0.27\n0.21\n−4.64\n12.35\n2.05\n1.94\n0.85\n    Mean\nTest\n0.12\n0.29\n0.21\n5.36\n11.73\n2.12\n2.07\n0.85\n    Naive\nTest\n0.00\n0.14\n0.10\n−0.15\n5.96\n0.99\n1.00\n−0.21\n  \n  \n  \n\n\n\n\nThe Naive method once again performs well. However, we note once again that the Naive method will provide the same forecast for one, two, three or more periods ahead. Additionally, there is no formal model telling us how data is generated or that explains the how the time series is generated."
  },
  {
    "objectID": "Benchmarks.html#sec-Bench",
    "href": "Benchmarks.html#sec-Bench",
    "title": "5  Model Benchmarks",
    "section": "5.1 Benchmarks",
    "text": "5.1 Benchmarks\nOne of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A naive prediction sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T\\)\n\n\nwhere \\(\\hat y_{T+h}\\) is the predicted value for \\(h\\) periods ahead, and \\(y_T\\) is the value observed at the current time period \\(T\\). We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would “drift” the naive prediction upward. Mathematically we would write:\n\n\\(\\hat y_{T+h}=y_T+h(\\frac{y_t-y_1}{T-1})\\)\n\n\nwhere \\(h(\\frac{y_t-y_1}{T-1})\\) can be thought as the average increase of \\(y\\) from period \\(1\\) to the current period \\(T\\). One could also predict weight by observing weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:\n\n\\(\\hat y_{T+h}=\\frac{(y_1+y_2+...+y_T)}{T}\\)\n\n\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:\n\n\\(\\hat y_{T+h}=b_0+b_1(T+h)\\)"
  },
  {
    "objectID": "Benchmarks.html#over-fitting",
    "href": "Benchmarks.html#over-fitting",
    "title": "5  Model Benchmarks",
    "section": "5.5 Over-Fitting",
    "text": "5.5 Over-Fitting\nOver-fitting can happen when a model is overly flexible. This can make the model fit to the random fluctuations or noise in the data, rather than the underlying pattern. This is a major failing in modeling as it ignores the systematic pattern that governs the time series.\nTo overcome this problem, we usually have a training set or subset of the data that we use to estimate the model’s parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the test set. A model that “memorizes” the training data, will often perform poorly when forecasting the test set.\nRecall that benchmarks were estimated for the period between 2015-01-04~2018-06-02. We will call this our training set. For our test set, we’ll use the 2018-06-02~2018-12-02 period. The code below creates the test set using the filter_index() function.\n\ncali %>%\n  as_tsibble(key=c(geography),\n             index=date, regular=T) %>%\n  filter_index(\"2018-06-02\"~\"2018-12-02\")-> calits_test\n\nNow we can plot the training set, the forecast, and the test set by using the code below.\n\ncalits_fc %>% autoplot(level=NULL) + \n  theme_classic() + \n  autolayer(calits_train, average_price) + \n  autolayer(calits_test, average_price)\n\n\n\n\nThe graph shows how the LS method does well with the test data and a long forecast period. This can be confirmed by obtaining the accuracy measures against the test set.\n\n\n\n\naccuracy(calits_fc, calits) %>% gt() %>%\n  cols_align(\"center\") %>% \n  tab_header(title = md(\"**Model Fit**\")) %>% \n  tab_style(locations = cells_column_labels(columns = everything()),\n  style = list(cell_borders(sides = \"bottom\", weight = px(3)),\n    cell_text(weight = \"bold\"))) %>% \n  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),\n             decimals = 2) %>% \n  tab_style_body(\n    style = cell_fill(color=\"lightgreen\"),\n    values = \"LS\",\n    targets =\"row\")\n\n\n\n\n\n  \n    \n      Model Fit\n    \n    \n    \n      .model\n      geography\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    Drift\nCalifornia\nTest\n0.17\n0.21\n0.17\n9.24\n9.24\n1.74\n1.55\n0.52\n    LS\nCalifornia\nTest\n−0.10\n0.16\n0.14\n−5.88\n7.78\n1.37\n1.14\n0.52\n    Naive\nCalifornia\nTest\n0.20\n0.24\n0.20\n10.85\n10.85\n2.03\n1.74\n0.50\n    mean\nCalifornia\nTest\n0.13\n0.18\n0.13\n6.72\n6.85\n1.31\n1.31\n0.50\n  \n  \n  \n\n\n\n\nInterestingly, the Naive method is no longer the best model. This is partly because it chases the series fluctuations (along with the noise) and always makes the same forecast regardless of how many periods we forecast ahead."
  },
  {
    "objectID": "Tools2.html#sec-Avocado",
    "href": "Tools2.html#sec-Avocado",
    "title": "4  Time Series Tools",
    "section": "4.2 The Avocado Data Set",
    "text": "4.2 The Avocado Data Set\nThe avocado data set is weekly retail scan data for U.S retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. Other avocados (e.g. greenskins) are not included in this data.\nNote: When inspecting the data you will notice that each entry is recorded weekly. However, there is an entry on 01/01/2018, that is right after 12/31/2017. This is a single observation that is not weekly. You will also note that there are missing dates from 12/02/2018-12/31/2018."
  },
  {
    "objectID": "Tools2.html#sec-Visual",
    "href": "Tools2.html#sec-Visual",
    "title": "4  Time Series Tools",
    "section": "4.5 Visualizing The Data",
    "text": "4.5 Visualizing The Data\nTo visualize the data, we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines the mapping of variables. The geom_line() function specifies the type of plot. In time series, we will use the line plot regularly. Labels for the graph are easily set with the labs function and there are plenty of themes available to customize your visualization. Below, the theme_classic() is displayed. To learn more about the ggplot package, you can refer to Wickham (2017) Chapter 2. Below is the code to create a line plot of California’s average avocado price.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price, group=geography),color=\"black\") +\n  theme_classic() + \n  labs(x=\"\",\n       y=\"Average Price\", \n       title=\"Organic Avocado Price in California\",\n       subtitle=\"2015-2018\")  \n\n\n\n\nThe average price of avocados in California has been increasing during the period considered. It reached a maximum of about 2.6 in 2017 and was at a minimum in the spring of 2015. There is also a seasonal pattern with low prices at the beginning of the year and peaks mid-year. As we will see in upcoming chapters, these are patterns that can be extrapolated and used to forecast time series."
  },
  {
    "objectID": "ARIMA.html#the-ar1-model",
    "href": "ARIMA.html#the-ar1-model",
    "title": "6  ARIMA",
    "section": "6.2 The AR(1) model",
    "text": "6.2 The AR(1) model\nIn the previous section, we identified the growth of personal income in California to have a decaying ACF and a single significant spike (at lag 1) in the PACF. These patterns can be generated with an AR(1) model. Specifically, an AR(1) model is of the form:\n\n\\(y_t=c+\\phi y_{t-1}+ \\epsilon_t\\)\n\nNote that this model uses the first lag of the series as the single independent variable. We can easily simulate some data based on this model using R.\n\ny<-c(0)\nphi<-0.7\nconst<-1\nnrep<-100\n\nfor (i in 2:nrep){\ny[i]=const+phi*y[i-1]+rnorm(1,0,0.5)\n}\n\nNow we can use the data to see what the ACF looks like for a AR(1) process.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% ACF(lag_max = 12, y) %>% autoplot()+theme_bw()+\nlabs(x=\"\", y=\"\", title=\"ACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\nNote the resemblance of the ACF of the simulated variable to that of the personal income. Now let’s take a look at the PACF.\n\ntsibble(y=y,period=seq(1,length(y)),index=period) %>% PACF(lag_max = 12, y) %>% autoplot()+theme_bw()+labs(x=\"\",y=\"PACF\",                         title=\"PACF For Simulated AR(1) phi=0.7, c=1\")\n\n\n\n\nOnce again, if you compare the PACF to the one in personal income growth you can see the resemblance in that there is one significant spike and all other partial autocorrelations are not statistically different from zero. This allows us to confirm that we can model personal consumption growth with an AR(1) process."
  },
  {
    "objectID": "ARIMA.html#model-selection",
    "href": "ARIMA.html#model-selection",
    "title": "6  ARIMA",
    "section": "6.4 Model Selection",
    "text": "6.4 Model Selection\nWe can further choose between these two models by looking at the AIC, AICc, or BIC.\n\nglance(PI_fit) %>% arrange(AICc) %>% select(.model:BIC)\n\n\n\n\n\n\n\n  \n    \n      Model Fit Measures\n    \n    \n    \n      .model\n      sigma2\n      AIC\n      AICc\n      BIC\n    \n  \n  \n    AR1\n4.30\n−20.25\n−19.89\n−17.09\n    LS\n5.51\n65.35\n66.10\n70.10\n  \n  \n  \n\n\n\n\nHere we note that the AR(1) model performs better in all of the metrics as they are significantly lower than those for the LS. The accuracy on the test set shown below, once more confirms that the AR(1) model performs better than the LS model.\n\nPI_fc<-PI_fit %>% forecast(new_data = PI_test)\nPI_fc  %>% accuracy(PI_test) \n\n\n\n\n\n\n\n  \n    \n      Accuracy Measures\n    \n    \n    \n      .model\n      .type\n      ME\n      RMSE\n      MAE\n      MPE\n      MAPE\n      MASE\n      RMSSE\n      ACF1\n    \n  \n  \n    AR1\nTest\n−1.51\n3.47\n2.45\n−115.69\n156.61\nNaN\nNaN\n0.33\n    LS\nTest\n2.60\n4.43\n3.87\n22.50\n106.69\nNaN\nNaN\n0.48\n  \n  \n  \n\n\n\n\nThe graph below shows the test set along with the forecast of the AR(1) model. Prediction confidence intervals are shown to highlight the uncertainty of the prediction. The blue line indicates the mean of the predictions which are assumed to follow a normal distribution.\n\nPI_fc %>% filter(.model==\"AR1\") %>% autoplot() + theme_classic() +\n  autolayer(PI_train, PI_Growth) +\n  autolayer(PI_test, PI_Growth) + \n  labs(title=\"Personal Income Growth AR(1) Forecast Accuracy\",\nsubtitle=\"1970-2021\", y=\"\",x=\"\")"
  },
  {
    "objectID": "Decisions.html#adding-uncertainty",
    "href": "Decisions.html#adding-uncertainty",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Adding Uncertainty",
    "text": "1.4 Adding Uncertainty\nTo solve decision trees with uncertainty use the folding-back procedure. Starting from the right of the decision tree and working back to the left at each probability node calculate the EMV. At each decision node, take the maximum of EMV’s to identify the optimal decision."
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company-ceo",
    "href": "Decisions.html#shopaholic-retail-company-ceo",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Shopaholic Retail Company CEO",
    "text": "1.1 Shopaholic Retail Company CEO\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome."
  },
  {
    "objectID": "Decisions.html#expected-monetary-value",
    "href": "Decisions.html#expected-monetary-value",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Expected Monetary Value",
    "text": "1.2 Expected Monetary Value\nAccording to your estimates, you believe that there is a 60% chance of success and a 40% chance of failure. Using this information, you can calculate the expected monetary value of the decision by multiplying the monetary value of each outcome by its respective probability and summing them up:\n\nExpected Value = (Prob. of Success * Value of Success) + (Prob. of Failure * Value of Failure)\nExpected Value = (0.6 * 5 million) + (0.4 * -3 million)\nExpected Value = 3 million + (-1.2 million)\nExpected Value = 1.8 million\n\nThe expected monetary value estimates the average monetary outcome you can expect from the decision. In this example, the expected value is positive, suggesting that launching the new product line results in a profit on average. Mathematically we express the monetary expected value as the sum product of probabilities and monetary values.\n\n\\(EMV=\\sum p_{i}x_{i}\\)\n\nwhere \\(EMV\\) is the expected value, \\(p_{i}\\) is the probability of decision \\(i\\) and \\(x_{i}\\) is the monetary values resulting from decision \\(i\\). Notice that if we had other decisions, we could also calculate their expected value. Hence, the expected value allows us to rank and choose decisions that yield the highest monetary value on average."
  },
  {
    "objectID": "Decisions.html#lessons-learned-in-this-chapter",
    "href": "Decisions.html#lessons-learned-in-this-chapter",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.7 Lessons Learned In This Chapter",
    "text": "1.7 Lessons Learned In This Chapter\n\nUse the concept of Expected Monetary Value.\nUse Decision Trees to map the business decision process.\nApply the backward induction method to solve decision problems.\n\n\n\n\n\nJaggia, Sanjiv, and Allison Kelly. 2022. “Business Statistics.”\n\n\nWinston, Wayne, and Christian Albright. 2019. “Practical Management Science.”"
  },
  {
    "objectID": "ARIMA.html#readings",
    "href": "ARIMA.html#readings",
    "title": "6  ARIMA",
    "section": "6.5 Readings",
    "text": "6.5 Readings\nHyndman (2021) Chapter 9 (ARIMA Models)."
  },
  {
    "objectID": "ARIMA.html#leasons-learned",
    "href": "ARIMA.html#leasons-learned",
    "title": "6  ARIMA",
    "section": "6.6 Leasons Learned",
    "text": "6.6 Leasons Learned\nIn this module you have been introduced to ARIMA model. Particularly you have learned to:\n\nUse the autocorrelations and partial autocorrelations to analyze time series.\nIdentify the AR(1) process using autocorrelations.\nModel the ARIMA process using the ARIMA() function.\nProvide forecasts of the ARIMA process in R.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "ETS.html#readings",
    "href": "ETS.html#readings",
    "title": "7  ETS",
    "section": "7.1 Readings",
    "text": "7.1 Readings\nHyndman (2021) Chapter 8 (Exponential Smoothing)."
  },
  {
    "objectID": "ETS.html#leasons-learned",
    "href": "ETS.html#leasons-learned",
    "title": "7  ETS",
    "section": "7.2 Leasons Learned",
    "text": "7.2 Leasons Learned\nIn this module you have been introduced to ETS model. Particularly you have learned to:\n\nUse the model() and ETS() functions to estimate the model.\nIdentify when ETS model is superior to Benchmarks or the ARIMA model using the accuracy() function.\nForecast time series with the ETS model.\n\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "Decisions.html#shopaholic-retail-company",
    "href": "Decisions.html#shopaholic-retail-company",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Shopaholic Retail Company",
    "text": "1.1 Shopaholic Retail Company\nImagine you are the CEO of the Shopaholic Retail Company, and you are considering whether to launch a new product line. You have conducted market research and have estimated two possible outcomes based on customer demand and competition.\n\nSuccess Scenario: If the new product line is well-received by customers and captures a significant market share, you anticipate an annual profit of $5 million.\nFailure Scenario: If the new product line fails to gain traction in the market, you estimate an annual loss of $3 million due to production costs and missed opportunities.\n\nIn this decision, you face uncertainty regarding customer preferences, market conditions, and competitive dynamics. The new product line’s success or failure will determine your company’s financial outcome."
  },
  {
    "objectID": "Decisions.html#consulting-team",
    "href": "Decisions.html#consulting-team",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Consulting Team",
    "text": "1.4 Consulting Team\nConsider now the option of hiring a consulting team that promises to give you a bit more certainty. The team provides a recommendation based on market research, historical data, and their expertise for $500,000. However, it is known that the team has a false positive rate (i.e., recommending to go with a project when the project would fail) of 10% and a false negative rate of 5% (i.e., recommending not going with a project and when the project would succeed). Below you can see the updated decision tree.\n\n\n\n\ngraph LR\n    F(( ))--&gt; |Positive p=?| A[ ]\n    A[ ] --&gt;|Introduce Production Line| C(( )) \n    A[ ] --&gt;|Don't Introduce| B( 0 )\n    C --&gt;|Success p=?| D( 4.5 )\n    C --&gt;|Failure p=?| E( -3.5 )\n    \n    F(( ))--&gt; |Negative p=?| G[ ]\n    G[ ] --&gt;|Introduce Production Line| H(( )) \n    G[ ] --&gt;|Don't Introduce| I( 0 )\n    H --&gt;|Success p=?| J( 4.5 )\n    H --&gt;|Failure p=?| K( -3.5 )\n\n\n\n\n\nNote that even though the consultants might recommend you not to introduce the production line, you can still decide to go against their recommendation. Also, there are some probabilities that are now unknown and must be calculated. For example, the probability that the introduction would be successful given that the recommendation is positive is unknown for now. We will use probability to uncover the missing probabilities below."
  },
  {
    "objectID": "Decisions.html#updating-probabilities",
    "href": "Decisions.html#updating-probabilities",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Updating Probabilities",
    "text": "1.5 Updating Probabilities\nThe Law of total probability is useful in determining the probabilities that we get a positive test or a negative test. In sum the law states:\n\n\\(p(A)=p(A|B)p(B)+p(A|B^c)p(B^c)\\)"
  },
  {
    "objectID": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-department-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.5 The VA Department of Transportation Wants Your Services",
    "text": "2.5 The VA Department of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they schedule the expected staff and number of ferries to run.\nAssume that the VA Department of transportation shares three weeks of data. The table below records the number of vehicles that used the ferry service:\n\n\n\nDay\nWeek 1\nWeek 2\nWeek 3\n\n\n\n\nMon\n1175\n1020\n1163\n\n\nTue\n1198\n1048\n1066\n\n\nWed\n1189\n1102\n1183\n\n\nThu\n1175\n1094\n1003\n\n\nFri\n1101\n1042\n1095\n\n\nSat\n1529\n1464\n1418\n\n\nSun\n1580\n1534\n1512\n\n\n\nWhat distribution would you use to simulate weekdays (Mon-Fri)? Would you simulate weekends (Sat and Sun) differently than weekdays? According to the data, what would be the minimum and maximum number of vehicles transported during weekdays (weekends)? Can you provide a sensible simulation for week 4?"
  },
  {
    "objectID": "Tools.html#using-loops-in-r",
    "href": "Tools.html#using-loops-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Using Loops in R",
    "text": "2.3 Using Loops in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to easily generate new variables for our model, or test different variations of our parameters to see how the model behaves.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true. Let’s illustrate the syntax of the for loop by simulating a demand for each book Monster Classics series and calculating the total revenue.\n\nRevenue&lt;-c()\n\nfor (i in MS$Price) {\n  Revenue&lt;-c(Revenue,i*rbinom(1,100,0.7))\n  print(Revenue)\n}\n\n[1] 722\n[1] 722.00 381.48\n[1] 722.00 381.48 450.14\n[1] 722.00 381.48 450.14 345.87\n[1] 722.00 381.48 450.14 345.87 188.65\n\n\nThe code above starts by creating an empty vector to store the revenue generated by each book. A for loop is then used to simulate the revenue for each book. The process starts by taking the first price in the MS$Price vector and multiplying it times a single random number drawn from the binomial distribution with \\(100\\) trials and probability \\(0.7\\) (this is our simulated demand). Note how the code combines the Revenue vector with the revenue generated by the simulation in the code c(Revenue, i*rbinom(1,100,0.7). This process is repeated for every number in the MS$Price vector, leading to a final vector that has the revenues for each book."
  },
  {
    "objectID": "Tools.html#using-conditionals-in-r",
    "href": "Tools.html#using-conditionals-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 Using Conditionals in R",
    "text": "2.4 Using Conditionals in R\nConditionals allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false.\nLet’s go back to the Monster Classic example and assume that the bookstore has gained additional insight on the demand of their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand&lt;-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand&lt;-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 90 85 74 71 70\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, then the random binomial number is drawn with the probability \\(0.9\\), if not the it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series one by one and adds a simulated demand. You can quickly realize that this becomes very efficient if the bookstore has a very large collection of books. Below is our data frame with the new simulated values.\n\n(MS&lt;-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     90\n2           Dracula  5.78     85\n3         Moby Dick  6.34     74\n4 War Of The Worlds  5.67     71\n5           Beowulf  2.45     70"
  }
]