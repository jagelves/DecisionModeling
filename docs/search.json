[
  {
    "objectID": "Tools2.html#data-wrangling-with-tidyverse.",
    "href": "Tools2.html#data-wrangling-with-tidyverse.",
    "title": "4  Time Series Tools",
    "section": "4.2 Data Wrangling With tidyverse.",
    "text": "4.2 Data Wrangling With tidyverse.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive to the user. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado<-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function imports the data as a tibble (a data structure similar to a data frame). There are three variables that are classified as character, while the rest are double. At this point you can preview the data with either the spec() or glimpse() commands.\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\"\n\n\nWhen using dplyr it’s always helpful to use piping. Generally speaking, piping allows us to chain functions. Piping (%>%) passes the object on the left of the pipe as the first argument to the right of the pipe. We can illustrate this by using the select() and arrange() functions.\n\navocado %>% select(c(average_price,geography)) %>%\n  arrange(desc(average_price)) \n\n# A tibble: 33,045 × 2\n   average_price geography           \n           <dbl> <chr>               \n 1          3.25 San Francisco       \n 2          3.17 Tampa               \n 3          3.12 San Francisco       \n 4          3.05 Miami/Ft. Lauderdale\n 5          3.04 Raleigh/Greensboro  \n 6          3.03 Las Vegas           \n 7          3    San Francisco       \n 8          3    Raleigh/Greensboro  \n 9          2.99 San Francisco       \n10          2.99 Jacksonville        \n# … with 33,035 more rows\n\n\n\navocado %>% filter(date!=ymd(\"2018-01-01\")) -> avocado\n\nThere is a lot to unpack in this line of code. Let’s start with the functions used. Both the select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data. By default it will sort in ascending order, hence we have used the desc() function to use descending order.\nNow, let’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%>%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. That data frame is sorted in descending order so that the highest average avocado price is displayed first.\nThis example highlights the use of dplyr functions to transform your data. There are plenty of other functions you can use, but learning these are outside the scope of this book. To find out more we recommend reading Wickham (2017) chapter 4. For now we will use one more data transformation technique to retrieve average price of organic avocados for California.\n\navocado %>% \n  filter(geography==\"California\", type==\"organic\",\n         year<=2018) %>%\n  select(date, average_price, geography) -> cali\n\nWhereas the select() function chooses particular variables, the filter() function chooses rows of the tibble that meet the conditions listed."
  },
  {
    "objectID": "Tools2.html#the-avocado-data-set",
    "href": "Tools2.html#the-avocado-data-set",
    "title": "4  Time Series Tools",
    "section": "4.1 The Avocado Data Set",
    "text": "4.1 The Avocado Data Set\nThe avocado data is weekly retail scan data for U.S retail volume (units) and price. Retail scan data comes directly from retailers’ cash registers based on actual retail sales of Hass avocados. The data reflects an expanded, multi-outlet retail data set. Multi-outlet reporting includes an aggregation of the following channels: grocery, mass, club, drug, dollar, and military. The Average Price (of avocados) in the table reflects a per unit (per avocado) cost, even when multiple units (avocados) are sold in bags. The Product Lookup codes (PLUs) in the data are only for Hass avocados. Other avocados (e.g. greenskins) are not included in this data.\nYou will notice that the data is weekly. However, there is an entry for 01/01/2018, that is right after 12/31/2017. Also, there are missing dates from 12/02/2018-12/31/2018."
  },
  {
    "objectID": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "href": "Tools2.html#chipotle-wants-you-to-forecast-avocado-prices",
    "title": "4  Time Series Tools",
    "section": "4.7 Chipotle Wants You to Forecast Avocado Prices",
    "text": "4.7 Chipotle Wants You to Forecast Avocado Prices\nChipotle is an American chain specializing in tacos and burritos that are made to order in front of the customer. Guacamole is the perfect pairing to their delicious food and one of Chipotle’s best sellers. Their guac uses just six ingredients: avocados, lime juice, cilantro, red onion, jalapeño, and kosher salt. Because of its popularity, each restaurant goes through approximately five cases of avocados a day, amounting to more than 44,000 pounds of avocados annually. Chipotle wants you to develop a model to forecast the price of avocados. This model will allow the company to"
  },
  {
    "objectID": "Tools2.html#lessons-learned-in-this-chapter",
    "href": "Tools2.html#lessons-learned-in-this-chapter",
    "title": "4  Time Series Tools",
    "section": "4.8 Lessons Learned in This Chapter",
    "text": "4.8 Lessons Learned in This Chapter\nIn this module you have been introduced to data wrangling, plotting and tsibbles. In particular the module dealt with:\n\nhandling dates with lubridate.\nrenaming, selecting and filtering variables.\nplotting using ggplot.\nCreating and managing tsibbles\nDecomposing a series using STL."
  },
  {
    "objectID": "Tools2.html#readings",
    "href": "Tools2.html#readings",
    "title": "4  Time Series Tools",
    "section": "4.9 Readings",
    "text": "4.9 Readings\nHyndman (2021) chapter 1 and chapter 2.\ntidy data: https://vita.had.co.nz/papers/tidy-data.pdf\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/.\n\n\nWickham, Hadley. 2017. “R for Data Science.” https://r4ds.hadley.nz."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Decision Modeling",
    "section": "",
    "text": "Preface\nThis book is based on three topics in Decision Modeling. Mainly, decision tree models, business simulation models and time series models. Special attention is given to business applications."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hyndman, Rob. 2021. “Forecasting Principles and Practice.”\nhttps://otexts.com/fpp3/."
  },
  {
    "objectID": "Benchmarks.html#benchmarks",
    "href": "Benchmarks.html#benchmarks",
    "title": "4  The Forecasting Process (Benchmarks)",
    "section": "4.1 Benchmarks",
    "text": "4.1 Benchmarks\nPerhaps one of the most intuitive (but Naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A Naive prediciton sets the prediction of a future period to the value of the preceding period. Your weight tomorrow is predicted to be the same as the weight observed today. Mathematically:\nWe can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). As kids grow, we might expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. Mathematically we would say:\nOne could also predict weight by observing your weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean can be a good predictor of your future weight. Mathematically:\nLastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of a diet, excercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:"
  },
  {
    "objectID": "Benchmarks.html#accuracy",
    "href": "Benchmarks.html#accuracy",
    "title": "4  The Forecasting Process (Benchmarks)",
    "section": "4.2 Accuracy",
    "text": "4.2 Accuracy\nWe will assess the fit of our models by comparing the fitted values against actual values. How far the fitted values are from the actual values will determine how well our model fits the data. If we square all of the distances (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE). Utlimately, how we decide to aggregate our errors will determine our measure of accuracy. For example is we follow the same procedure as the one for MSE’s but then find the square root, we have caluclated the RMSE. If we are to determine the ability of our model to predict values, we will instead compare forecasted values (derived from a training set) to a test value (or values). The training set is data that will allow us to fine-tune our model. On the other hand, the test set was not used to develop the model. By following this procedure, we ensure that our model doesn’t simply replicate the behavior in the training set but instead teases out the essential factors in the series that will allow us to forecast general tendencies effectively."
  },
  {
    "objectID": "Benchmarks.html#leasons-learned",
    "href": "Benchmarks.html#leasons-learned",
    "title": "4  The Forecasting Process (Benchmarks)",
    "section": "4.3 Leasons Learned",
    "text": "4.3 Leasons Learned\nIn this module you have been introduced to the general procedure in forecasting time series.\n\nLearning how create forecasts with simple heuristics."
  },
  {
    "objectID": "Benchmarks.html#readings",
    "href": "Benchmarks.html#readings",
    "title": "4  The Forecasting Process (Benchmarks)",
    "section": "4.4 Readings",
    "text": "4.4 Readings\nHyndman (2021) chapter 5.\n\n\n\n\nHyndman, Rob. 2021. “Forecasting Principles and Practice.” https://otexts.com/fpp3/."
  },
  {
    "objectID": "Tools.html#r-basics",
    "href": "Tools.html#r-basics",
    "title": "1  Tools for Working With Simulation",
    "section": "1.1 R Basics",
    "text": "1.1 R Basics\nObjects, vectors and data frames are all important in the R programming language. They mainly help us store and manipulate data. An object is a piece of data that can be stored in a variable. It can be as simple as a single integer or as output of regression analysis. Vectors are one-dimensional arrays of data that cn be stored in an object. Vectors can contain elements of various data types, such as numerical values, character strings, and logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Lastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record."
  },
  {
    "objectID": "Tools.html#r-intermediate",
    "href": "Tools.html#r-intermediate",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 R Intermediate",
    "text": "1.2 R Intermediate\nLoops and conditionals\nrandom number generators"
  },
  {
    "objectID": "Tools.html#readings",
    "href": "Tools.html#readings",
    "title": "2  Tools for Working With Simulation",
    "section": "2.6 Readings",
    "text": "2.6 Readings\nThese reading will help you review the concepts and theory necessary for completing this module. Grolemund (2014) reviews the R basics needed to perform computer simulation, while Gelves (2022) reviews the probability concepts necessary to understand the different random number generators.\nGrolemund (2014) Chapter 1 (The Very Basics), Chapter 3 (R Objects), Chapter 7.2, 7.3 (Conditional Statements), Chapter 9.3, 9.4, 9.5 (Loops).\nGelves (2022) Chapter 10 (Discrete Random Variables), Chapter 11 (Continuous Random Variables).\nADD SOME ARTICLE ON SIMULATION\n\n\n\n\nGelves, J. Alejandro. 2022. “Business Statistics.” https://jagelves.github.io/BusinessStatistics/.\n\n\nGrolemund, Garret. 2014. “Hands-on Programming with r.” https://jjallaire.github.io/hopr/#license."
  },
  {
    "objectID": "Tools.html#storing-our-data-in-r",
    "href": "Tools.html#storing-our-data-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.1 Storing Our Data in R",
    "text": "2.1 Storing Our Data in R\nObjects, vectors and data frames are all important in the R programming language. They are useful when storing and manipulating data in R. An object is a piece of data that can be stored in a variable. It can be as simple as a single integer or as informative as the output in regression analysis. The code below creates an object x that stores the number \\(5\\).\n\nx<-5\n\nVectors are one-dimensional arrays of data that can be stored in an object. They can contain elements of various data types, such as numerical values, character, or logical values (i.e., TRUE or FALSE). However, every component of the vector must be the same data type. Below, the vector Books stores the titles of \\(5\\) monster classics a bookstore plans to release as characters.\n\nbooks<-c(\"Frankenstein\",\"Dracula\",\"Moby Dick\",\n         \"War Of The Worlds\",\"Beowulf\")\n\nLastly, a data frame, is a two-dimensional data table with rows and columns. Each column in a data frame represents a different variable, and each row represents a single observation or record. You can think of a data frame as a collection of vectors that are related to each other. We can easily construct a data frame by combining one or more vectors using the data.frame() function in R.\n\ndata.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45))\n\n              Books Price\n1      Frankenstein  9.50\n2           Dracula  5.78\n3         Moby Dick  6.34\n4 War Of The Worlds  5.67\n5           Beowulf  2.45"
  },
  {
    "objectID": "Tools.html#automating-the-trial-process-in-r",
    "href": "Tools.html#automating-the-trial-process-in-r",
    "title": "1  Tools for Working With Simulation",
    "section": "1.2 Automating The Trial Process in R",
    "text": "1.2 Automating The Trial Process in R\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions. A loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true.\n\n\n\n\n\nConditionals, on the other hand, allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. By using loops and conditionals, you can create more complex and powerful programs in R."
  },
  {
    "objectID": "Tools.html#generating-random-numbers-in-r",
    "href": "Tools.html#generating-random-numbers-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.2 Generating Random Numbers in R",
    "text": "2.2 Generating Random Numbers in R\nThere are several functions available in R that can be used to generate random numbers. These functions are based on a specific probability distribution. For instance, the rbinom() function generates random numbers based on the binomial distribution, while the rnorm() function generates random numbers based on the normal distribution. By using these functions, we can generate random numbers that follow a specific probability distribution. For example, the binomial distribution may be useful in estimating the probability that a certain number of customers respond to a marketing campaign. However, as we will see below, we can also use the distribution to generate random customers who either responded or not responded to the campaign.\nAssume the bookstore is unsure on how many customers will buy their Monster Classic Series at list price. Their plan is to send \\(100\\) catalogs by mail to potential customers. Before they send the catalogs, they decide to get a rough idea on demand. Past data reveals that the probability a customer would buy any of the titles at the given prices is \\(0.70\\). Let’s modify our data frame by simulating demand with the rbinom() function.\n\n(MS<-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=rbinom(5,100,0.7)))\n\n              Books Price Demand\n1      Frankenstein  9.50     68\n2           Dracula  5.78     67\n3         Moby Dick  6.34     67\n4 War Of The Worlds  5.67     71\n5           Beowulf  2.45     71\n\n\nAs you can see, the rbinom() function has yielded \\(5\\) simulated outcomes from the binomial experiment with a probability of \\(0.7\\). The bookstore can now use this data to perform analysis on the profitability of the Monster Series if it desires."
  },
  {
    "objectID": "Tools.html#company-x-want-your-help",
    "href": "Tools.html#company-x-want-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Want Your Help",
    "text": "1.4 Company X Want Your Help"
  },
  {
    "objectID": "Tools.html#leasons-learned-in-this-chapter",
    "href": "Tools.html#leasons-learned-in-this-chapter",
    "title": "1  Tools for Working With Simulation",
    "section": "1.5 Leasons Learned In This Chapter",
    "text": "1.5 Leasons Learned In This Chapter"
  },
  {
    "objectID": "Tools.html#using-loops-and-conditional-in-r",
    "href": "Tools.html#using-loops-and-conditional-in-r",
    "title": "2  Tools for Working With Simulation",
    "section": "2.3 Using Loops and Conditional in R",
    "text": "2.3 Using Loops and Conditional in R\nLoops and conditionals are extremely useful when creating our simulation models. Among the many benefits, they will allow us to easily generate new variables for our model, or test different variations of our parameters to see how the model behaves.\nLoops and conditionals are two important control structures in the R programming language that allow you to execute specific blocks of code based on certain conditions.\nA loop is a programming construct that allows you to repeat a block of code a specified number of times or until a certain condition is met. There are several types of loops in R, including for loops, which execute a block of code for a fixed number of iterations, and while loops, which execute a block of code as long as a certain condition is true. Below you can see the structure of the while loop.\n\n\n\n\n\nConditionals allow you to execute different blocks of code based on whether a certain condition is true or false. The most common type of conditional in R is the if-else statement, which executes one block of code if a condition is true and a different block of code if the condition is false. Below is the structure of the conditional statement.\n\n\n\n\n\nLet’s go back to the Monster Classic example and assume that the bookstore has gained additional insight on the demand of their collection. In particular, assume that if the book is either Frankenstein or Dracula, the probability of a customer buying it is \\(0.9\\) (the probability of the other books remains at \\(0.7\\)). We can now modify our demand simulation using a loop and a conditional.\n\ndemand<-c()\n\nfor (i in MS$Books){\n  if (i==\"Frankenstein\"| i==\"Dracula\"){\n    p=0.9\n  }\n  else {p=0.7}\n  demand<-c(demand,rbinom(1,100,p))\n}\n\nprint(demand)\n\n[1] 89 92 64 56 71\n\n\nIn the code above, the inner conditional checks whether the titles are either Frankenstein or Dracula. If so, then the random binomial number is drawn with the probability \\(0.9\\), if not the it is drawn with probability \\(0.7\\). The for loop, goes through all the books in the series one by one and adds a simulated demand. You can quickly realize that this becomes very efficient if the bookstore has a very large collection of books. Below is our data frame with the new simulated values.\n\n(MS<-data.frame(Books=c(\"Frankenstein\",\"Dracula\",\n                     \"Moby Dick\",\n                     \"War Of The Worlds\",\"Beowulf\"), \n           Price=c(9.5,5.78,6.34,5.67,2.45),\n           Demand=demand))\n\n              Books Price Demand\n1      Frankenstein  9.50     89\n2           Dracula  5.78     92\n3         Moby Dick  6.34     64\n4 War Of The Worlds  5.67     56\n5           Beowulf  2.45     71"
  },
  {
    "objectID": "Tools.html#company-x-wants-your-help",
    "href": "Tools.html#company-x-wants-your-help",
    "title": "1  Tools for Working With Simulation",
    "section": "1.4 Company X Wants Your Help",
    "text": "1.4 Company X Wants Your Help"
  },
  {
    "objectID": "Tools.html#lessons-learned-in-this-chapter",
    "href": "Tools.html#lessons-learned-in-this-chapter",
    "title": "2  Tools for Working With Simulation",
    "section": "2.5 Lessons Learned In This Chapter",
    "text": "2.5 Lessons Learned In This Chapter\n\nGenerating Random Numbers with R functions.\nUsing Loops and Conditionals.\nUnderstanding objects, vectors, and data frames."
  },
  {
    "objectID": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "href": "Tools.html#the-va-deaprtment-of-transportation-wants-your-services",
    "title": "2  Tools for Working With Simulation",
    "section": "2.4 The VA Deaprtment of Transportation Wants Your Services",
    "text": "2.4 The VA Deaprtment of Transportation Wants Your Services\nThe VA ferry crossing the James River was first established in \\(1925\\). The ferry transports vehicles back and forth from Jamestown to Scotland in a \\(15\\)-minute ride. The VA Department of Transportation wants you to simulate the daily demand for the ferry so that they schedule the expected staff and number of ferries to run."
  },
  {
    "objectID": "Simulation.html#inputs-calculated-and-objective",
    "href": "Simulation.html#inputs-calculated-and-objective",
    "title": "2  Simulation in R",
    "section": "2.1 Inputs, Calculated, and Objective",
    "text": "2.1 Inputs, Calculated, and Objective"
  },
  {
    "objectID": "Simulation.html#law-of-large-numbers",
    "href": "Simulation.html#law-of-large-numbers",
    "title": "3  Simulation in R",
    "section": "3.4 Law of Large Numbers",
    "text": "3.4 Law of Large Numbers\nBefore we answer the question of how much fish to order, we must realize a couple of flaws of the model we have created. Mainly, we have run the simulation once and it is unlikely (although possible) that the attendance will be exactly \\(44\\). Instead, we want to provide the restaurant with a set of eventualities. Worst case scenarios (low attendance), best case scenarios (high attendance), and most likely outcome for their decision. This is only possible if we generate several attendance numbers, and see how the output behaves.\nAn additional problem is that if we provide the average profit of our model, we want to make sure that the average is not biased. Recall that as the sample size of a study increases, the average of the sample will converge towards the true population mean. In other words, as the number of simulations in our model increases, the estimate of the expected profit becomes more and more accurate. This is known as the Law of Large Numbers.\nThe code below repeats the simulation not once, or twice, but several times. Although, there is not a set number of times one should run a simulation to get a good estimate of the mean (or distribution), computers are powerful enough to run thousands if not millions of simulations. Below we run the simulation 10,000 times for illustration purposes.\n\nn<-10000\nV_Order_Oz<-rep(Order_Oz,n)\nV_Price_Fish_Oz<-rep(Price_Fish_Oz,n)\nV_Price_Miso<-rep(Price_Miso,n)\nV_Entry_Fee<-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz<-rep(Fish_Entitled_Oz,n)\n\nset.seed(12)\nV_Attendance<-round(rtriang(n,20,50,30),0)\nConsumption<-V_Attendance*V_Fish_Entitled_Oz\nAvailable<-V_Order_Oz\n\nV_Profit<-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\n\nFrom the simulation it seems like that the restaurant would make on average a profit of about 211.5 dollars if they order 160 ounces of fish. There are however, a couple of questions left unanswered. First, what are the other possible profit when ordering 160 ounces? Second, is there another amount of fish that would give them a higher expected profit?"
  },
  {
    "objectID": "Simulation.html#the-flaw-of-averages",
    "href": "Simulation.html#the-flaw-of-averages",
    "title": "2  Simulation in R",
    "section": "2.6 The Flaw of Averages",
    "text": "2.6 The Flaw of Averages\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population. For example, if a group of people has an average height of 5’10”, it does not mean that every individual in the group is exactly 5’10” tall. Some people may be shorter, while others may be taller. The flaw of averages can lead to inaccurate assumptions and incorrect conclusions, particularly when making predictions or decisions based on the average value. It is important to consider the distribution and variability of the characteristic within the population when making predictions or decisions, rather than relying solely on the average value."
  },
  {
    "objectID": "Simulation.html#the-newsvendor-problem",
    "href": "Simulation.html#the-newsvendor-problem",
    "title": "2  Simulation in R",
    "section": "2.5 The Newsvendor Problem",
    "text": "2.5 The Newsvendor Problem\nThe news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order and at what price to sell it. The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them. In the news vendor problem, there are two main factors that influence the decision: the demand for the product and the cost of the product. The demand for the product is uncertain, as it depends on various factors such as the weather, the competition, and the price of the product. The cost of the product is also uncertain, as it may vary based on factors such as production costs, transportation costs, and discounts. The goal of the news vendor is to maximize profits by ordering the optimal amount of the product and setting the optimal price. The news vendor must make this decision based on incomplete information about the demand and cost of the product, which makes the problem challenging and requires the use of probabilistic models and optimization techniques."
  },
  {
    "objectID": "Simulation.html#sensitivity-analysis",
    "href": "Simulation.html#sensitivity-analysis",
    "title": "3  Simulation in R",
    "section": "3.7 Sensitivity Analysis",
    "text": "3.7 Sensitivity Analysis\nSensitivity analysis is a tool used in decision-making to assess the robustness of a model or decision by evaluating the impact of changes in certain key input variables on the output of the model or decision. Sensitivity analysis helps to identify which variables are most important and how sensitive the output is to changes in those variables. It is often used in financial, economic, and engineering contexts to evaluate the feasibility and risk of different scenarios or to identify potential areas of improvement."
  },
  {
    "objectID": "Simulation.html#a-restaurant-needs-your-help",
    "href": "Simulation.html#a-restaurant-needs-your-help",
    "title": "2  Simulation in R",
    "section": "2.1 A Restaurant Needs Your Help",
    "text": "2.1 A Restaurant Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. They purchase the fish at the market and discard any unsold fish at the end of the day. However, they are unsure of how many people will attend the event. To ensure they have enough sushi and budget appropriately, they want to estimate the total cost and amount of sushi needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (which is divisible). Each guest at the festival is entitled to 5 ounces of sushi. If the sushi runs out, Sushi X has promised to serve their famous Miso soup, which cost them a total of 300 dollars for the event. Given that the festival charges 20 dollars per entry, how much yellow-tail would you recommend Sushi X purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#model-framework",
    "href": "Simulation.html#model-framework",
    "title": "3  Simulation in R",
    "section": "3.2 Model Framework",
    "text": "3.2 Model Framework\nThe restaurant provides you with a lot of information which might be overwhelming at first glance. To make the task less daunting, we should organize/classify the information so that we can create a model. In general, for many business problems, the data can be classified into the following parts:\n\nThe inputs have given fixed values and provide the model’s basic structure. These are values that are most likely to be given and determined.\nThe decision variables are values the decision maker controls. We are usually interested in finding the optimal level of this variable.\nThe calculated values transform inputs and decision variables to other values that help describe our model. These make the model more informative and often are required to derive outputs.\nThe random variables are the primary source of uncertainty in the model. They are often modeled by sampling probability distributions.\nThe outputs are the ultimate values of interest; the inputs, decision variables, random variables, or calculated values determine them.\n\nBelow, you can see how we can classify and input the information in R.\n\nlibrary(extraDistr)\nOrder_Oz<-160 # Decision Variable\n\nPrice_Fish_Oz<-9/16 # Input\nPrice_Miso<-300 # Input\nEntry_Fee<-20 # Input\nFish_Entitled_Oz<-5 #Input\n\nset.seed(20)\nAttendance<-round(rtriang(1,20,50,30),0) # Random Variable\n\nConsumption<-Attendance*Fish_Entitled_Oz # Calculated \nAvailable<-Order_Oz # Calculated\n\nProfit<-min(Consumption,Available)/Fish_Entitled_Oz*Entry_Fee-Order_Oz*Price_Fish_Oz-Price_Miso #Outcome\n\nIn the model above, you can see how the inputs have fixed values. These values were given to us by the restaurant and it seems like the they have little or no control over them (the market price of the fish, the cost of making Miso soup, etc.). The random variable, captures the source of uncertainty (i.e., how many people attend the event). As you can see we have used here the rtraing() function from the extraDistr package to generate the attendance. We have chosen the triangle distribution since the restaurant has provided us with a lower limit, an upper limit, and a most likely case for attendance. Note also the use of the set.seed() function. This allows you to replicate the random numbers generated.\nThe calculated variables combine inputs, the decision variable, and the random variable to provide insight on how much is fish is expected to be both consumed and available. They also help us determine the output, which is our main guide in knowing whether the decision of ordering \\(x\\) ounces of fish is the “best”."
  },
  {
    "objectID": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "href": "Simulation.html#roll-with-it-sushi-needs-your-help",
    "title": "3  Simulation in R",
    "section": "3.1 Roll With It Sushi Needs Your Help",
    "text": "3.1 Roll With It Sushi Needs Your Help\nRenowned for serving the freshest sushi in the city, Roll With It Sushi is hosting a yellow-tail festival. Every morning, the Sous-Chef purchases fish at the market and any unsold fish at the end of the day gets discarded.\nThe restaurant has contacted you because they are still determining how many people will attend the event and worry about how this will impact their business financially. To ensure they have enough sushi and budget appropriately, they want you to recommend the amount of fish needed. Based on past events, they expect at least 20 people to attend and have the capacity to seat up to 50 guests. They anticipate the most likely attendance to be around 30 people, and without your guidance they feel like this is the best guide in determining the fish needed.\nRoll With It Sushi can purchase any quantity of yellow-tail from the fresh market at 9 dollars for 16 ounces (divisible). Each guest at the festival is entitled to 5 ounces of fish. If the sushi runs out, some customers will not be happy. However, the restaurant has promised to refund their entry fee. Additionally, they have promised to serve their famous Miso soup to every attendee. The cost of making a batch for up to 50 guests is 300 dollars.\nGiven that the festival charges 20 dollars per entry, how much yellow-tail would you recommend the restaurant to purchase to maximize expected profits?"
  },
  {
    "objectID": "Simulation.html#the-news-vendor-problem",
    "href": "Simulation.html#the-news-vendor-problem",
    "title": "3  Simulation in R",
    "section": "3.3 The News Vendor Problem",
    "text": "3.3 The News Vendor Problem\nNote how the decision variable (Order_Oz) affects directly our outcome (Profit). We can see that it decreases the restaurant’s profit through costs, but also affects revenue through the amount of fish available. This is the heart of the problem. We don’t know how many people will attend, so if the restaurant orders too much fish their profits will go down because their costs are large. However, if they order too little then they will have to issue refunds, which decrease their revenue.\nAs you observe the Profit formula in the code above, you’ll notice the use of the min() function. This function returns the minimum of the Attendance and Consumption. The intuition here is that the restaurant can only collect entry fees for the people who consumed the sushi when the attendance is greater than the amount of sushi available. Likewise, their revenue will be capped at the total amount of people who attended, even if they ordered plenty of fish.\nThe problem illustrated above is called the news vendor problem. The news vendor problem is a classic decision-making problem in operations research and economics that involves deciding how much of a product to order (and sometimes at what price to sell it). The problem is called the “news vendor” problem because it was originally used to model the decision-making process of a newspaper vendor trying to decide how many copies of a newspaper to order and at what price to sell them."
  },
  {
    "objectID": "Simulation.html#recommendation",
    "href": "Simulation.html#recommendation",
    "title": "2  Simulation in R",
    "section": "2.5 Recommendation",
    "text": "2.5 Recommendation\nThere are a couple of things that are now evident. It seems likely that the restaurant would make on average a profit of about 234 dollars if they order 160 ounces of fish. We can now inform the restaurant that it is likely they will make a profit if they order 160 ounces. There are a couple of questions left unanswered. First, 234 dollars are expected of profits, what are the other possible outcomes when ordering 160 ounces? Secondly, is there another amount of fish that would give them a higher expected profit?\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering 160 ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to 250 dollars. So a better recommendation to the restaurant would be to inform them that when ordering 160 ounces, they will most likely get a profit of 250 dollars. There is a small risk of them making less that 100 dollars, but that they should not expect more than 250 dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is commonly known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population.\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz<-9/16 # Input\nPrice_Miso<-300 # Input\nEntry_Fee<-20 # Input\nFish_Entitled_Oz<-5 #Input\n\nProfits<-c()\n\nfor (i in Order_Oz){\nn<-10000\nV_Order_Oz<-rep(i,n)\nV_Price_Fish_Oz<-rep(Price_Fish_Oz,n)\nV_Price_Miso<-rep(Price_Miso,n)\nV_Entry_Fee<-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz<-rep(Fish_Entitled_Oz,n)\n\nV_Attendance<-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption<-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable<-V_Order_Oz # Calculated\n\nV_Profit<-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits<-c(Profits,mean(V_Profit))\n}\n\n(results<-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order Profits\n1   160 212.934\n2   170 225.413\n3   180 235.406\n4   190 241.335\n5   200 242.848\n6   210 243.837\n7   220 240.640\n8   230 234.759\n9   240 230.958"
  },
  {
    "objectID": "Simulation.html#flaw-of-averages",
    "href": "Simulation.html#flaw-of-averages",
    "title": "3  Simulation in R",
    "section": "3.5 Flaw Of Averages",
    "text": "3.5 Flaw Of Averages\nTo answer the first question we can generate a histogram of all the results of our simulation model. We can then report this to the restaurant and make them aware of all of the possible outcomes of ordering 160 ounces of fish. Below, we show the histogram of our model’s outcomes.\n\nhist(V_Profit, main=\"Expected Profits of an Order of 160 Ounces\",\n     xlab=\"\")\nabline(v=mean(V_Profit), lwd=2)\n\n\n\n\nAs you can see most of the outcomes are close to 250 dollars. So a better recommendation to the restaurant would be to inform them that when ordering 160 ounces, they will most likely get a profit of 250 dollars. There is a small risk of them making less that 100 dollars, but that they should not expect more than 250 dollars. The average in this case seems to be a poor predictor of what is expected as its frequency is not very large as shown in the histogram. This result is known as the flaw of averages.\nThe flaw of averages, also known as the “law of averages fallacy,” is the idea that the average value of a particular characteristic in a population can be used to represent the value of that characteristic for individual members of the population. This is often not the case because the average value can be misleading and does not take into account the variability and distribution of the characteristic within the population."
  },
  {
    "objectID": "Simulation.html#optimal-order-amount",
    "href": "Simulation.html#optimal-order-amount",
    "title": "3  Simulation in R",
    "section": "3.6 Optimal Order Amount",
    "text": "3.6 Optimal Order Amount\nNow to answer the main question, what should be the amount ordered of fish? To answer this question we will substitute several possible order options into our model and then retrieve the one that gives us the highest expected profit. We can easily do this in R with a loop. Below is the code:\n\nOrder_Oz=seq(160,240,10)\nPrice_Fish_Oz<-9/16 # Input\nPrice_Miso<-300 # Input\nEntry_Fee<-20 # Input\nFish_Entitled_Oz<-5 #Input\n\nProfits<-c()\n\nfor (i in Order_Oz){\nn<-10000\nV_Order_Oz<-rep(i,n)\nV_Price_Fish_Oz<-rep(Price_Fish_Oz,n)\nV_Price_Miso<-rep(Price_Miso,n)\nV_Entry_Fee<-rep(Entry_Fee,n)\nV_Fish_Entitled_Oz<-rep(Fish_Entitled_Oz,n)\n\nV_Attendance<-round(rtriang(n,20,50,30),0) # Random Variable\nConsumption<-V_Attendance*V_Fish_Entitled_Oz # Calculated\nAvailable<-V_Order_Oz # Calculated\n\nV_Profit<-pmin(Consumption,Available)/V_Fish_Entitled_Oz*V_Entry_Fee-V_Order_Oz*V_Price_Fish_Oz-V_Price_Miso\nProfits<-c(Profits,mean(V_Profit))\n}\n\n(results<-data.frame(Order=Order_Oz,Profits=Profits))\n\n  Order Profits\n1   160 212.934\n2   170 225.413\n3   180 235.406\n4   190 241.335\n5   200 242.848\n6   210 243.837\n7   220 240.640\n8   230 234.759\n9   240 230.958\n\n\nThis table suggests that 160 ounces is not optimal. Once again highlighting that the average attendance is not a good estimate of how much fish we should order. Instead, we can see that 210 ounces of fish should be ordered (this feeds about 42 people) to maximize the expected profits."
  },
  {
    "objectID": "Decisions.html#expected-value",
    "href": "Decisions.html#expected-value",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.1 Expected Value",
    "text": "1.1 Expected Value"
  },
  {
    "objectID": "Decisions.html#decision-trees",
    "href": "Decisions.html#decision-trees",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.2 Decision Trees",
    "text": "1.2 Decision Trees"
  },
  {
    "objectID": "Decisions.html#bayes-theorem",
    "href": "Decisions.html#bayes-theorem",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.3 Bayes’ Theorem",
    "text": "1.3 Bayes’ Theorem"
  },
  {
    "objectID": "Decisions.html#readings",
    "href": "Decisions.html#readings",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.4 Readings",
    "text": "1.4 Readings"
  },
  {
    "objectID": "Decisions.html#lessons-learned",
    "href": "Decisions.html#lessons-learned",
    "title": "1  Decisions Under Uncertainty",
    "section": "1.5 Lessons Learned",
    "text": "1.5 Lessons Learned"
  },
  {
    "objectID": "Simulation.html#lessons-learned-in-this-chapter",
    "href": "Simulation.html#lessons-learned-in-this-chapter",
    "title": "3  Simulation in R",
    "section": "3.8 Lessons Learned In This Chapter",
    "text": "3.8 Lessons Learned In This Chapter\n\nUnderstanding the Parts of a Simulation Model.\nCreating a Simulation Model in R.\nUnderstanding the Flaw of Averages.\nFinding the Optimal Value."
  },
  {
    "objectID": "Simulation.html#readings",
    "href": "Simulation.html#readings",
    "title": "3  Simulation in R",
    "section": "3.9 Readings",
    "text": "3.9 Readings\nThese reading will help you review the concepts and theory necessary for completing this module.\nADD SOME ARTICLE ON SIMULATION"
  },
  {
    "objectID": "Tools2.html#visualizing-the-data",
    "href": "Tools2.html#visualizing-the-data",
    "title": "4  Time Series Tools",
    "section": "4.4 Visualizing The Data",
    "text": "4.4 Visualizing The Data\nTo visualize the data we will be using ggplot2. One of the main functions in ggplot2 is the aes() function. This function sets the plotting canvas and determines which variables are to be plotted. The geom_line() function specifies the type of plot. In time series we will use the line plot regularly. Below you can see the code to create a line plot of the average price of avocados for California.\n\nggplot(data=cali) + \n  geom_line(mapping=aes(x=date,y=average_price, group=geography),color=\"black\") +\n  theme_classic() + labs(x=\"\",y=\"Average Price\", \n                         title=\"Organic Avocado Price in California\", subtitle=\"2015-2018\")  \n\n\n\n\nThe graph shows that the average price of avocados in California has been increasing. The average price reached a maximum of about 2.6 in 2017 and was at a minimum in in the spring of 2015. There seems to be seasonal patterns with low prices at the beginning of the year and peaks mid year."
  },
  {
    "objectID": "Tools2.html#setting-up-the-date-variable",
    "href": "Tools2.html#setting-up-the-date-variable",
    "title": "4  Time Series Tools",
    "section": "4.3 Setting Up the Date Variable",
    "text": "4.3 Setting Up the Date Variable\nYou will notice that the date variable is of type character. You can convince yourself of this by using the class() function:\n\nclass(avocado$date)\n\n[1] \"character\"\n\n\nTo coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced:\n\nclass(avocado$date)\n\n[1] \"Date\""
  },
  {
    "objectID": "Tools2.html#tssibbles-and-decomposition",
    "href": "Tools2.html#tssibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.4 tssibbles and Decomposition",
    "text": "4.4 tssibbles and Decomposition\n\nlibrary(fpp3, quietly = T, warn.conflicts = F)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tsibble     1.1.3     ✔ feasts      0.3.0\n✔ tsibbledata 0.4.1     ✔ fable       0.3.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-31\")-> avocadots\n\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) -> calits\n\ncalits  %>% mutate(\n  `10-MA`=slider::slide_dbl(average_price,mean,\n                            .before=5, .after=4,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`10-MA`), col=\"red\")\n\nWarning: Removed 9 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits  %>% mutate(\n  `2X10-MA`=slider::slide_dbl(`10-MA`,mean,\n                           .before=1, .after=0,.complete=T)) -> calits\n\ncalits %>%\n  autoplot(average_price)+\n  geom_line(aes(y=`2X10-MA`), col=\"red\")\n\nWarning: Removed 10 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-31\") %>%\n  model(STL(average_price~trend(window=150)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()"
  },
  {
    "objectID": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "href": "Tools2.html#loading-tidyverse-and-inspecting-the-data.",
    "title": "4  Time Series Tools",
    "section": "4.2 Loading tidyverse and Inspecting the Data.",
    "text": "4.2 Loading tidyverse and Inspecting the Data.\ntidyverse is a collection of packages in R that allow us to manipulate, explore and visualize data. There are a couple of packages within tidyverse (dplyr and tidyr) that we will be using to transform our data and get it ready for analysis. dplyr will allow us to do most of our data manipulation: creating new variables, renaming variables, filtering values, sorting, grouping, and summarizing, among others. tidyr will allow us to pivot data sets, unite or separate columns, and deal with missing values. Although it is always possible to complete these tasks using base R, tidyverse allows us to efficiently perform these operations using data manipulation verbs that are very intuitive to the user. Below we load the library.\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ purrr   0.3.5 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nAs you can see, several packages were attached (loaded) when we write library(tidyverse). As mentioned, both tidyr and dplyr are part of this overall package. Now that the package is loaded we can import our data by using the read_csv() function from the readr package.\n\navocado<-read_csv(\"https://jagelves.github.io/Data/avocado2020.csv\")\n\nRows: 33045 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): date, type, geography\ndbl (10): average_price, total_volume, 4046, 4225, 4770, total_bags, small_b...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe function imports the data as a tibble (a data structure similar to a data frame). There are three variables that are classified as character, while the rest are double. At this point you can preview the data with either the spec() or glimpse() commands.\n\nspec(avocado)\n\ncols(\n  date = col_character(),\n  average_price = col_double(),\n  total_volume = col_double(),\n  `4046` = col_double(),\n  `4225` = col_double(),\n  `4770` = col_double(),\n  total_bags = col_double(),\n  small_bags = col_double(),\n  large_bags = col_double(),\n  xlarge_bags = col_double(),\n  type = col_character(),\n  year = col_double(),\n  geography = col_character()\n)\n\n\nYou will notice that the date variable is of type character. To coerce this variable to a date we can use the lubridate package. Specifically, since the date variable is formatted as month/day/year we will use the mdy() function.\n\nlibrary(lubridate, warn.conflicts = F, quietly = T)\navocado$date<-mdy(avocado$date)\n\nWe can confirm that the type of the variable has now been coerced by using the class() function.\n\nclass(avocado$date)\n\n[1] \"Date\""
  },
  {
    "objectID": "Tools2.html#piping-and-dplyr",
    "href": "Tools2.html#piping-and-dplyr",
    "title": "4  Time Series Tools",
    "section": "4.3 Piping and dplyr",
    "text": "4.3 Piping and dplyr\nWhen using dplyr it’s always helpful to use piping. Generally speaking, piping allows us to chain functions. Piping (%>%) passes the object on the left of the pipe as the first argument to the right of the pipe. We can illustrate this by using the select() and arrange() functions.\n\navocado %>% select(c(average_price,geography)) %>%\n  arrange(desc(average_price)) \n\n# A tibble: 33,045 × 2\n   average_price geography           \n           <dbl> <chr>               \n 1          3.25 San Francisco       \n 2          3.17 Tampa               \n 3          3.12 San Francisco       \n 4          3.05 Miami/Ft. Lauderdale\n 5          3.04 Raleigh/Greensboro  \n 6          3.03 Las Vegas           \n 7          3    San Francisco       \n 8          3    Raleigh/Greensboro  \n 9          2.99 San Francisco       \n10          2.99 Jacksonville        \n# … with 33,035 more rows\n\n\nThere is a lot to unpack in this line of code. Let’s start with the functions used. Both the select() and arrange() functions are part of the dplyr package. As the name indicates, the select() function selects variables from a tibble or data frame. The arrange() function sorts the data. By default it will sort in ascending order, hence we have used the desc() function to use descending order.\nNow, let’s focus on the entire code by reading it from left to right. avocado is the tibble that contains all of the data. Since it is to the left of the pipe (%>%), it passes as the first argument of the select() function. That is why you don’t see avocado as the first argument listed in the select() function. The new data frame (i.e., the one with only the geography and the average price) then passes as the first argument of the arrange() function that follows the second pipe. That data frame is sorted in descending order so that the highest average avocado price is displayed first. The filter() function is used below:\n\navocado %>% filter(date!=ymd(\"2018-01-01\")) -> avocado\n\nWhereas the select() function chooses particular variables, the filter() function chooses rows of the tibble that meet the conditions listed.\nThese examples highlight the use of dplyr functions to transform your data. There are plenty of other functions you can use, but learning these are outside the scope of this book. To find out more, we recommend reading Wickham (2017) chapter 4. For now we will use one more data transformation technique to retrieve average price of organic avocados for California for the period 2015-2018.\n\navocado %>% \n  filter(date!=ymd(\"2018-01-01\"), \n         geography==\"California\", type==\"organic\", \n         year<=2018) %>%\n  select(date, average_price, geography) -> cali"
  },
  {
    "objectID": "Tools2.html#tsibbles-and-decomposition",
    "href": "Tools2.html#tsibbles-and-decomposition",
    "title": "4  Time Series Tools",
    "section": "4.5 tsibbles and Decomposition",
    "text": "4.5 tsibbles and Decomposition\nWhen dealing with time series, we will be using a data structure called a tsibble (time series tibble). These are defined by a time index (i.e., the date), and some keys (i.e., some dimensions). In the avocado data set we are mostly interested in the average price of the avocados. These are classified by location (geography) and type (e.g., organic and conventional). tsibbles, as well a variety of packages that help us analyze time series are all part of the fpp3 package.\n\nlibrary(fpp3, quietly = T, warn.conflicts = F)\n\n── Attaching packages ──────────────────────────────────────────── fpp3 0.4.0 ──\n\n\n✔ tsibble     1.1.3     ✔ feasts      0.3.0\n✔ tsibbledata 0.4.1     ✔ fable       0.3.2\n\n\n── Conflicts ───────────────────────────────────────────────── fpp3_conflicts ──\n✖ lubridate::date()    masks base::date()\n✖ dplyr::filter()      masks stats::filter()\n✖ tsibble::intersect() masks base::intersect()\n✖ tsibble::interval()  masks lubridate::interval()\n✖ dplyr::lag()         masks stats::lag()\n✖ tsibble::setdiff()   masks base::setdiff()\n✖ tsibble::union()     masks base::union()\n\navocado %>%\n  as_tsibble(key=c(type, geography),\n           index=date, regular=T) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-> avocadots\n\nNote that the as_tsibble() function was called with the parameter regular set at true since the data is weekly. filter_index() is a helpful function that allows you to determine windows for analysis. Since there are some missing dates in December 2018, we limit the analysis to 2015-2018.\nWe can now specify the tsibble for analysis, by using dplyr.\n\navocadots %>% filter(geography==\"California\", type==\"organic\") %>%\n  select(date,geography,average_price,total_volume) %>%\n  filter_index(\"2015-01-04\"~\"2018-12-02\")-> calits"
  },
  {
    "objectID": "Tools2.html#decomposition",
    "href": "Tools2.html#decomposition",
    "title": "4  Time Series Tools",
    "section": "4.6 Decomposition",
    "text": "4.6 Decomposition\nAs mentioned above, the avocado data for California seems to have a trend and a seasonal pattern. There are methods available to tease out these components. STL (Season Trend decomposition using LOESS) decomposes the series into a trend, seasonality, and an error (unexplained) component. It is easy to run in R by using the command below:\n\ncalits %>% filter_index(\"2015-01-04\"~\"2018-12-02\") %>%\n  model(STL(average_price~trend(window=200)+\n              season(window=52), robust=TRUE)) %>%\n  components() %>% autoplot()+ theme_classic()\n\n\n\n\nAs shown above, the trend is increasing and the seasonal component confirms low levels at the beginning of the year and high levels in the summer.\nThe decomposition itself is constructed by first finding a moving average of the series to track the trend. As you can see the window in the trend is set to a high window so that moving average tracks the general trend and not the small fluctuations of the series. This trend is then subtracted from the series to obtain a de-trended series. The seasonal component is calculated by averaging the values based on the window provided (52 weeks or yearly). The error is the remaining fluctuation of the series that is not explained by the trend or the seasonal component (Series-Trend-Seasonal=Error)."
  }
]