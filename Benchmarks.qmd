# Model Benchmarks

This module will introduce the forecasting procedure by illustrating the process with four different benchmarks (Naive, LS, Mean, and Drift). We will calculate accuracy measures for each of these benchmarks (e.g., Mean Error, Mean Absolute Error, Root Mean Squared Error). In general, accuracy measures compare the fitted values with the actual values. A good model will account for most of the series's variation, leaving a small random error. We will ensure that we do not over fit a model to a training set by assessing its accuracy using a test set.

## Benchmarks

Perhaps one of the most intuitive (but Naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A Naive prediction sets the prediction of a future period to the value of the preceding period. Your weight tomorrow is predicted to be the same as the weight observed today. Mathematically:

<center>
$\hat y_{T+h}=y_T$  

</center>


We can adjust the Naive prediction by accounting for some natural drift (an increase or decrease). As kids grow, we might expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. Mathematically we would say:

<center>
$\hat y_{T+h}=y_T+h(\frac{y_t-y_1}{T-1})$  

</center>

One could also predict weight by observing your weight during a period and averaging the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean can be a good predictor of your future weight. Mathematically:

<center>
$\hat y_{T+h}=\frac{(y_1+y_2+...+y_T)}{T}$  

</center>

Lastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of a diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward trend of our weight. Formally:

<center>
$\hat y_{T+h}=b_0+b_1(T+h)$  

</center>

## Predicting the Avocado Data

Let's apply the forecasting methods to the average prices for avocados in California. Start by loading the `fpp3` package and importing the data from https://jagelves.github.io/Data/CaliforniaAvocado.csv. Recall, that we can create a tsibble from the csv file by using the `as_tsibble()` function.
```{r warning=FALSE, echo=FALSE, message=FALSE}
library(tidyverse, quietly = T, warn.conflicts = F)
library(fpp3, quietly=T, warn.conflicts = F)
cali<-read_csv("https://jagelves.github.io/Data/CaliforniaAvocado.csv")
```


```{r}
cali %>%
  as_tsibble(key=c(geography),
             index=date, regular=T) -> calits
```

The `model()` function can run the models discussed in section 5.1. We save this to an object called *fit*.

```{r}
fit <- model(calits,mean=MEAN(average_price),
              Naive=NAIVE(average_price),
              Drift=RW(average_price~drift()),
              LS=TSLM(average_price~date))
```

To explore the model coefficients, we can use the `coef()` function. We have used the `gt` package to make the table visually appealing. 

```{r ,echo=FALSE}
library(gt)
coef(fit)%>% gt() %>% 
  cols_align("center") %>% 
  tab_header(title = 
               md("Model Coefficients For The Avocado Data")) %>% tab_style(locations =                                                    cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "bold"))) %>% 
  fmt_number(columns =c(statistic,estimate,std.error,p.value),
             decimals = 2)
```


```{r}
calits %>% autoplot(average_price) + theme_classic() + 
  geom_line(aes(y = .fitted), col="red",
            data = augment(fit) %>% filter(`.model`=="LS"))
```
## Accuracy

We will assess the fit of the benchmarks by comparing the fitted values against actual values. Generally, a good fit is determined by how far the fitted values are from the actual ones. If we square all of the distances (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE). Formally:

How we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE's but then find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since the measures are the smallest. We have highlighted these results and made the table more appealing by using the `gt` library.
```{r, results=FALSE}
accuracy(fit)
```

```{r, echo=FALSE}
accuracy(fit) %>% gt() %>%
  cols_align("center") %>% 
  tab_header(title = md("Model Fit")) %>% 
  tab_style(locations = cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "normal"))) %>% 
  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),
             decimals = 2) %>% 
  tab_style_body(
    style = cell_fill(color="lightgreen"),
    values = "Naive",
    targets ="row")
```



## Leasons Learned

In this module you have been introduced to the general procedure in forecasting time series.

-   Learning how create forecasts with simple heuristics.

## Readings

@FPP3 chapter 5.
