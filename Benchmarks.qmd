# Model Benchmarks

This module will introduce the modeling procedure by illustrating the process with four different benchmarks (Naive, LS, Mean, and Drift). The fit of these benchmarks will be assessed using accuracy measures such as the Mean Error, Mean Absolute Error, Root Mean Squared Error, among others. In general, these accuracy measures compare the fitted values with observed values. A good model will account for most of the series's systematic variation, leaving a small random error. Lastly, to avoid over-fitting the models, we will use a training set and test set along with cross validation.

## Benchmarks {#sec-Bench}

One of the most intuitive (but naive) predictions we can make about the future is to expect that the value of a variable will behave as it did in the past. A **naive prediction** sets the prediction of a future period to the value of the preceding period. For example, if you consider the task of predicting your weight, a simple heuristic would be to think that your weight tomorrow be the same as the weight observed today. Mathematically we would write:

<center>$\hat y_{T+h}=y_T$</center> \n

where $\hat y_{T+h}$ is the predicted value for $h$ periods ahead, and $y_T$ is the value observed at the current time period $T$. We can adjust the Naive prediction by accounting for some natural **drift** (an increase or decrease). Thinking about weight once again, we note that as kids grow, we expect their weight to be close to the previous measurement but slightly higher as we need to account for growth. We would "drift" the naive prediction upward. Mathematically we would write:

<center>$\hat y_{T+h}=y_T+h(\frac{y_t-y_1}{T-1})$</center> \n

where $h(\frac{y_t-y_1}{T-1})$ can be thought as the average increase of $y$ from period $1$ to the current period $T$. One could also predict weight by observing weight during a period and **averaging** the values. Every day the data recorded would be slightly different, but if diets, exercise, sleep, etc., remain relatively constant, the mean could be a good predictor of your future weight. Formally:

<center>$\hat y_{T+h}=\frac{(y_1+y_2+...+y_T)}{T}$</center> \n

Lastly, we can use the weight data collected from a period and observe if there is any trend. If we find ourselves motivated to lose weight we can start recording our weight every day. Ideally, we will start seeing the effect of our diet, exercise and healthy sleep in the data. We can predict tomorrows weight by taking into account the downward **trend** of our weight. Formally:

<center>$\hat y_{T+h}=b_0+b_1(T+h)$</center> \n

## Modeling the the Average Price of Avocados

Let's apply these four models to forecast the average price of avocados in California. We'll start by loading the `tidyverse` and `fpp3` packages and importing the data.

```{r warning=FALSE, echo=TRUE, message=FALSE}
library(tidyverse)
library(fpp3)
cali<-read_csv("https://jagelves.github.io/Data/CaliforniaAvocado.csv")
```

Recall, that we can create a tsibble from the csv file by using the `as_tsibble()` function. The index argument is set to the weekly *date* variable. The `filter_index()` function is used so that we can focus our analysis for the period of 2015-01-04\~2018-06-02.

```{r}
cali %>%
  as_tsibble(key=c(geography),
             index=date, regular=T) %>%
  filter_index("2015-01-04"~"2018-06-02")-> calits_train
```

Now we can use the `model()` function to run the benchmarks discussed in @sec-Bench. We have saved the models to an object called *fit*.

```{r}
fit <- model(calits_train,mean=MEAN(average_price),
              Naive=NAIVE(average_price),
              Drift=RW(average_price~drift()),
              LS=TSLM(average_price~date))
```

The *fit* object is saved as a mable (model table). To explore the coefficients of the models estimated, we use the `coef()` function with *fit* as its single argument. The output table has been enhanced visually by using the [`gt`](https://gt.rstudio.com) package.

```{r , echo=TRUE, warning=FALSE, message=FALSE}
library(gt)
coef(fit) %>% gt() %>% 
  cols_align("center") %>% 
  tab_header(title = 
               md("**Model Coefficients For The Avocado Data**")) %>% tab_style(locations =                                                    cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "bold"))) %>% 
  fmt_number(columns =c(statistic,estimate,std.error,p.value),
             decimals = 2)
```

The table records the estimates for all the benchmarks discussed in @sec-Bench. The naive method has no estimate, as it is simply the previous period's observed value. Below we illustrate the fit of the least squares model by the red line and the Naive model by the orange line.

```{r, warning=FALSE}
calits_train %>% autoplot(average_price) + theme_classic() + 
  geom_line(aes(y = .fitted), col="red",
            data = augment(fit) %>% filter(`.model`=="LS")) +
geom_line(aes(y = .fitted), col="orange",
            data = augment(fit) %>% filter(`.model`=="Naive")) +
  labs(y="", title= "California's Average Price Of Avocados",
       subtitle = "2015-2018",
       x="")
```

The graph illustrates how closely the Naive method follows the data. This might seem like a good model, but consider how every period the heuristic is making a mistake since average prices are constantly changing every week. Consider as well how the Naive prediction provides no explanation of the series governing process. The LS model, on the other hand, does provide some insight to a force that is influencing the data. Mainly, a rising trend. We can use characteristics such as a trend or seasonality to forecast a series.

## Model Fit

The model fit will be assessed by comparing the fitted values against actual values. In general, a good fit is determined by how far the fitted values are from the observed ones. If we square all of the distances between actual points and predicted values (i.e., errors) and then average them, we calculate the Mean Squared Error (MSE).

<center>$MSE = \frac{ \sum (\hat{y}_t-y_t)^2}{T}$</center>

How we decide to aggregate our errors will determine our measure of accuracy. For example, if we follow the same procedure as the one for MSE's but we then find the square root, we have calculated the RMSE. Below you will find a collection of accuracy measures for our benchmarks. You will notice that the Naive method provides the best results since all the accuracy metrics are the smallest. We highlighted these results and once again made the table more appealing by using the `gt` library.

```{r, results='hide'}
accuracy(fit)
```

```{r, echo=FALSE}
accuracy(fit) %>% gt() %>%
  cols_align("center") %>% 
  tab_header(title = md("**Model Fit**")) %>% 
  tab_style(locations = cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "bold"))) %>% 
  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),
             decimals = 2) %>% 
  tab_style_body(
    style = cell_fill(color="lightgreen"),
    values = "Naive",
    targets ="row")
```

## Forecast

The forecast of the series is obtained by using the `forecast()` function and specifying the number of periods ($h$) ahead we need to forecast. We can easily forecast 30 weeks of data in R with the code below.

```{r}
calits_fc<- fit %>% forecast(h=30)
```

The `autoplot()` and `autolayer()` functions are used below to create a graph with the forecasts and the training set.

```{r, warning=FALSE}
calits_fc %>% autoplot(level=NULL) + theme_classic() + 
  autolayer(calits_train, average_price)
```

Note how the Mean and Naive models forecast that the series will continue without a trend. The LS model predicts that the series will continue its trend but as all of the other methods, does not take into account the seasonal pattern discussed in @sec-Visual. In future chapters, we will be looking at models that account for trend and seasonality.

## Over-Fitting

Over-fitting can happen when a model is overly flexible. This can make the model fit to the random fluctuations or noise in the data, rather than the underlying pattern. This is a major failing in modeling as it ignores the systematic pattern that governs the time series.

To overcome this problem, we usually have a **training set** or subset of the data that we use to estimate the model's parameters. Once the model is estimated, we assess its performance on a new subset of the data that was not used in estimating the model. This second subset of data is called the **test set**. A model that "memorizes" the training data, will often perform poorly when forecasting the test set.

Recall that benchmarks were estimated for the period between 2015-01-04\~2018-06-02. We will call this our training set. For our test set, we'll use the 2018-06-02\~2018-12-02 period. The code below creates the test set using the `filter_index()` function.

```{r, results='hide'}
cali %>%
  as_tsibble(key=c(geography),
             index=date, regular=T) %>%
  filter_index("2018-06-02"~"2018-12-02")-> calits_test
```

Now we can plot the training set, the forecast, and the test set by using the code below.

```{r}
calits_fc %>% autoplot(level=NULL) + 
  theme_classic() + 
  autolayer(calits_train, average_price) + 
  autolayer(calits_test, average_price)
```

The graph shows how the LS method does well with the test data and a long forecast period. This can be confirmed by obtaining the accuracy measures against the test set.

```{r, results='hide', echo=FALSE}
cali %>%
  as_tsibble(key=c(geography),
             index=date, regular=T) %>%
  filter_index("2015-01-04"~"2018-12-02") -> calits
```

```{r, warning=FALSE}
accuracy(calits_fc, calits) %>% gt() %>%
  cols_align("center") %>% 
  tab_header(title = md("**Model Fit**")) %>% 
  tab_style(locations = cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "bold"))) %>% 
  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),
             decimals = 2) %>% 
  tab_style_body(
    style = cell_fill(color="lightgreen"),
    values = "LS",
    targets ="row")
```

Interestingly, the Naive method is no longer the best model. This is partly because it chases the series fluctuations (along with the noise) and always makes the same forecast regardless of how many periods we forecast ahead.

## Cross Validation

Instead of selecting a single training set and test set, we can create several. Specifically, we could take the first three observations and define them as the training set. We can then estimate a model and forecast the fourth observation. The forecast error is recorded and the training set is changed so that now the first four observations are used to estimate the model and forecast the fifth observation. This procedure is repeated as many times as the data allows. Below we create a table that enables us to follow the **cross-validation** of our benchmarks.

```{r}
avocado_cv <- calits_train %>% 
  select(-geography, -total_volume) %>% 
  stretch_tsibble(.init = 3, .step = 1)
```

The `stretch_tsibble()` is a handy function that creates a variable called *id* that is initialized with the *.init* argument. In this case, the first three observations are given $id=1$. The *id* then changes with a step of $.step=1$. That is, $id=2$ for the first four observations, then $id=3$ for the first five observations, and so on. Below is a sample of the tsibble.

```{r, echo=FALSE}
head(avocado_cv,8) %>% gt() %>% 
  cols_align("center") %>% 
  tab_header(title = md("**CV tsibble**"))
```

Using this new tsibble, the benchmarks are estimated for each *id* and forecasts are generated for one period ahead ($h=1$). The accuracy is measured and averaged across all iterations for each model. Results are shown in the table below.

```{r}
avocado_cv %>%
  model(Mean=MEAN(average_price),
        Naive=RW(average_price),
        Drift=RW(average_price ~ drift()),
        LS=TSLM(average_price~date)) %>%
  forecast(h = 1) %>% accuracy(calits) %>% gt() %>%
  cols_align("center") %>% 
  tab_header(title = md("**Model Fit Cross Validation**")) %>% 
  tab_style(locations = cells_column_labels(columns = everything()),
  style = list(cell_borders(sides = "bottom", weight = px(3)),
    cell_text(weight = "bold"))) %>% 
  fmt_number(columns =c(ME,RMSE,MAE,MPE,MAPE,MASE,RMSSE,ACF1),
             decimals = 2) %>% 
  tab_style_body(
    style = cell_fill(color="lightgreen"),
    values = "Naive",
    targets ="row")
```

The Naive method once again performs well. However, we note once again that the Naive method will provide the same forecast for one, two, three or more periods ahead. Additionally, there is no formal model telling us how data is generated or that explains the how the time series is generated.

## Readings

@FPP3 Chapter 5 (The Forcaster's Toolbox).

gt package: https://gt.rstudio.com

## Leasons Learned

In this module you have been introduced to the general procedure in forecasting time series. Particularly you have learned to:

-   Create forecasts with simple heuristics.

-   Assess the fit of the model with accuracy measures.

-   Create a test set and train set to avoid over-fitting.

-   Perform cross validation.
